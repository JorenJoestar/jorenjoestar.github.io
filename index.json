[{"authors":["admin"],"categories":null,"content":"Video-games Lover. Guitar enthusiast. Life-surfer.\nPassion, mistakes and curiosity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jorenjoestar.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Video-games Lover. Guitar enthusiast. Life-surfer.\nPassion, mistakes and curiosity.","tags":null,"title":"Gabriel Sassone","type":"authors"},{"authors":[],"categories":[],"content":"Hello everyone! How have you been ?\nA little more than a year has passed since my last post, but for a good reason!\nLast year was all dedicated to writing this book, Mastering Graphics Programming with Vulkan, available in different countries on Amazon US,UK,India,Italy and many more, with Marco Castorina.\nAll my coding free time went into the writing of this book with him, and this post is a little recap of what happened behind the scenes and my experience of writing a book for the first time!\nHow it started It all started thanks to this blog!\nAt the end of 2021 Marco wrote to me if I was interested to collaborate, in any form we would agree, on writing a book about Vulkan.\nThanks to the articles I wrote here and the code on Github he wanted to know if I could help.\nWe jumped on a Skype call and talked: since the beginning there was a good feeling talking to him. My gut instict was right!\nHe asked me in which way I wanted to help him, including being a co-author\u0026hellip;and me being me, I thought \u0026ldquo;I\u0026rsquo;ve never wrote a book, this could be an amazing experience!\u0026rdquo;.\nSo we agreed that we will write the book together, and from there he introduced me to the team at Packt and we started discussing about the topics.\nOne thing we really wanted to do is to dedicate time learning topics that we did not have time to study, and that was a huge risk but also a great reward.\nAfter some talking with Packt, we agreed on a 9 months writing time: a tight schedule, but we felt we could make it working together with Marco.\nWe succedeed, albeit not to the level we wanted, but it was a huge learning experience for both of us (I am pretty sure is the same for Marco).\nIn this case I measure success as a combination of effort, learning new things, delivering and enjoyment of the process.\nAs you will see later on in this post, this book is far from perfect and make me question a better way of delivering the rendering knowledge we wanted to share.\nThe purpose of the book Since the beginning we had clear in mind that we needed to show how to use Vulkan in a real, modern and AAA rendering context.\nBoth Marco and me work in the industry, and we decided to try to create a package that included the latest rendering tech that you would find in a AAA renderer. We knew we could not possibly deliver a bulletproof technology written in few months writing code after work, but at least we could give some pointers to what is used right now.\nWe also wanted to demistify Vulkan and show it in a practical full-blown demo. In this sense, after reading some reviews, I think we did not hit the mark - many complains are that we used a reincarnation of the Hydra engine, stripped down to accomodate a Vulkan only mindset.\nWe will talk about that later.\nAt the end, if I can simplify everything in one phrase, it would be:\nDemistification of Vulkan as an API and its application to modern rendering techniques\nChoosing the topics This was actually a very fun part!\nBoth me and Marco wanted to explore and write some techniques from scratch.\nWe started with asking ourselves which techniques a modern rendering engine supports, based on our work knowledge, and started jotting down those.\nThere also some techniques that we singularly never did, and we tried to do what we would have most fun doing.\nThis is a simple list:\nPure bindless mode Frame graph based Clustered Deferred Rendering Point lights with cubemap shadows Mesh shaders GPU driven rendering Temporal Anti-Aliasing Volumetric Fog Raytracing: shadows Global Illumination With this list we started writing collecting links to the various topics before the next big decision\u0026hellip;where do we start from a code pespective ?\nChoosing an engine We always knew we wanted to use the thinnest layer on top of Vulkan.\nWe explored various libraries, but something was clear: we need to learn how to use those libraries as well, and that would mean some time.\nAfter some thoughts\u0026hellip;we chose to take Hydra, my own creature ehm engine, and cleanup some code, make the API Vulkan only and use that.\nHydra is the foundation for the code of all the articles on this blog, and already the only backend working was Vulkan.\nSome could argue that this is not the best choice, but actually we have two advantages: first, having written it from scratch I knew every nook and crannies of the code; second it has a lot of boilerplate code for Vulkan and for other simple but useful subsystems (cameras, input, \u0026hellip;).\nAnother choice we made was to go fully Vulkan based: GLSL was the default choice for writing shaders, using JSON files to create a smaller Techniques/FX framework, and even better\u0026hellip;parsing SpirV to take informations about the pipeline!\nMarco came out with this idea, and I really liked it.\nBetween all the different skills he has, he is great in reading the Vulkan specifications and use them.\nAnd with these choices\u0026hellip;all was set to begin!\nHow to structure the code? This was another big decision to make.\nWe thought a lot about it: we wanted a way to freeze the code in time for each chapter, but we didn\u0026rsquo;t want to rely on github history to do that.\nThe natural evolution of this thought is to create the code for each chapter in a separate folder.\nSo we separated the code in the common code area, where code between different chapters is shared, and the rendering code went into each chapter.\nThis included both shaders and the core rendering code. The current structure of code is something like:\nsource\n__chapter1\n____graphics\n__raptor\n____application\n____foundation\nThe main raptor folder contains foundation and application subfolders: application is contains a high level wrapper for window, input and application so that it is easier to fire a working application, while foundation the main data structures (array and hash map) and some utility functions/subsystems (like file management, string buffers, time queries, memory management, logging, gltf parsing, camera\u0026hellip;).\nAll the code in these folders is shared between chapters, so we duplicate and evolve only the rendering part, as it is the main focus of the book.\nWas it perfect ?\nAbsolutely not.\nWas it painful ?\nYes, a little. Could we do better ? Absolutely yes!\nThere are many problems with these choices, but at the end it worked for us: for every chapter you have a snapshot of the rendering code, while everthing else that is not rendering related is unchanged, and should actually not be the focus.\nThere are some big downsides: for example every time we would add something to a chapter, we would need to port the code to the following chapters. Fixing a bug in chapter 4 meant porting it up to the 15th chapter!\nSame is happening now that the code is in the wild: when we receive a bugfix, we need to port it to all subsequent chapters.\nAgain not the best choice, but the best with our knowledge at the time we started.\nThe awesomeness of having a publisher I\u0026rsquo;ve never worked with a publisher, but working with Packt was a great experience.\nThey gave us the power to concentrate only on writing the book and the code, without worrying about anything else.\nPrinting, delivering, organizing, they really took care of us.\nWhen we thought we were late, or we needed more time, they listened as well.\nThey also knew that the time to write a book was crucial, as this book covers a missing piece of information.\nEvery feedback on both sides was thoughtful and well received, so it was a great experience.\nThank you to all the team behind Packt, we managed to have a deep focus on working on our side of things and only on that.\nThat was great to experience!\nLessons This whole journey has been powerful and helped me navigating some personal difficult times as well.\nThe power of deadlines Personally I love to have projects in my free time, because I can explore what I want, even if just redoing the same thing just in a completely different way.\nProblem is that not having a fixed objective can make me feel lost sometimes, same for not having any timeline.\nArticles in this blog helped that, but the book is the ultimate example!\nDeadlines gives you that huge lesson:\nDo the best that you can, with the resources you can.\nResources include both knowledge and time.\nIn the realm of knowledge I include not only technical knowledge, but also personal knowledge.\nThe problems of deadlines Deadlines have also an uncomfortable side with it.\nWe did not manage to put everything we wanted into the code, for example shader reloading, code generation, local tonemapping, and many many more.\nDeadlines really push you to think about what you can and cannot achieve given the time.\nAnd many times you over or underestimate the time, and have to change priorities.\nThis is one of the biggest problems with game development, as there are many unknowns unknowns.\nStill, you need to deliver, thus you can\u0026rsquo;t loose too much time thinking and writing code in the best way possible.\nYou deliver, and this is important.\nAlso, the time we put on this book was our free time, after a day of work, or in the weekend, so possibly at times you had to figure out things with very low energies.\nFor every piece of code we were writing, this was the question:\nIs this that I am doing really necessary given the time I have ?\nKnow thyself Another great lesson is that before writing a book, there is one \u0026lsquo;persona\u0026rsquo; - Gabriel - that is writing the book.\nDo you listen to yourself ? To your needs ? Are you ready to know Gabriel with a new perspective, as the writer of a book ?\nThese can seem like crazy questions, but our identity changes a bit depending on the context.\nWho do you think you are when writing a book ?\nAre you overconfident ? Underconfident (not sure this word exists) ?\nAre you ready to change your mindset on a technical topic ?\nAre you ready to do something wrong and being called out for that ?\nI think one of our superpowers is the possibility to change perspective based on experience.\nEven if it is painful: seeing that you don\u0026rsquo;t know enough about a topic that you should know, after all this year of rendering, can be painful.\nAs candid as I can be, there are still many topics that I don\u0026rsquo;t know deeply, nor I understand fully.\nThat is what I love about rendering: is such a huge topic, that really puts your ego in check.\nI will always know less than a year before, where multiple people are researching and advancing the knowledge.\nBut still, I can help crafting amazing games and rendering tech!\nAgain, the relationship with our reality is what matters.\nThis is one of the biggest lessons about writing this book: the combination of effort, changing perspective and mindset is the real win.\nAnd on that topic, I have no regrets: we acted at our best given the constraints.\nBut we can do better!\nAre you ready to change perspective in front of new inputs ?\nTechnical knowledge After a digression about the human side of things (I would talk about that for hours\u0026hellip;) let\u0026rsquo;s get back to the technical knowledge.\nWhat I\u0026rsquo;ve learnt is that\u0026hellip;I did not know many new topics, only read about it, and by actually doing them I have a better understanding.\nUltimate understanding ?\nNot even close!\nFor me, re-writing or re-implementing a technique makes me understand it.\nI found over and over that reading a paper or some code does not work much for me.\nI can have an idea about something, but re-doing is what makes me create my mental model of it.\nSome have the power to learn by just reading or hearing, not me.\nThat is why I am also active in my free time: real knowledge comes from models arising from experience.\nThe thinnest Vulkan layer possible This is a post-launch thought, and one of the most common critiques I\u0026rsquo;ve read around about the book.\nMany did not like the usage of Hydra/Raptor, and complained that this was too much of a \u0026lsquo;Raptor manual\u0026rsquo; instead of a pure Vulkan book.\nCould have we written a thinner layer ?\nI think we could have done it, and it would be an interesting challenge for the future.\nI still stand with our choice, as I think the abstraction we provided is so thin that does not change the API understanding, but actually elevates it to a more common \u0026rsquo;next generation APIs\u0026rsquo; mindset and knowledge that will help on using Vulkan, DirectX12, consoles API and such.\nStill I get it: this is a Vulkan book, so you should focus on Vulkan only!\nMaybe is the fact that, even though I\u0026rsquo;ve used (and written from scratch) Vulkan renderers in the past, I can\u0026rsquo;t help but seeing a pattern in the all APIs.\nAt the end rendering is a combination of big concepts, that are present in both Vulkan and DirectX12, like Images, Buffers, Pipelines, Swapchains, Command Buffers and such.\nOnce you master these concepts, you can use those on any API.\nYet with Raptor the usage is a slightly higher level compared to simple Vulkan, but I really feel still now that helps visualizing what is happening even better.\nI get that some areas, like descriptor sets and pipeline management can be too Raptory, but still it would have been a problem to fix anyway, even with simple Vulkan: this bookeeping is that you don\u0026rsquo;t find many times in tutorials around, but what gives you the power to develop complex techniques without writing a ton of work.\nI am (slowly) in the active process of trying to strip even more layering to have a simpler Vulkan, so I am happy about this critique.\nI will learn something new, and maybe next edition, if will ever exists, will be an even more barebone Vulkan code.\nIs there a way to write organized Vulkan code that is evern more straight-forward ?\nThe joy of exploration When writing the book, for each chapter we first wrote the code to be working to a good level, then we would write the corresponding chapter and fix/improve the code after.\nThis was an interesting way of working, really focused on having something measurable and usable before writing any sentence about a chapter.\nWith Marco we decided to put a lot of current topics, like Raytracing, Temporal Anti-Aliasing, Bindless, Mesh shaders and such, so that we could have a go at implementing them all together.\nThere are better implementations out there, but I think we delivered a solid demo in the final chapter, that showcases all those together in harmony.\nDuring the development, I would ask myself multiple times:\nAre you enjoying the exploration ?\nOften times when navigating uncharted territories, we are stressed out about it. And I was many times, but this question really helped me not reaching the breaking point.\nI still need to enjoy the process, otherwise what is the point of it ?\nIt was not always possible, and sometimes you have to grind through it, but still it was great.\nHaving Marco as an ally, and a friend, helped a lot.\nWhen any of the two was stuck, or tired, the other would come in and help.\nWe went with the flow, we worked together on some chapters, and singularly on others.\nWe talked and I shared also my personal difficulties, as 2022 was a though year: useful, but though.\nAnd when we were stressed, or tired, we would talk about that.\nNever forget about the human behind the role!\nI really enjoyed working with him!\nRendering is about synergies The more chapters we wrote, the more I thought how rendering is a beautiful synergy, and that is why re-creating a renderer visuals is hard.\nEvery pass in a frame needs to find its space in other passes as well (well not every one of them).\nIt is an orchestra of GPU programs, textures, buffers and pipelines that create beautiful pixels.\nBuilding each chapter with something more really shaped this.\nAgain, it could have been done in a better way, yes!\nBut the effort and the knowledge at the time, plus the time constraints, are all that matters.\nThat is why I am happy with the result: we forged something that, albeit not being perfect, really touched a little of everything you find in a modern renderer. Something you will not find in a tech demo, that shows just a portion of this.\nOnce we\u0026rsquo;ve added Clustered Deferred rendering, we added pointlights shadows (with the vertex/mesh shader only output) then we built on top Volumetric Fog, and TAA, and then started exploring indirect lighting with ray tracing\u0026hellip;everything worked together.\nRe-using the clustered light structure to read lights into the Volumetric Fog for example.\nWe did it step by step, trying to show the way.\nFor each step, the question was always:\nWhat is this technique adding to the scene ?\nWe always had an eye for performances as well, even though having a 2070 did not help :p\nBut still, as you see opening the demo, there is a GPU profiler that gives you a frame timing breakdown and helps keeping track of where the time budget is going.\nThis is a game developer mindset: always think about performances.\nTo create realtime-usable tech, you need it to be fast, or at least keep the total frame time to an acceptable level!\nFinal thoughts I\u0026rsquo;ve touched a lot of topics in this post, and I will gladly talk more about this stuff.\nOne thing is clear: I learnt a lot, both technically and personally, and I will continue to learn more.\nThere are other things on the horizon sparked by this book, like a talk about Vulkan at Eurographics 2023, or the idea of doing a \u0026lsquo;write your modern renderer\u0026rsquo; in italian, my native language, and more.\nFor sure a lot of feedback and things to evolve and experiment, and I am thankful for that.\nI also wish that we could help streamlining the usage of Vulkan, maybe developing some lightweight library to help developers (like the Vulkan Memory Allocator), but let\u0026rsquo;s see the future.\nFor now I enjoy the experience that the book gave me, the many lessons, the knowledge and the thought that I can still improve and learn, while remembering to enjoy the process.\n","date":1675781188,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675781188,"objectID":"574376728bf84cbed3d39dcd74d0f078","permalink":"https://jorenjoestar.github.io/post/year_of_the_vulkan_book/","publishdate":"2023-02-07T15:46:28+01:00","relpermalink":"/post/year_of_the_vulkan_book/","section":"post","summary":"Hello everyone! How have you been ?\nA little more than a year has passed since my last post, but for a good reason!\nLast year was all dedicated to writing this book, Mastering Graphics Programming with Vulkan, available in different countries on Amazon US,UK,India,Italy and many more, with Marco Castorina.\nAll my coding free time went into the writing of this book with him, and this post is a little recap of what happened behind the scenes and my experience of writing a book for the first time!","tags":[],"title":"The Year of the Vulkan Book","type":"post"},{"authors":[],"categories":[],"content":" The sprites rendered with 1 draw call using the techniques described in this article. Overview Sprite batching is one of the fundamental techniques used in almost all pixel art games (that are lovingly back in town after the first era of 3D), and yet I never found any recent documentation.\nSince the Bindless Age has started old algorithms can be implemented in new ways.\nIn this short article I would like to talk about how easy is to manage sprites, including UI, with the bindless model.\nSprite Batching: how ? Sprite batching is a way of reducing the number of draw calls (still something to not abuse) by grouping them based on similar properties.\nNormally when batching sprites, we would submit a batch of sprites when any texture would change, an operation that could be reduced by using texture atlases.\nFor tiles this can be more easily done, but for characters with a lot of complex sprite sheets it is harder.\nWe would have something like (pseudocode):\nfor each sprite\rsprite_batch.set( sprite.texture )\rsprite_batch.draw( x, y, width, height ) The \u0026lsquo;set\u0026rsquo; method would check if the current texture is differen than the sprite one, and if so then it would submit the accumulated sprites, then cache the sprite texture and start filling the new sprites.\nA good code to check is the one for libGDX sprite batching.\nCan we do better ? The answer is yes!\nSprite Batch: caching To have an effective Sprite Batch we need to cache some informations in order to know when to group sprites and when not. A possible list is the following:\nRender States (depth, blend, \u0026hellip;) Vertex Layouts Shader Textures In Vulkan world, render states, shader and vertex layouts are all included into a pipeline. But Textures ?\nThey are in used in descriptor sets, but with bindless we can simply ignore them, because they are passed down as integers into constants.\nRender states, vertex layouts and shaders are all inside a Pipeline State Object, so the only caching really needed here is:\nPipeline Descriptor Set We still need to differentiate shaders that use different constant/structured buffers, but otherwise we can share a common shader!\nThe core of the caching is in sprite_batch.cpp:\nvoid SpriteBatch::set( hydra::gfx::PipelineHandle pipeline, hydra::gfx::ResourceListHandle resource_list ) {\rusing namespace hydra::gfx;\rif ( current_pipeline.index != k_invalid_pipeline.index \u0026amp;\u0026amp; current_resource_list.index != k_invalid_list.index \u0026amp;\u0026amp;\r( current_pipeline.index != pipeline.index )) {\r// Add a batch\rDrawBatch batch { current_pipeline, current_resource_list, previous_offset, num_sprites - previous_offset };\rdraw_batches.push( batch );\r}\rprevious_offset = num_sprites;\rcurrent_pipeline = pipeline;\rcurrent_resource_list = resource_list;\r} We use the pipeline and the resource list as informations to know when a batch should be changed.\nPassing texture index to the GPU As we saw in the previous article, with bindless we define a global array of textures and we simply index into it using shader constants or other tricks (more on that later).\n// (glsl code), Platform.h:\r#if defined(HYDRA_BINDLESS)\r#extension GL_EXT_nonuniform_qualifier : enable\rlayout ( set = 1, binding = 10 ) uniform sampler2D textures[];\r// Use aliasing to sample any kind of texture using one single bindless texture array:\rlayout ( set = 1, binding = 10 ) uniform sampler3D textures_3d[];\r#endif // HYDRA_BINDLESS In our implementation we will use per-instance sprite data to encode the texture id.\n//\r//\rstruct SpriteGPUData {\rvec4s position;\rvec2s uv_size;\rvec2s uv_offset;\rvec2s size;\ru32 flag0;\ru32 flag1;\rvoid set_albedo_id( u32 albedo_id ) { flag1 = albedo_id; }\ru32 get_albedo_id() const { return flag1; }\r}; // struct SpriteGPUData In this case we use flag1 as the index containing the texture to read.\nIn Hydra I have a freelist for all rendering resources, so I can always use the texture index without worrying about index collisions and such.\nSprite Shader The sprite shader can be modified to include uints as vertex layout inputs, and pass the a flat uint to the fragment program:\n#if defined VERTEX\rlayout (location = 0) in vec4 position;\rlayout (location = 1) in vec4 uv_size_offset;\rlayout (location = 2) in vec2 size;\rlayout (location = 3) in uvec2 flags; layout (location = 0) out vec4 uv_alpha;\r// NOTE: flat attribute!\rlayout (location = 1) flat out uint out_albedo_id;\rvoid main() {\rconst uint vertex_index = gl_VertexID % 6;\r...\r// Pass through albedo id\rout_albedo_id = flags.y;\r... We can now sample the texture in the fragment program easily.\nNOTE in Vulkan we need an additional keyword to properly sample a texture in the bindless model without incurring in problems, and this is the nonuniformEXT keyword.\nThanks to Christian Forfang for the email, I totally forgot about that!\nI\u0026rsquo;ve added a toggle so you can see the problems that could happen when you don\u0026rsquo;t include that keyword.\nThe correct shader is this:\n#if defined FRAGMENT\r#pragma include \u0026#34;platform.h\u0026#34;\rlayout (location = 0) in vec4 uv_alpha;\r// NOTE: flat attribute!\rlayout (location = 1) flat in uint albedo_id;\rlayout (location = 0) out vec4 out_color;\rvoid main() {\rvec4 color = texture( textures[nonuniformEXT(albedo_id)], uv_alpha.xy ); By simply passing the uint in the instance data we have what we need to render any sprite.\nNon-Uniform-EXT keyword importance There is an incredibly informative blog post about the nonuniformEXT on Anki 3D blog, as well as the spec itself.\nAgain thanks a lot to Christian Forfang to point out the missing keyword. I could see the problem without the keyword only on my integrated AMD card from my AMD 5900HX CPU, not on my Nvidia 2070, but it is great to be aware of these kind of problems.\nHere I highlighted the problem as you can see without that keyword:\nNon synchronized pixels for missing nonuniformEXT keyword I also updated the code so you can choose when to disable it and see the problem yourself.\nOne draw call RenderDoc truth: 1 draw call for the background, 1 for all those sprites! As we can see from this RenderDoc capture we are using an instanced draw call with bindless to render all the sprites on the screen in this demo, even though they are coming from 5 different files!\nIf we can have similar shaders (and many times for pixel art games we CAN), then it is guaranteed that draw calls will be kept at minimum.\nWe can use SSBO/StructuredBuffer with more per instance data to further specialize the shader without changing the sprites (like manual vertex pulling) but as always, numbers will tell what is better for your application, and finding a balance between draw calls, permutations and shader complexity is paramount!\nBonus: post-process bindless trick A trick I started using with bindless for post-process shaders is to use the instance id as texture index to be retrieved into the shader!\nYou can use push constants as well, but I figured something even simpler would work.\n// Pass through from main rt to swapchain\rcb-\u0026gt;bind_pass( sort_key++, renderer-\u0026gt;gpu-\u0026gt;swapchain_pass );\rcb-\u0026gt;bind_pipeline( sort_key++, debug_gpu_font_material-\u0026gt;passes[ gpu_text::pass_through ].pipeline );\rcb-\u0026gt;bind_resource_list( sort_key++, \u0026amp;debug_gpu_font_material-\u0026gt;passes[ gpu_text::pass_through ].resource_list, 1, 0, 0 );\r// Use first_instance to retrieve texture ID for bindless use.\rcb-\u0026gt;draw( sort_key++, hydra::gfx::TopologyType::Triangle, 0, 3, main_texture-\u0026gt;handle.index, 1 ); We have only 1 instance of the fullscreen triangle, and the instance index tells us the texture index.\n#if defined VERTEX\rlayout (location = 0) out vec2 vTexCoord;\rlayout (location = 1) flat out uint texture_id;\rvoid main() {\rvTexCoord.xy = vec2((gl_VertexID \u0026lt;\u0026lt; 1) \u0026amp; 2, gl_VertexID \u0026amp; 2);\rgl_Position = vec4(vTexCoord.xy * 2.0f - 1.0f, 0.0f, 1.0f);\r// If rendering to a RenderTarget, flip Y\rgl_Position.y = -gl_Position.y;\r// Using instance index to simply read an uint\r// storing the texture index in the bindless array.\rtexture_id = gl_InstanceIndex; Bonus2: ImGUI rendering A similar trick can be found inside the Hydra ImGui backend, using texture ids:\nvoid ImGuiService::render( hydra::gfx::Renderer* renderer, hydra::gfx::CommandBuffer\u0026amp; commands ) {\r...\rTextureHandle new_texture = *(TextureHandle*)( pcmd-\u0026gt;TextureId );\r...\rcommands.draw_indexed( sort_key++, hydra::gfx::TopologyType::Triangle, pcmd-\u0026gt;ElemCount, 1, index_buffer_offset + pcmd-\u0026gt;IdxOffset, vtx_buffer_offset + pcmd-\u0026gt;VtxOffset, new_texture.index ); With bindless there is no need to create different descriptor sets based on the texture used by the ImGUI window, and with the instance index we can again draw every texture in a simple way!\nConclusion In this article we saw a simple way to evolve a sprite batch to group sprites without the need to worry about texture changes.\nFor ages this was the main parameter to separate batches, but with bindless active is not anymore.\nFurthermore, Pipeline State Objects contains most of the informations needed to know if something has changed from a render state/layout perspective, and we can balance out dynamic parts of the pipeline to cache even more.\nAs awlays if you enjoyed this article please comment, share, send feedback! Gabriel\n","date":1639946201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639946201,"objectID":"537c4fcecb537940028a45c36088b082","permalink":"https://jorenjoestar.github.io/post/modern_sprite_batch/","publishdate":"2021-12-19T21:36:41+01:00","relpermalink":"/post/modern_sprite_batch/","section":"post","summary":"The sprites rendered with 1 draw call using the techniques described in this article. Overview Sprite batching is one of the fundamental techniques used in almost all pixel art games (that are lovingly back in town after the first era of 3D), and yet I never found any recent documentation.\nSince the Bindless Age has started old algorithms can be implemented in new ways.\nIn this short article I would like to talk about how easy is to manage sprites, including UI, with the bindless model.","tags":[],"title":"Modern (Bindless) Sprite Batch for Vulkan (and more!)","type":"post"},{"authors":[],"categories":[],"content":"Overview Here is a very short article on how to use Vulkan bindless features.\nAll the relevant c++ code is into gpu_device_vulkan.cpp and command_buffer.cpp, and you can follow along HYDRA_BINDLESS defines to see what changes.\nTHIS IS THE FASTEST ROUTE TO START USING BINDLESS TEXTURES THAT I KNOW OF.\nThis means also that is architecturally not optimal but it will serve its purposes!\nThere is a great article about the different features used here at arm, check it out first!\n1. Querying for GPU support In a pure Vulkan way, we want to query for the support of a couple of features, like this:\nVkPhysicalDeviceDescriptorIndexingFeatures indexing_features{ VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_DESCRIPTOR_INDEXING_FEATURES_EXT, nullptr }; VkPhysicalDeviceFeatures2 device_features{ VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2, \u0026amp;indexing_features }; vkGetPhysicalDeviceFeatures2( vulkan_physical_device, \u0026amp;device_features ); bool bindless_supported = indexing_features.descriptorBindingPartiallyBound \u0026amp;\u0026amp; indexing_features.runtimeDescriptorArray; 2. Enabling GPU features I will put a little more context here.\nWith this code we enable all possible supported features, thus when we create the device we already have pNext used.\nBeing a linked list, to add more features to a device we need to add another link to our indexing features created above:\n// Enable all features: just pass the physical features 2 struct. VkPhysicalDeviceFeatures2 physical_features2 = { VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2 }; vkGetPhysicalDeviceFeatures2( vulkan_physical_device, \u0026amp;physical_features2 ); VkDeviceCreateInfo device_create_info = {}; device_create_info.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO; device_create_info.queueCreateInfoCount = sizeof( queue_info ) / sizeof( queue_info[ 0 ] ); device_create_info.pQueueCreateInfos = queue_info; device_create_info.enabledExtensionCount = device_extension_count; device_create_info.ppEnabledExtensionNames = device_extensions; device_create_info.pNext = \u0026amp;physical_features2; #if defined(HYDRA_BINDLESS) if ( bindless_supported ) { // This should be already set to VK_TRUE, as we queried before. indexing_features.descriptorBindingPartiallyBound = VK_TRUE; indexing_features.runtimeDescriptorArray = VK_TRUE; physical_features2.pNext = \u0026amp;indexing_features; } #endif // HYDRA_BINDLESS result = vkCreateDevice( vulkan_physical_device, \u0026amp;device_create_info, vulkan_allocation_callbacks, \u0026amp;vulkan_device ); check( result ); 3. Descriptor Pool Creation When creating the Descriptor Pool, we need to add the flag VK_DESCRIPTOR_POOL_CREATE_UPDATE_AFTER_BIND_BIT_EXT.\nThis is needed so we can update textures in the bindless array without any problem or validation error.\nStill we need to be sure to not change a used resource in command buffers that are running!\nstatic const u32 k_max_bindless_resources = 16536; // Create bindless descriptor pool VkDescriptorPoolSize pool_sizes_bindless[] = { { VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, k_max_bindless_resources } }; // Update after bind is needed here, for each binding and in the descriptor set layout creation. pool_info.flags = VK_DESCRIPTOR_POOL_CREATE_UPDATE_AFTER_BIND_BIT_EXT; pool_info.maxSets = k_max_bindless_resources * ArraySize( pool_sizes_bindless ); pool_info.poolSizeCount = ( u32 )ArraySize( pool_sizes_bindless ); pool_info.pPoolSizes = pool_sizes_bindless; result = vkCreateDescriptorPool( vulkan_device, \u0026amp;pool_info, vulkan_allocation_callbacks, \u0026amp;vulkan_descriptor_pool_bindless ); 4. Global DescriptorSet/Descriptor Layout Creation At this point we create a global descriptor layout and a global descriptor set.\nThis is just because we want the fastest route to use bindless, but architecturally speaking is not the best.\nFirst we create the Descriptor Set Layout with at least the flags VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT_EXT and VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT_EXT .\nThese are needed because we will not fill all the array elements of the bindless array (partially bound flag) and we will update when a new texture is added to the device (update after bind).\nAlso adding the Variable Descriptor Count in case we want more than one descriptor layout /set with a bindless array:\n// Create bindless global descriptor layout. { VkDescriptorBindingFlags bindless_flags = VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT_EXT | VK_DESCRIPTOR_BINDING_VARIABLE_DESCRIPTOR_COUNT_BIT_EXT | VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT_EXT; VkDescriptorSetLayoutBinding vk_binding; vk_binding.descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER; vk_binding.descriptorCount = k_max_bindless_resources; vk_binding.binding = k_bindless_texture_binding; vk_binding.stageFlags = VK_SHADER_STAGE_ALL; vk_binding.pImmutableSamplers = nullptr; VkDescriptorSetLayoutCreateInfo layout_info = { VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO }; layout_info.bindingCount = 1; layout_info.pBindings = \u0026amp;vk_binding; layout_info.flags = VK_DESCRIPTOR_SET_LAYOUT_CREATE_UPDATE_AFTER_BIND_POOL_BIT_EXT; VkDescriptorSetLayoutBindingFlagsCreateInfoEXT extended_info{ VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_BINDING_FLAGS_CREATE_INFO_EXT, nullptr }; extended_info.bindingCount = 1; extended_info.pBindingFlags = \u0026amp;bindless_flags; layout_info.pNext = \u0026amp;extended_info; vkCreateDescriptorSetLayout( vulkan_device, \u0026amp;layout_info, vulkan_allocation_callbacks, \u0026amp;vulkan_bindless_descriptor_layout ); At this point we create the actual descriptor set from the bindless pool:\nVkDescriptorSetAllocateInfo alloc_info{ VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO }; alloc_info.descriptorPool = vulkan_descriptor_pool_bindless; alloc_info.descriptorSetCount = 1; alloc_info.pSetLayouts = \u0026amp;vulkan_bindless_descriptor_layout; VkDescriptorSetVariableDescriptorCountAllocateInfoEXT count_info{ VK_STRUCTURE_TYPE_DESCRIPTOR_SET_VARIABLE_DESCRIPTOR_COUNT_ALLOCATE_INFO_EXT }; u32 max_binding = k_max_bindless_resources - 1; count_info.descriptorSetCount = 1; // This number is the max allocatable count count_info.pDescriptorCounts = \u0026amp;max_binding; alloc_info.pNext = \u0026amp;count_info; check_result( vkAllocateDescriptorSets( vulkan_device, \u0026amp;alloc_info, \u0026amp;vulkan_bindless_descriptor_set ) ); } 5. Texture Upload At this point whenever we create a new texture we can update the global bindless descriptor set.\nI prefer to batch this and do it at the end of the frame, here it is:\nif ( num_texture_updates ) { // Handle deferred writes to bindless textures. // TODO: use dynamic array instead. static constexpr u32 k_num_writes_per_frame = 16; VkWriteDescriptorSet bindless_descriptor_writes[ k_num_writes_per_frame ]; VkDescriptorImageInfo bindless_image_info[ k_num_writes_per_frame ]; u32 current_write_index = 0; for ( i32 it = num_texture_updates - 1; it \u0026gt;= 0; it-- ) { ResourceUpdate\u0026amp; texture_to_update = texture_to_update_bindless[ it ]; if ( current_write_index == k_num_writes_per_frame ) break; TextureVulkan* texture = access_texture( { texture_to_update.handle } ); VkWriteDescriptorSet\u0026amp; descriptor_write = bindless_descriptor_writes[ current_write_index ]; descriptor_write = { VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET }; descriptor_write.descriptorCount = 1; descriptor_write.dstArrayElement = texture-\u0026gt;handle.index; descriptor_write.descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER; descriptor_write.dstSet = vulkan_bindless_descriptor_set; descriptor_write.dstBinding = k_bindless_texture_binding; SamplerVulkan* vk_default_sampler = access_sampler( default_sampler ); VkDescriptorImageInfo\u0026amp; descriptor_image_info = bindless_image_info[ current_write_index ]; descriptor_image_info.sampler = vk_default_sampler-\u0026gt;vk_sampler; descriptor_image_info.imageView = texture-\u0026gt;vk_image_view; descriptor_image_info.imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL; descriptor_write.pImageInfo = \u0026amp;descriptor_image_info; texture_to_update.current_frame = u32_max; texture_to_update_bindless[ it ] = texture_to_update_bindless[ --num_texture_updates ]; ++current_write_index; } if ( current_write_index ) { vkUpdateDescriptorSets( vulkan_device, current_write_index, bindless_descriptor_writes, 0, nullptr ); } } 6. Adding the Bindless Descriptor Layout to layouts Again a quick and dirty example of adding the bindless layout to every layout:\nu32 bindless_active = 0; #if defined(HYDRA_BINDLESS) vk_layouts[ creation.num_active_layouts ] = vulkan_bindless_descriptor_layout; bindless_active = 1; #endif VkPipelineLayoutCreateInfo pipeline_layout_info = { VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO }; pipeline_layout_info.pSetLayouts = vk_layouts; pipeline_layout_info.setLayoutCount = creation.num_active_layouts + bindless_active; VkPipelineLayout pipeline_layout; check( vkCreatePipelineLayout( vulkan_device, \u0026amp;pipeline_layout_info, vulkan_allocation_callbacks, \u0026amp;pipeline_layout ) ); 7. Descriptor Set Binding at Draw The final (c++ wise) step is to bind the bindless set, here in command buffer code:\nvoid CommandBuffer::bind_resource_list( uint64_t sort_key, ResourceListHandle* handles, uint32_t num_lists, uint32_t* offsets, uint32_t num_offsets ) { // ... static const u32 k_first_set = 0; vkCmdBindDescriptorSets( vk_command_buffer, current_pipeline-\u0026gt;vk_bind_point, current_pipeline-\u0026gt;vk_pipeline_layout, k_first_set, num_lists, vk_descriptor_sets, num_offsets, offsets_cache ); if ( device-\u0026gt;bindless_supported ) { static const u32 k_bindless_set = 1; vkCmdBindDescriptorSets( vk_command_buffer, current_pipeline-\u0026gt;vk_bind_point, current_pipeline-\u0026gt;vk_pipeline_layout, k_bindless_set, 1, \u0026amp;device-\u0026gt;vulkan_bindless_descriptor_set, 0, nullptr ); } 8. GLSL shader! And here the modified code for the shader.\nAlbedo index can come from anywhere, for fullscreen triangle I use the trick of using the instance id for example!\n#extension GL_EXT_nonuniform_qualifier : enable layout (set = 1, binding = 10) uniform sampler2D textures[]; vec4 color = texture( textures[albedo_id], final_uv.xy ); Conclusions We saw the minimal code needed to have a fully working bindless texture implementation in Vulkan.\nAPI-Wise this is not the best, with hardcoded values, Vulkan only global descriptor and such, but it is a jumpstart to experiment with the feature.\nI will cleanup this code and update the library in the future, but this should already be a way to start bindlessly coding.\nHappy Bindless!\n","date":1635788657,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635788657,"objectID":"60feb683e323d5fd015cf9bab897e6f5","permalink":"https://jorenjoestar.github.io/post/vulkan_bindless_texture/","publishdate":"2021-11-01T18:44:17+01:00","relpermalink":"/post/vulkan_bindless_texture/","section":"post","summary":"Overview Here is a very short article on how to use Vulkan bindless features.\nAll the relevant c++ code is into gpu_device_vulkan.cpp and command_buffer.cpp, and you can follow along HYDRA_BINDLESS defines to see what changes.\nTHIS IS THE FASTEST ROUTE TO START USING BINDLESS TEXTURES THAT I KNOW OF.\nThis means also that is architecturally not optimal but it will serve its purposes!\nThere is a great article about the different features used here at arm, check it out first!","tags":[],"title":"Vulkan Pills 1: Bindless Textures","type":"post"},{"authors":[],"categories":[],"content":"Overview Recently on Twitter there was an interesting conversation about GPU-Driven line rendering.\nThis reminded me of a system that I use to render values that live only on the GPU, like GPU VFX: it gives you the possibility to output text and values from shaders!\nNormally these values can be mapped and read back into the GPU, but sometimes can be easier to just write them from a shader.\nI remember seeing this incredible shadertoy and being in awe: a never-done pet project of mine was to create a game only on the GPU, but instead I use it in standard ways many times.\nHere we are rendering\u0026hellip;text straight from a shader ? WHAT ?\nThe idea comes from this article as far as I understood, but the idea is the following: compress each character data into an array of 4 floats (vec4/float4), with each component being the packed bits of one quadrant, and given an UV reconstruct the shape of the character.\nTaken straight from that website as deeper explanation:\n/* -------- -███---- ██-██--- ██-██--- -███---- █████-█- ██-████- ██--██-- ██-███-- -███-██- -------- -------- 00000000 01110000 11011000 11011000 01110000 11111010 11011110 11001100 11011100 01110110 00000000 00000000 //Broken up into 4 8x3 (24 bit) chunks for each component of the vec4. //Hexadecimal is being used to reduce clutter in the code but decimal still works. 00000000 01110000 -\u0026gt; 00000000 01110000 11011000 -\u0026gt; 0x0070D8 11011000 11011000 01110000 -\u0026gt; 11011000 01110000 11111010 -\u0026gt; 0xD870FA 11111010 11011110 11001100 -\u0026gt; 11011110 11001100 11011100 -\u0026gt; 0xDECCDC 11011100 01110110 00000000 -\u0026gt; 01110110 00000000 00000000 -\u0026gt; 0x760000 00000000 vec4(0x0070D8,0xD870FA,0xDECCDC,0x760000) */ There is quite some bit trickery involved that I will not dwelve into, but the gist of it is simple, even though the process to arrive there must have been interesting!\nGiven these compressed fonts, we can use GPU buffers to store all the informations needed to write anything we want on the screen.\nThe Solution From a higher level view, what we will do is the following:\nReserve some memory to write our text Choose and save the position of the string, cache the reserved memory offset and count Write a per-character dispatch information Draw sprites with all those informations. The Buffers We will use 4 different buffers to have this working. It can be improved honestly, but this is good enough for a debug only feature!\nData and atomics buffer This buffer will contain the atomics and the data to be filled with each character of each string.\n// Common buffers used to render gpu driven font layout (std430, binding=2) buffer DebugGpuFontBuffer { uint current_data_index; uint current_entry_index; uint padding1; uint padding2; vec4 data[]; }; Entry buffer For each string we want to render, we need a position and where in the global data memory we need to read, thus offset and count.\nstruct DebugGPUStringEntry { float x; float y; uint offset; uint count; }; layout (std430, binding=3) buffer DebugGpuFontEntries { DebugGPUStringEntry entries[]; }; Dispatch buffer This buffer is used to generate draw for each character in the global data.\nWe need this so that each character knows where it belongs in a string.\nlayout(std430, binding=4) buffer DebugGPUFontDispatch { uvec4 dispatches[]; }; Indirect buffer Final buffer is the one driving how many total characters we will draw.\nlayout(std430, binding=5) buffer DebugGPUIndirect { uint vertex_count; uint instance_count; uint first_vertex; uint first_instance; uint pad00; uint pad01; uint pad02; uint pad03; }; Now that we know the data used, let\u0026rsquo;s see the algorithm itself.\nReserving memory and writing text to GPU buffers The main ingredient for this solution is the possibility to write to StructuredBuffers/SSBOs (depending on the API of your choice) PLUS the usage of atomic operations.\nThese operations are possible since OpenGL 4+ (actually my initial implementation is on an OpenGL-backed engine), so all newer APIs are supported.\nLet\u0026rsquo;s have a look at some code, from the vertex shader of a sprite:\n// 1. Reserve memory // Reserve 1 entry, that will include position and where in the data buffer we will write our text. uint entry_index = atomicAdd(current_entry_index, 1); // Reserve 16 characters uint data_index = atomicAdd(current_data_index, 16); // 2. Cache string entry // Cache data offset and count. We will use this to drive the text rendering. entries[entry_index].x = STRWIDTH(5); entries[entry_index].y = STRHEIGHT(5); entries[entry_index].offset = data_index; entries[entry_index].count = 16; // 3. Write actual text // MAVERICK: I did not found a better way to write this... data[data_index] = ch_p; data[data_index + 1] = ch_o; data[data_index + 2] = ch_s; data[data_index + 3] = ch_spc; data[data_index + 4] = get_digit(position.x, 2); data[data_index + 5] = get_digit(position.x, 1); data[data_index + 6] = get_digit(position.x, 0); data[data_index + 7] = ch_per; data[data_index + 8] = get_digit(position.x, -1); data[data_index + 9] = ch_com; data[data_index + 10] = get_digit(position.y, 2); data[data_index + 11] = get_digit(position.y, 1); data[data_index + 12] = get_digit(position.y, 0); data[data_index + 13] = ch_per; data[data_index + 14] = get_digit(position.y, -1); data[data_index + 15] = ch_spc; As we can see the way of writing the text is horrible, but it works.\nWe are first reserving memory using atomic operations.\nWe then cache the string informations, like position and where to access the global data.\nFinally we write the text!\nGenerating the dispatch Once we collected all the characters around, we need to generate the per-character dispatches and the indirect draw.\nI wrote a compute shader for that, optimizable, but good for the purpose here.\nIt also writes another string with GPU data about the system itself:\nlayout (local_size_x = 1, local_size_y = 1, local_size_z = 1) in; void main() { // Write global label with gpu font system data uint entry_index = atomicAdd(current_entry_index, 1); uint data_index = atomicAdd(current_data_index, 16); data[data_index] = ch_t; data[data_index + 1] = ch_e; data[data_index + 2] = ch_s; data[data_index + 3] = ch_t; data[data_index + 4] = ch_spc; data[data_index + 5] = ch_h; data[data_index + 6] = ch_g; data[data_index + 7] = ch_per; data[data_index + 8] = ch_4; data[data_index + 9] = ch_spc; data[data_index + 10] = get_digit(current_data_index, 1); data[data_index + 11] = get_digit(current_data_index, 0); data[data_index + 12] = ch_spc; data[data_index + 13] = get_digit(current_entry_index, 1); data[data_index + 14] = get_digit(current_entry_index, 0); vec2 print_pos = floor(vec2(STRWIDTH(5), STRHEIGHT(4))); entries[entry_index].x = print_pos.x; entries[entry_index].y = print_pos.y; entries[entry_index].offset = data_index; entries[entry_index].count = 16; Here we are \u0026ldquo;simply\u0026rdquo; writing a string containing the indices used by the system, just to keep track of them.\n// Write single character dispatch informations uint global_index = 0; for (uint e = 0; e \u0026lt; current_entry_index; ++e) { uint entry_data_index = entries[0].offset; for (uint i = 0; i \u0026lt; entries[0].count; ++i) { dispatches[global_index].x = e; dispatches[global_index].y = i; ++global_index; } } This is where we create the per character data.\nWe can optimize this, probably using one uint instead of 4.\n// Write indirect draw values vertex_count = 6; instance_count = global_index; first_vertex = 0; first_instance = 0; pad00 = 0; } Finally we draw 2 triangles for each character data.\n(Indirect) Rendering of the GPU Driven Text We are now ready to write the actual text on the screen!\nHere is the final shader:\n#if defined VERTEX layout (location = 0) out vec2 uv; layout (location = 1) flat out uint global_data_index; // Per vertex positions and uvs of a quad vec3 positions[6] = vec3[6]( vec3(-0.5,-0.5,0), vec3(0.5,-0.5,0), vec3(0.5, 0.5, 0), vec3(0.5, 0.5, 0), vec3(-0.5,0.5,0), vec3(-0.5,-0.5,0) ); vec2 uvs[6] = vec2[6]( vec2(0.0, 1.0), vec2(1.0, 1.0), vec2(1.0, 0.0), vec2(1.0, 0.0), vec2(0.0, 0.0), vec2(0.0, 1.0) ); void main() { const uint vertex_index = gl_VertexID % 6; // Calculate UVs uv.xy = uvs[vertex_index]; // Sprite size const vec2 sprite_size = CHAR_SIZE; // Calculate world position vec4 world_position = vec4( vec2(positions[ vertex_index ].xy * sprite_size ), 0, 1 ); uint global_char_index = gl_InstanceIndex; uint entry_index = dispatches[global_char_index].x; uint entry_char_index = dispatches[global_char_index].y; DebugGPUStringEntry entry = entries[entry_index]; // Calculate actual position from the start of the string world_position.xy += vec2(entry.x + entry_char_index * sprite_size.x, entry.y); // Move position to upper left corner world_position.xy += sprite_size * 0.5f; // Pass entry data to read the final compressed font character global_data_index = entry.offset + entry_char_index; gl_Position = locals.projection_matrix_2d * world_position; } #endif // VERTEX #if defined FRAGMENT layout (location = 0) in vec2 uv; layout (location = 1) flat in uint global_data_index; layout (location = 0) out vec4 out_color; void main() { vec4 char_data = data[global_data_index]; vec2 duv = uv * CHAR_SIZE; vec2 print_pos = vec2(0, 10); // Decompress char and color pixel! float textPixel = print_char(char_data, duv, print_pos); if (textPixel \u0026lt; 0.01f) discard; vec3 col = vec3(1); col *= mix(vec3(0.2),vec3(0.5,1,0),textPixel); out_color = vec4(col.rgb, 1); } #endif // FRAGMENT It should be pretty straighforward, but basically what is happening is this:\nRead the dispatch information, to know where the sprite will be located Expand the quad to the correct location Read the information packed font character based on the character Color the pixel! The real magic happens in the \u0026lsquo;print_char\u0026rsquo; method, and here I will post all the code that comes from the shadertoy example I wrote before!\nconst float DOWN_SCALE = 1.0f; #define MAX_INT_DIGITS 4 //#define FLIP_Y const vec2 CHAR_SIZE = vec2(8, 12); const vec2 CHAR_SPACING = vec2(8, 12); float STRWIDTH(float c) { return c * CHAR_SPACING.x; } float STRHEIGHT(float c) { return c * CHAR_SPACING.y; } #define NORMAL 0 #define INVERT 1 #define UNDERLINE 2 int TEXT_MODE = NORMAL; vec4 ch_spc = vec4(0x000000,0x000000,0x000000,0x000000); vec4 ch_exc = vec4(0x003078,0x787830,0x300030,0x300000); vec4 ch_quo = vec4(0x006666,0x662400,0x000000,0x000000); vec4 ch_hsh = vec4(0x006C6C,0xFE6C6C,0x6CFE6C,0x6C0000); vec4 ch_dol = vec4(0x30307C,0xC0C078,0x0C0CF8,0x303000); vec4 ch_pct = vec4(0x000000,0xC4CC18,0x3060CC,0x8C0000); vec4 ch_amp = vec4(0x0070D8,0xD870FA,0xDECCDC,0x760000); vec4 ch_apo = vec4(0x003030,0x306000,0x000000,0x000000); vec4 ch_lbr = vec4(0x000C18,0x306060,0x603018,0x0C0000); vec4 ch_rbr = vec4(0x006030,0x180C0C,0x0C1830,0x600000); vec4 ch_ast = vec4(0x000000,0x663CFF,0x3C6600,0x000000); vec4 ch_crs = vec4(0x000000,0x18187E,0x181800,0x000000); vec4 ch_com = vec4(0x000000,0x000000,0x000038,0x386000); vec4 ch_dsh = vec4(0x000000,0x0000FE,0x000000,0x000000); vec4 ch_per = vec4(0x000000,0x000000,0x000038,0x380000); vec4 ch_lsl = vec4(0x000002,0x060C18,0x3060C0,0x800000); vec4 ch_0 = vec4(0x007CC6,0xD6D6D6,0xD6D6C6,0x7C0000); vec4 ch_1 = vec4(0x001030,0xF03030,0x303030,0xFC0000); vec4 ch_2 = vec4(0x0078CC,0xCC0C18,0x3060CC,0xFC0000); vec4 ch_3 = vec4(0x0078CC,0x0C0C38,0x0C0CCC,0x780000); vec4 ch_4 = vec4(0x000C1C,0x3C6CCC,0xFE0C0C,0x1E0000); vec4 ch_5 = vec4(0x00FCC0,0xC0C0F8,0x0C0CCC,0x780000); vec4 ch_6 = vec4(0x003860,0xC0C0F8,0xCCCCCC,0x780000); vec4 ch_7 = vec4(0x00FEC6,0xC6060C,0x183030,0x300000); vec4 ch_8 = vec4(0x0078CC,0xCCEC78,0xDCCCCC,0x780000); vec4 ch_9 = vec4(0x0078CC,0xCCCC7C,0x181830,0x700000); vec4 ch_col = vec4(0x000000,0x383800,0x003838,0x000000); vec4 ch_scl = vec4(0x000000,0x383800,0x003838,0x183000); vec4 ch_les = vec4(0x000C18,0x3060C0,0x603018,0x0C0000); vec4 ch_equ = vec4(0x000000,0x007E00,0x7E0000,0x000000); vec4 ch_grt = vec4(0x006030,0x180C06,0x0C1830,0x600000); vec4 ch_que = vec4(0x0078CC,0x0C1830,0x300030,0x300000); vec4 ch_ats = vec4(0x007CC6,0xC6DEDE,0xDEC0C0,0x7C0000); vec4 ch_A = vec4(0x003078,0xCCCCCC,0xFCCCCC,0xCC0000); vec4 ch_B = vec4(0x00FC66,0x66667C,0x666666,0xFC0000); vec4 ch_C = vec4(0x003C66,0xC6C0C0,0xC0C666,0x3C0000); vec4 ch_D = vec4(0x00F86C,0x666666,0x66666C,0xF80000); vec4 ch_E = vec4(0x00FE62,0x60647C,0x646062,0xFE0000); vec4 ch_F = vec4(0x00FE66,0x62647C,0x646060,0xF00000); vec4 ch_G = vec4(0x003C66,0xC6C0C0,0xCEC666,0x3E0000); vec4 ch_H = vec4(0x00CCCC,0xCCCCFC,0xCCCCCC,0xCC0000); vec4 ch_I = vec4(0x007830,0x303030,0x303030,0x780000); vec4 ch_J = vec4(0x001E0C,0x0C0C0C,0xCCCCCC,0x780000); vec4 ch_K = vec4(0x00E666,0x6C6C78,0x6C6C66,0xE60000); vec4 ch_L = vec4(0x00F060,0x606060,0x626666,0xFE0000); vec4 ch_M = vec4(0x00C6EE,0xFEFED6,0xC6C6C6,0xC60000); vec4 ch_N = vec4(0x00C6C6,0xE6F6FE,0xDECEC6,0xC60000); vec4 ch_O = vec4(0x00386C,0xC6C6C6,0xC6C66C,0x380000); vec4 ch_P = vec4(0x00FC66,0x66667C,0x606060,0xF00000); vec4 ch_Q = vec4(0x00386C,0xC6C6C6,0xCEDE7C,0x0C1E00); vec4 ch_R = vec4(0x00FC66,0x66667C,0x6C6666,0xE60000); vec4 ch_S = vec4(0x0078CC,0xCCC070,0x18CCCC,0x780000); vec4 ch_T = vec4(0x00FCB4,0x303030,0x303030,0x780000); vec4 ch_U = vec4(0x00CCCC,0xCCCCCC,0xCCCCCC,0x780000); vec4 ch_V = vec4(0x00CCCC,0xCCCCCC,0xCCCC78,0x300000); vec4 ch_W = vec4(0x00C6C6,0xC6C6D6,0xD66C6C,0x6C0000); vec4 ch_X = vec4(0x00CCCC,0xCC7830,0x78CCCC,0xCC0000); vec4 ch_Y = vec4(0x00CCCC,0xCCCC78,0x303030,0x780000); vec4 ch_Z = vec4(0x00FECE,0x981830,0x6062C6,0xFE0000); vec4 ch_lsb = vec4(0x003C30,0x303030,0x303030,0x3C0000); vec4 ch_rsl = vec4(0x000080,0xC06030,0x180C06,0x020000); vec4 ch_rsb = vec4(0x003C0C,0x0C0C0C,0x0C0C0C,0x3C0000); vec4 ch_pow = vec4(0x10386C,0xC60000,0x000000,0x000000); vec4 ch_usc = vec4(0x000000,0x000000,0x000000,0x00FF00); vec4 ch_a = vec4(0x000000,0x00780C,0x7CCCCC,0x760000); vec4 ch_b = vec4(0x00E060,0x607C66,0x666666,0xDC0000); vec4 ch_c = vec4(0x000000,0x0078CC,0xC0C0CC,0x780000); vec4 ch_d = vec4(0x001C0C,0x0C7CCC,0xCCCCCC,0x760000); vec4 ch_e = vec4(0x000000,0x0078CC,0xFCC0CC,0x780000); vec4 ch_f = vec4(0x00386C,0x6060F8,0x606060,0xF00000); vec4 ch_g = vec4(0x000000,0x0076CC,0xCCCC7C,0x0CCC78); vec4 ch_h = vec4(0x00E060,0x606C76,0x666666,0xE60000); vec4 ch_i = vec4(0x001818,0x007818,0x181818,0x7E0000); vec4 ch_j = vec4(0x000C0C,0x003C0C,0x0C0C0C,0xCCCC78); vec4 ch_k = vec4(0x00E060,0x60666C,0x786C66,0xE60000); vec4 ch_l = vec4(0x007818,0x181818,0x181818,0x7E0000); vec4 ch_m = vec4(0x000000,0x00FCD6,0xD6D6D6,0xC60000); vec4 ch_n = vec4(0x000000,0x00F8CC,0xCCCCCC,0xCC0000); vec4 ch_o = vec4(0x000000,0x0078CC,0xCCCCCC,0x780000); vec4 ch_p = vec4(0x000000,0x00DC66,0x666666,0x7C60F0); vec4 ch_q = vec4(0x000000,0x0076CC,0xCCCCCC,0x7C0C1E); vec4 ch_r = vec4(0x000000,0x00EC6E,0x766060,0xF00000); vec4 ch_s = vec4(0x000000,0x0078CC,0x6018CC,0x780000); vec4 ch_t = vec4(0x000020,0x60FC60,0x60606C,0x380000); vec4 ch_u = vec4(0x000000,0x00CCCC,0xCCCCCC,0x760000); vec4 ch_v = vec4(0x000000,0x00CCCC,0xCCCC78,0x300000); vec4 ch_w = vec4(0x000000,0x00C6C6,0xD6D66C,0x6C0000); vec4 ch_x = vec4(0x000000,0x00C66C,0x38386C,0xC60000); vec4 ch_y = vec4(0x000000,0x006666,0x66663C,0x0C18F0); vec4 ch_z = vec4(0x000000,0x00FC8C,0x1860C4,0xFC0000); vec4 ch_lpa = vec4(0x001C30,0x3060C0,0x603030,0x1C0000); vec4 ch_bar = vec4(0x001818,0x181800,0x181818,0x180000); vec4 ch_rpa = vec4(0x00E030,0x30180C,0x183030,0xE00000); vec4 ch_tid = vec4(0x0073DA,0xCE0000,0x000000,0x000000); vec4 ch_lar = vec4(0x000000,0x10386C,0xC6C6FE,0x000000); //Extracts bit b from the given number. //Shifts bits right (num / 2^bit) then ANDs the result with 1 (mod(result,2.0)). float extract_bit(float n, float b) { b = clamp(b,-1.0,24.0); return floor(mod(floor(n / pow(2.0,floor(b))),2.0)); } //Returns the pixel at uv in the given bit-packed sprite. float sprite(vec4 spr, vec2 size, vec2 uv) { uv = floor(uv); #if defined(FLIP_Y) // Invert y uv coordinate uv.y = 1 - uv.y; #endif // FLIP_Y //Calculate the bit to extract (x + y * width) (flipped on x-axis) float bit = (size.x-uv.x-1.0) + uv.y * size.x; //Clipping bound to remove garbage outside the sprite\u0026#39;s boundaries. bool bounds = all(greaterThanEqual(uv,vec2(0))) \u0026amp;\u0026amp; all(lessThan(uv,size)); float pixels = 0.0; pixels += extract_bit(spr.x, bit - 72.0); pixels += extract_bit(spr.y, bit - 48.0); pixels += extract_bit(spr.z, bit - 24.0); pixels += extract_bit(spr.w, bit - 00.0); return bounds ? pixels : 0.0; } //Prints a character and moves the print position forward by 1 character width. float print_char(vec4 ch, vec2 uv, inout vec2 print_pos) { if( TEXT_MODE == INVERT ) { //Inverts all of the bits in the character. ch = pow(2.0,24.0)-1.0-ch; } if( TEXT_MODE == UNDERLINE ) { //Makes the bottom 8 bits all 1. //Shifts the bottom chunk right 8 bits to drop the lowest 8 bits, //then shifts it left 8 bits and adds 255 (binary 11111111). ch.w = floor(ch.w/256.0)*256.0 + 255.0; } float px = sprite(ch, CHAR_SIZE, uv - print_pos); print_pos.x += CHAR_SPACING.x; return px; } //Returns the digit sprite for the given number. vec4 get_digit(float d) { d = floor(d); if(d == 0.0) return ch_0; if(d == 1.0) return ch_1; if(d == 2.0) return ch_2; if(d == 3.0) return ch_3; if(d == 4.0) return ch_4; if(d == 5.0) return ch_5; if(d == 6.0) return ch_6; if(d == 7.0) return ch_7; if(d == 8.0) return ch_8; if(d == 9.0) return ch_9; return vec4(0.0); } //Prints out the given number starting at pos. float print_number(float number, vec2 uv, inout vec2 print_pos) { float result = 0.0; for(int i = 3;i \u0026gt;= -1;i--) { float digit = mod( number / pow(10.0, float(i)) , 10.0); if(i == -1) //Add a decimal point. { result += print_char(ch_per,uv, print_pos); } if(abs(number) \u0026gt; pow(10.0, float(i)) || i == 0) //Clip off leading zeros. { result += print_char(get_digit(digit),uv, print_pos); } } return result; } vec4 get_digit(float number, int position) { float digit = mod( number / pow(10.0, float(position)) , 10.0); return get_digit( digit ); } float print_integer(float number, int zeros, vec2 uv, inout vec2 print_pos) { float result = 0.0; for(int i = MAX_INT_DIGITS;i \u0026gt;= 0;i--) { float digit = mod( number / pow(10.0, float(i)) , 10.0); if(abs(number) \u0026gt; pow(10.0, float(i)) || zeros \u0026gt; i || i == 0) //Clip off leading zeros. { result += print_char(get_digit(digit),uv, print_pos); } } return result; } It is a little code intensitve, but the gist is the same: decompress the packed infomration to know if the current pixel falls into the compressed font character or not!\nConclusion I will probably add the code to my DataDrivenRendering repository, but in the meantime I wanted to write this article.\nThe general idea should be simple, the execution can be improved a lot but the help can be great to debug GPU only systems, that are becoming more and more used.\nMaybe this is the start of a GPU-Driven ImGui system ? :p\nHope you enjoy!\n","date":1635269659,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635269659,"objectID":"37e4924cd2996cd5fc89c7098ca000cd","permalink":"https://jorenjoestar.github.io/post/gpu_driven_text/gpu_driven_text/","publishdate":"2021-10-26T19:34:19+02:00","relpermalink":"/post/gpu_driven_text/gpu_driven_text/","section":"post","summary":"Overview Recently on Twitter there was an interesting conversation about GPU-Driven line rendering.\nThis reminded me of a system that I use to render values that live only on the GPU, like GPU VFX: it gives you the possibility to output text and values from shaders!\nNormally these values can be mapped and read back into the GPU, but sometimes can be easier to just write them from a shader.","tags":[],"title":"Gpu Driven Text","type":"post"},{"authors":[],"categories":[],"content":"Overview We will go through all the logical steps (and the code!) to write a binary serializer that supports versioning and memory mappability. It is not an automatic process, and it will not contain any data definition schema.\nAs use cases, we have:\nCreation and reading of game resources/assets Game world states (\u0026ldquo;savegames\u0026rdquo;) Networking transmission/reception Not having the schema is a problem, and we will try to go around it in the best way possible.\nFollowing is a simple list of terms used in the article, explained in depth in the following sections but useful to have here:\nSerialization: writing to blob from a data structure, or reading from the blob to fill a data structure Blob: contiguous section of memory Versioning: data with version to skip parts of the serialization Memory mappability: ability to load a blob and use it without any processing Code can be found at my github.com/datadrivenrendering repo.\nSource and Data.\nThe Problem What is serialization ? Using a simple definition from Wikipedia:\nIn computing, serialization (US spelling) or serialisation (UK spelling) is the process of translating a data structure or object state into a format that can be stored (for example, in a file or memory data buffer) or transmitted (for example, over a computer network) and reconstructed later (possibly in a different computer environment).\nWe are about to start a journey of data structure conversion.\nThere are also two requirements for this system, one strong (versioning) and the other soft (not always possible, the memory mappability).\nWhen converting two different binaries without a schema, we still need to have some sort of structure.\nWe will rely on the binary itself to reconstruct itself.\nBlob The first concept to get acquainted with is the blob, and a great article is from our friends at the then BitSquid, now working on OurMachinery.\nA blob is a contiguous section of memory, that is movable, copiable and self-contained.\nWe can pass the blob around, and used to build at runtime data structures that we need, even with some complexity.\nWe will see how later on.\nVersioning Versioning is the strong requirement for this system: we always want to support different versions of binaries.\nThis makes the other requirement, memory mappability, somewhat not always achievable.\nWe will see also that later on.\nThe main inspiration is from Media Molecule Serialization Article (thanks to Oswald Hurlem for this article!), that explains very well how to create a binary versioning serialization system.\nThe gist of it is to create a global version and use that to skip or not some parts of the binary generated.\nIt seems too good and easy to be true, and actually the devil\u0026rsquo;s in the details, and we will see it!\nMemory Mappability The soft requirement, something we would like to have but not always possible.\nThe \u0026lsquo;why\u0026rsquo; we want this feature is because at runtime, we would like to have data in its final form so we just need to use it, without doing more adjustements.\nThe combination of blob and relative data structures can give you a ready to go binary, that can contain pointers, arrays and strings (and more!).\nIt is not all easy, as when binary versions differ we will need still to manually serialize the structures. But more on that later!\nThe idea of memory mappability for me comes from the incredible implementation done by Sony Santa Monica for God of War - I was exposed to the genius ideas of the original creator of the SMScheme, and was honestly blown away. This should be the gold standard for serialization in my opinion, but I digress!\nThe secret weapon here are some data structures called relative data structures, which I found here in a little more detail and found them very promising, used also in SmSchema as far as I understand, even though in much better way.\nThe idea of relative data structures is simple:\nAnything that contians a pointer, translates the pointer to be an offset from\u0026hellip;itself.\nIn C++ lingo, the address of the data pointed is (this) + offset.\nIt is genius that you need only to store the offset, as the this is stored for you by the language!\nWhen the offset is 0, the pointer is considered null.\nThree data structures have been implemented in this way:\nRelative Pointer Relative Array Relative String They work perfectly with the blob of memory we will use as main tool to achieve this.\nWhat is really interesting is that even a normal array can be turned into a Relative Array, and it becomes a really powerful tool.\nWe will see how in the code.\nSerializing, Allocating, Reading, Writing We need to clear some terms to finally start to see the solution/implementation.\nThey are all interwined, and honestly they are what required me to rewrite this serialization code few times before understanding better what I am talking about. I still feel I am not precise enough with words, so any feedback is more than appreciated!\nReading and Writing change the process of Serializing and Allocating in different ways, so we better be precise.\nReading:\nSerializing is from Blob to Runtime Data. Allocating is for the Runtime part for dynamic structures, or reading from the Blob for Relative Data Structures. Writing:\nSerializing is from Runtime Data to Blob. Allocating is reserving memory into the Blob. The Solution We can finally start seeing how we implemented this serialization system.\nThe process will be particular, a mix depth first and breath first for both allocation and serialization.\nStarting from the root data structure, we will visit each member, serialize it and if needed allocate memory from it.\nAllocating memory can be both in-blob memory or runtime memory, depending on the situation.\nWhen writing, it will be the in-blob memory. When reading, it could be just moving into the blob memory (for relative structures) or allocating runtime data (for dynamic arrays and such).\nOne of the strengths of this approach is that if we use all relative data structures we can allocate once and just memory map everything.\nFor this reason when we write the binary data, we need to leave traces to read the proper memory from the blob, when reading.\nRemember, we don\u0026rsquo;t have any schema so we need to rely on the serialization process to guide us through the bytes.\nSerialization: Write We will start with writing the blob.\nI\u0026rsquo;ve attempted at some diagrams to show a more step-by-step mechanism, so the algorithm can be a little clearer.\nIn figure 1 we see the runtime data we want to write into our blob.\nThe data structure contains an array, to show how the algorithm behaves with dynamically allocated memory.\nWe always have a serialization offset and an allocation offset.\nWe always allocate from the current end of the blob.\nThe serialization offset is used to write data into the blob, and can be used to jump around the blob memory, we will see how.\nThe first step to write into the blob is to allocate from the blob memory the blob header, that contains a version and a mappable flag. We will use that later to decide how we read data.\nRuntime data and initial blob Serialization offset is serializing each member of the blob header, like so:\n// Write data into blob memory void MemoryBlob::serialize( u32* data ) { memcpy( \u0026amp;blob_memory[ serialized_offset ], data, sizeof( u32 ) ); serialized_offset += sizeof( u32 ); } After each serialization we move the offset by the correct size.\nNext we allocate the root data structure into the memory blob, and we start serializing its members, as shown in Figure 2:\nAllocated root data structure, but not serialized After some primitive member types, we arrive at an array.\nIn Figure 3, we started serializing the array itself (its struct memory is part of the root data structure), but we miss the array data:\nAllocated array data, array struct serialization We finally allocate the array data and jump serialization to this new location, so we can start serializing each element, as you see from serialization offset:\nArray data serialization Once we finished with the array, we store the serialization offset so we can resume the serialization of the other root data structure members:\nJump back to serialization of root data structure In a nutshell this is all the algorithm to serialize an arbitrarly complex network of data structures.\nIn conjunction with relative data structures, a blob can point to its own memory and enable pointers and arrays to be used without any patch-up.\nSerializing Read Not suprisingly reading is very similar to writing, but the source and destination of the operations are inverted: we read from the blob and write into the runtime data.\nThere is one huge caveat: mappable blobs.\nIn that case we don\u0026rsquo;t need any serialization process, but instead we cast memory to the root data structure and it all works!\nIn order for that to work, for each data structure that points to other parts of the blob, we need two things:\nSave an offset to read from the blob A runtime conversion mechanism between the offset and the needed type. Let\u0026rsquo;s see an example of that, the Relative Pointer class.\n// // RelativePointer //////////////////////////////////////////////////////// // template \u0026lt;typename T\u0026gt; struct RelativePointer { T* get() const; bool is_equal( const RelativePointer\u0026amp; other ) const; bool is_null() const; bool is_not_null() const; // Operator overloading to give a cleaner interface T* operator-\u0026gt;() const; T\u0026amp; operator*() const; void set( char* raw_pointer ); void set_null(); i32 offset; }; // struct RelativePointer As we can see we save an offset, but the real power comes from the operator overload.\nLet\u0026rsquo;s see the implementation:\n// RelativePointer //////////////////////////////////////////////////////// template\u0026lt;typename T\u0026gt; inline T* RelativePointer\u0026lt;T\u0026gt;::get() const { // For debugging purposes leave the address variable. char* address = ( ( char* )\u0026amp;this-\u0026gt;offset ) + offset; return offset != 0 ? ( T* )address : nullptr; } template\u0026lt;typename T\u0026gt; inline T* RelativePointer\u0026lt;T\u0026gt;::operator-\u0026gt;() const { return get(); } template\u0026lt;typename T\u0026gt; inline T\u0026amp; RelativePointer\u0026lt;T\u0026gt;::operator*() const { return *( get() ); } The genius idea, coming from the talk I\u0026rsquo;ve written before, comes from using the current memory, the offset member variable, to give a reference point in memory, and adding the content of the offset itself to retrieve the memory!\nchar* address = ( ( char* )\u0026amp;this-\u0026gt;offset ) + offset;\nEmploying this simple trick, you can now point to any part of the blob in a transparent way.\nEven for a dynamic array implementation, we can add a similar method and reuse its data to use it as memory mappable array:\n// Relative data access. template\u0026lt;typename T\u0026gt; inline T* Array\u0026lt;T\u0026gt;::get() { if ( relative ) { char* address = ( ( char* )\u0026amp;size ) + capacity; return capacity != 0 ? ( T* )address : nullptr; } return nullptr; } Writing Serialization Code In this section we will go through some examples of data structures to explain the read and write code written.\nBut first, how do we write our custom serialization code ?\nSerializing user data structures Let\u0026rsquo;s see a very simple example, a vector2 struct:\n// Vec2s ////////////////////////////////////////////////////////////////// struct vec2s { f32 x; f32 y; }; // Serialization template\u0026lt;\u0026gt; void MemoryBlob::serialize\u0026lt;vec2s\u0026gt;( vec2s* data ) { serialize( \u0026amp;data-\u0026gt;x ); serialize( \u0026amp;data-\u0026gt;y ); } In the MemoryBlob struct, we have a method that we will use with template specialization to actually specialize the serialization code path.\nRemember, there is no schema so we use the data structure itself to guide the serialization.\nThe method that we use with template specialization is this:\ntemplate\u0026lt;typename T\u0026gt; inline void MemoryBlob::serialize( T* data ) { // Should not arrive here! hy_assert( false ); } Thus we need to implement our own version.\nTemplate Specialization Caveat The correct way to use template specialization without template errors, is to define in an header the following method:\ntemplate\u0026lt;\u0026gt; void hydra::MemoryBlob::serialize\u0026lt;vec2s\u0026gt;( vec2s* data ); And then in a cpp the specialized version.\nVersioning Let\u0026rsquo;s see the serialization code of a data structure with versioning:\n// // struct EntityBlueprint { RelativeString name; u32 v1_padding; // Added to test different versions. RelativePointer\u0026lt;RenderingBlueprint\u0026gt; rendering; RelativePointer\u0026lt;AnimationBlueprint\u0026gt; animation; vec2s position; f32 offset_z; }; // struct EntityBlueprint template\u0026lt;\u0026gt; void hydra::MemoryBlob::serialize\u0026lt;EntityBlueprint\u0026gt;( EntityBlueprint* data ) { serialize( \u0026amp;data-\u0026gt;name ); if ( serializer_version \u0026gt; 0 ) serialize( \u0026amp;data-\u0026gt;v1_padding ); serialize( \u0026amp;data-\u0026gt;rendering ); serialize( \u0026amp;data-\u0026gt;animation ); if ( serializer_version \u0026gt; 1 ) { serialize( \u0026amp;data-\u0026gt;position ); } else { // When reading, we should still initialize variables to a \u0026#39;valid\u0026#39; state. data-\u0026gt;position = { 0.f, 0.f }; } serialize( \u0026amp;data-\u0026gt;offset_z ); } This is a test data structure I was using to try the serialization system itself.\nIt serializes an Entity into a Scene.\nI\u0026rsquo;ve added some padding as test, but then the position as real need for a second version.\nLike the Little Big Planet serialization system, the data structure will contain all the members of all its history.\nIt is the serializer code itself that will jump the members not needed.\nSerializing relative pointers Let\u0026rsquo;s see the writing code of the Relative Pointers:\ntemplate\u0026lt;typename T\u0026gt; void MemoryBlob::serialize( RelativePointer\u0026lt;T\u0026gt;* data ) { //... { // WRITING! // Data --\u0026gt; Blob // Calculate offset used by RelativePointer. // Remember this: // char* address = ( ( char* )\u0026amp;this-\u0026gt;offset ) + offset; // Serialized offset points to what will be the \u0026#34;this-\u0026gt;offset\u0026#34; // Allocated offset points to the still not allocated memory, // Where we will allocate from. i32 data_offset = allocated_offset - serialized_offset; serialize( \u0026amp;data_offset ); // To jump anywhere and correctly restore the serialization process, // cache the current serialization offset u32 cached_serialized = serialized_offset; // Move serialization to the newly allocated memory at the end of the blob. // Serialization is where we WRITE code! serialized_offset = allocated_offset; // Allocate memory in the blob allocate_static\u0026lt;T\u0026gt;(); // Serialize/visit the pointed data structure serialize( data-\u0026gt;get() ); // Restore serialized serialized_offset = cached_serialized; } } Code is heavily commented to help understanding what is happening.\nThe reading code is as follows:\n// READING! // Blob --\u0026gt; Data i32 source_data_offset; serialize( \u0026amp;source_data_offset ); // Early out to not follow null pointers. if ( source_data_offset == 0 ) { data-\u0026gt;offset = 0; return; } // This is the important bit. // Use the offset to the passed member variable to calculate the DESTINATION offset // to write to. data-\u0026gt;offset = get_relative_data_offset( data ); // Allocate memory and set pointer allocate_static\u0026lt;T\u0026gt;(); // Cache source serialized offset. u32 cached_serialized = serialized_offset; // Move serialization offset. // The offset is still \u0026#34;this-\u0026gt;offset\u0026#34;, and the serialized offset // points just right AFTER it, thus move back by sizeof(offset). // Serialization is where READ from in the blob! serialized_offset = cached_serialized + source_data_offset - sizeof(u32); // Serialize/visit the pointed data structure, using the offset calculated above. serialize( data-\u0026gt;get() ); // Restore serialization offset serialized_offset = cached_serialized; There is a method here that is very important: get_relative_data_offset.\nThis highlights how we can use the struct itself to guide the serialization:\ni32 MemoryBlob::get_relative_data_offset( void* data ) { // data_memory points to the newly allocated data structure to be used at runtime. const i32 data_offset_from_start = ( i32 )( ( char* )data - data_memory ); const i32 data_offset = allocated_offset - data_offset_from_start; return data_offset; } When we read, we are writing into some data structure, that can differ from the binarized data.\nLet\u0026rsquo;s say the binary has some missing fields (an older version), we need to calculate the writing offset based on our code.\nSo first we get the offset of the passed variable from the start of the memory, then we calculate the offset to the data memory that will be allocated shortly after.\nWith this code, we actually showed the algorithm that, with little modifications, can be adapted to any data structure.\nNOTE: something that really confused me at the beginning was the usage of data, serialization offset and allocation offset.\nThe fact is that they change meaning when we are reading and when we are writing, so we need to mentally picture it to really understand how they are used.\nI tried to add comments to help remembering this.\nSerializing relative arrays and dynamic arrays Relative Arrays are very similar to Relative Pointers, but they just store more data and contain a size member.\n... allocate_static( data-\u0026gt;size * sizeof( T ) ); for ( u32 i = 0; i \u0026lt; data-\u0026gt;size; ++i ) { T* data = \u0026amp;data-\u0026gt;get()[ i ]; serialize( data ); } The only real difference is that we iterate through all the members by calling their serialize method.\nSpecial: writing blob from a json file Something we might need to do, especially in a build step, is to convert from a non-binary format to our blob.\nThis is something done a lot in games, so that the final binary format is as fast to use as possible, compared to always parsing a json (or other formats) and doing some work on the loaded data.\nI\u0026rsquo;ve added an example of writing a json file containing commands for a cutscene system, and they use a different way of writing the blob.\nSo far we\u0026rsquo;ve only seen the passing of an already binary version of data, to be compacted in a blob.\nIn this case we are performing a real conversion.\nLet\u0026rsquo;s start with the json (available under data\\articles\\SerializationDemo\\cutscene.json) describing the cutscene:\n{ \u0026#34;name\u0026#34;:\u0026#34;new_game\u0026#34;, \u0026#34;scene\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;commands\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;fade\u0026#34;, \u0026#34;start\u0026#34; : 0.0, \u0026#34;end\u0026#34; : 1.0, \u0026#34;duration\u0026#34; : 0.0 }, { \u0026#34;type\u0026#34; : \u0026#34;move_entity\u0026#34;, \u0026#34;x\u0026#34; : -16, \u0026#34;y\u0026#34; : -8, \u0026#34;instant\u0026#34; : true, \u0026#34;entity_name\u0026#34; : \u0026#34;cronos_mum\u0026#34; }, { \u0026#34;type\u0026#34; : \u0026#34;dialogue\u0026#34;, \u0026#34;text\u0026#34; : \u0026#34;{SPEED=0.025}Crono...{PAGE}Crono!{PAGE}Crono, are you still sleeping?{PAGE} \u0026#34; } ] } We have different commands that needs to be parsed and converted in a binary format.\nAs you probably spotted, I was using this serialization system with Chrono Trigger in mind as something to clone, and even if I did not cloned the whole game, I could test a cutscene and some gameplay informations to be used.\nAnyway, let\u0026rsquo;s see the conversion code.\nFirst, the main data structures:\n// // struct CutsceneEntry { CutsceneCommandType type; RelativeArray\u0026lt;u8\u0026gt; data; }; // struct CutsceneEntry // // struct CutsceneBlueprint { RelativeArray\u0026lt;CutsceneEntry\u0026gt; entries; static constexpr u32 k_version = 0; }; // struct CutsceneBlueprint I should change the names maybe, but the root data structure is the CutsceneBlueprint. It contians just an array of entries, each one with some data associated and a type.\nFirst, we create the blob and allocate a fixed size (yes, it should be dynamic, I know!):\nMemoryBlob blob; blob.write\u0026lt;CutsceneBlueprint\u0026gt;( allocator, 0, blob_size, nullptr ); Passing a \u0026rsquo;nullptr\u0026rsquo; as last argument means we don\u0026rsquo;t have any root data structure ready to be serialized.\nWe instead proceed manually to build the blob.\nIn this case we heavily use allocate and allocate and set methods.\n// Reserve memory for root data structure CutsceneBlueprint* root = blob.allocate_static\u0026lt;CutsceneBlueprint\u0026gt;(); // Allocate array data and set its offset: blob.allocate_and_set( root-\u0026gt;entries, num_entries ); With this code we can already write into the single entries, like this:\n// Declare an empty std::string to convert json strings std::string name_string; // Read the json entries json entries = parsed_json[ \u0026#34;commands\u0026#34; ]; for ( u32 i = 0; i \u0026lt; entries.size; ++i ) { json element = entries[ i ]; // Convert field \u0026#39;type\u0026#39; to std::string element[ \u0026#34;type\u0026#34; ].get_to( name_string ); // Access the allocated array to write into its entries. CutsceneEntry\u0026amp; entry = root-\u0026gt;entries[ i ]; // Yes yes, this can be improved! if ( name_string.compare( \u0026#34;dialogue\u0026#34; ) == 0 ) { element[ \u0026#34;text\u0026#34; ].get_to( name_string ); // Allocate memory for the string + null terminator! char* memory = blob.allocate_static( name_string.size() + 1 ); memcpy( memory, name_string.c_str(), name_string.size() ); memory[ name_string.size() ] = 0; entry.type = CutsceneCommandType::Dialogue; // Calculate the offset for the data RelativeArray of the CutsceneEntry entry.data.set( memory, ( u32 )name_string.size() ); } ... } With this simple code we can see a common pattern when writing into the blob from a non binary input.\nWe allocate the root data structure, and we use it to fill the blob.\nEvery time we need to allocate memory, we do it and then use the newly allocated memory.\nI will leave more examples in the source code, also with pointers and arrays of arrays, but the mindset is this one!\nFor reading this, if we did everything correctly and the data version is the latest, we can simply memory map it and use it.\nConclusion We saw in depth a serialization system that supports memory mappability.\nWe explored the different basic bricks that makes this possible, and presented a couple of examples (and included more in the code) to see different usages.\nWhile not perfect, I believe this could be a great starting point to serializing anything needed into your code.\nAdding a custom serialized type is a matter of adding a method, both for reading and writing.\nI decided to use templates instead of the Little Big Planet C-style way just to have something more modern, but a C version could be used that is very similar.\nBinary compatibility is not 100% safe, I am sure there are edge cases that breaks (like padding between member variables!), but I feel that this is a good start.\nThere are still some things that needs to be solved, like the support for dynamically sized blobs (as reallocating during the serialization invalidates the memory you are using, needing a more careful approach), and the API it\u0026rsquo;s still not very mature - code ergonomy is not high.\nPersonally I already converted hydra fx to use this system and it works like a charm, and I am planning to use it more and more to refine the system better.\nCode can be found at my github.com/datadrivenrendering repo.\nSource and Data.\nAs always, please send any feedback to my twitter.\nHope you enjoyed!\n","date":1628496338,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628496338,"objectID":"b0bdd8710a339b33fee6621a041d5ec5","permalink":"https://jorenjoestar.github.io/post/serialization_for_games/","publishdate":"2021-08-09T10:05:38+02:00","relpermalink":"/post/serialization_for_games/","section":"post","summary":"Overview We will go through all the logical steps (and the code!) to write a binary serializer that supports versioning and memory mappability. It is not an automatic process, and it will not contain any data definition schema.\nAs use cases, we have:\nCreation and reading of game resources/assets Game world states (\u0026ldquo;savegames\u0026rdquo;) Networking transmission/reception Not having the schema is a problem, and we will try to go around it in the best way possible.","tags":[],"title":"Serialization For Games","type":"post"},{"authors":[],"categories":[],"content":" An example of non integer scaling of pixel art. Overview Pixel art is living a beautiful renaissance these last years, with more and more studios embracing its visuals and more artists going back to this unique way of describing visuals in a game.\nOne of the most annoying problem we are seeing, especially since we don\u0026rsquo;t have all the CRT TVs that \u0026lsquo;apply their filters\u0026rsquo;, is the pixel \u0026lsquo;shimmering\u0026rsquo; happening when scaling or moving a pixel art image with non-integer values.\nThis article aims to be a light introduction to the problem and more a practical testbed to experiment with filtering. I found that just using Shadertoy was not enough, and I would like to give programmers a more advanced testbed that they can modify to improve their games!\nSummary/TLDR First of all we will state what is the problem we are trying to solve, even though enough posts already talked about that. There is an intuition that is at the base of all the filters, so it is important to have that in mind.\nThen we will go through each filter I found around, with a simple explanation and link to the original article and code. Hopefully it will be easy to follow!\nFinally some informations about the included source code, so you can experiment with Pixel Art Filtering.\nEnjoy!\nThe Problem What we are trying to fix is to find the relationship between pixel on the screen and pixel of the texture, or texels, to perceptually preserve the original texture appearance.\nOne key component here is that we want to apply any kind of transform to the sprite/mesh that will use this texture and still preserve the texture appearance. Something that can be done withouth further venturing into this realm is to use both integer-only camera movement increments, and scale to only integer number. This works only for sprite based games, and it can suffice if you don\u0026rsquo;t add any zoom or rotation to the game. Many indie games, that are beautifully 2D and sprite based, can simply do that if no zoom (thus scaling) or rotation is applied.\nBut if you are wanting to add something more dynamic then it can become a problem!\nExamples: Samurai Showdown and Children of Morta Already back in the day scaling and rotation were a problem, and artifacts were visible. I can think of Samurai Showdown games when zooming out the camera, and in the quick action of the game, despite the purposely quick zoom, you can see pixel shimmering around.\nA more recent example, that received a lot of attention from Handmade Hero, is Children of Morta (very nice game, go and play it if you haven\u0026rsquo;t!).\nDisclaimer: it is not my intention to talk bad about the game, just to show an example that already suffered for this problem, and maybe, at the end of the article, have a testbed to try different solutions!\nAs you can see also from videos of the game, zooming out (especially done with a slow speed) show the pixel shimmering that causes the problem.\nFor some (unknown to me) reason, some shaders use the term \u0026lsquo;Fat Pixel\u0026rsquo; to refer to this problem, and it will help you finding some solutions on Shadertoy as well!\nPixels and Texels (and Fat Pixels) To simplify the problem, we will look at sprite based games, so no 3D rotations, but the solution can be generalized to 3D as well.\nI found this article to have the easiest explanation of the problem we are trying to solve, but I will restate it so it is clear visually inside this article as well.\nThe appearance we want to preserve is the one of a Pixel Art texture, so we want pixel of the texture, that we will call Texels to be distinguishable from each other. We thus need to apply some sort of filtering to the texture when rendering it on the screen.\nLet\u0026rsquo;s see a simple case: an integer scale:\n2X Scale. Here is clear that the pixels will be 1 to 1 with the texels, thus maintaining the pixel art visuals. NOTE: using bilinear will add blurriness at the edges though, so normally integer scaling and translation need a neighbour/nearest filter.\nAnd now let\u0026rsquo;s see a non integer scale with some translation applied as well:\n1.5X Scale and Translation. The situation is worst. Some pixels are full while some others are edges. I tried to highlight the original texture (a simple quad) with grey outline and color the pixels in the fully filled pixels.\nWe see that only 4 pixels are fully colored, the others all need some sort of filter for the edges.\nFinally, some scale and rotation as well:\n1.5X Scale, Translation and Rotation. This is the most difficult situation. Only 3 pixels are full and everything else needs a proper edge management.\nIt is clear (I hope!) that using only bilinear filter is not enough: the fact that each pixel can always take up 4 samples smudges everything, and we lose exactly what we want: sharp pixel art!\nSame for neighbour/nearest: on fully contained pixels-in-texels it works, but on edges it is straight wrong. This causes non-square texels to fill the image.\nWe need a combination of both!\nThe Intuition The idea behind the filtering is simple: apply neighbour/nearest filter on pixels that are fully inside a texel, and use bilinear with custom weights at the edges!\nThis is what is all about.\nThe tricky part is to come out with some custom weights for the bilinear filter to work out, and that is why there are so many different filters.\nWe will now see the different filters included in the testbed. Nothing that I wrote, so I will include links to the original material and you can see the awesomeness of the authors!\nThe Filters Here we analyze the different filters proposed and separate their code. All the shaders are into the DataDrivenRendering/data/articles/PixelArtFiltering/pixel_art_filtering.hfx file.\nNOTE!: textures are sampled with Bilinear filtering and Alpha Blending is enabled, so that edges can be smoother. This is very important for the final results!\nAll the filters will give back a modified UV that will leverage the bilinear sampling.\nNearest Even though is not a part of the \u0026lsquo;solution\u0026rsquo;, it is interesting to see how we can obtain manually the nearest filter coordinates, despite having enabled bilinear filtering.\nTaken directly from the OpenGL Specification (page 281):\nOpenGL nearest formula. In the footer, the idea we are using is explained:\nImplementations may instead round the texture layer using the nearly equivalent computation |value + 1/2|.\nWe need to search the Manhattan distance from the texel center, thus using floor(pixel) + 0.5 works perfectly.\nvec2 uv_nearest( vec2 uv, ivec2 texture_size ) { vec2 pixel = uv * texture_size; pixel = floor(pixel) + .5; return pixel / texture_size; } CSantos Claudio Santos (github) in his blog already in 2014 tackled the problem, as we can see from the following blog posts:\nmanual-texture-filtering-for-pixelated-games-in-webgl automatically-detecting-the-texture-filter-threshold-for-pixelated-magnifications He was searching for a way to modify the UV closer to the texel seam, and came up with his own way.\nIn order to have smooth transitions between texels, this offset should be replaced by a function that increases linearly at the margin of the texel, remains constant at its “blocky” region (with a value of 0.5) and then increases to 1.0 on the opposite margin of the texel.\nHe mathematically found the 2 functions to add close to the \u0026lsquo;minimum\u0026rsquo; seam (towards the 0 of the texel) and towards the \u0026lsquo;maximum\u0026rsquo; seam (towards 1):\nclamp( .5 / alpha * pixels_fract, 0, .5 ) clamp( .5 / alpha * (pixels_fract - 1) + .5, 0, .5 ) In the second blog post, he found also that using fwidth on the texel coordinates give you a resolution independent value that can be used to have a more consistent look:\nvec2 uv_cstantos( vec2 uv, vec2 res ) { vec2 pixels = uv * res; // Updated to the final article vec2 alpha = 0.7 * fwidth(pixels); vec2 pixels_fract = fract(pixels); vec2 pixels_diff = clamp( .5 / alpha * pixels_fract, 0, .5 ) + clamp( .5 / alpha * (pixels_fract - 1) + .5, 0, .5 ); pixels = floor(pixels) + pixels_diff; return pixels / res; } We will see in many other filters that the usage of fwidth is crucial to work with resolution independent filtering.\nFor more informations on fwidth:\nhttps://www.khronos.org/registry/OpenGL-Refpages/gl4/html/fwidth.xhtml https://computergraphics.stackexchange.com/questions/61/what-is-fwidth-and-how-does-it-work https://www.ronja-tutorials.com/post/046-fwidth/ Cole Cecil In a similar way, Cole Cecil in this article creates two different functions that define the steepness of the function that interpolation amount.\nvec2 uv_colececil( vec2 uv, ivec2 texture_size ) { vec2 pixel = uv * texture_size; vec2 locationWithinTexel = fract(pixel); // Calculate the inverse of texels_per_pixel so we multiply here instead of dividing. vec2 interpolationAmount = clamp(locationWithinTexel * locals.texels_per_pixel, 0, .5) + clamp((locationWithinTexel - 1) * locals.texels_per_pixel + .5, 0, .5); return (floor(pixel) + interpolationAmount) / texture_size; } The \u0026rsquo;texels_per_pixel\u0026rsquo; variable is calculated on the CPU, and it is basically a ratio between the camera render target size and the screen size, including the possible zoom of it.\nIn the original post from the author it can be found in the comment section.\nI write it down here as reference:\nstatic f32 calculate_texels_per_pixel( f32 camera_width, f32 camera_height, f32 camera_zoom, f32 screen_width, f32 screen_height ) { f32 texels_per_pixel = 1.f; const f32 camera_aspect_ratio = camera_width / camera_height; const f32 screen_aspect_ratio = screen_width / screen_height; if ( screen_aspect_ratio \u0026gt; camera_aspect_ratio ) { texels_per_pixel = camera_height / screen_height; } else { texels_per_pixel = camera_width / screen_width; } // Zoom is inverted compared to ColeCecil post, so we keep same calculation here but in the shader we multiply. return texels_per_pixel / camera_zoom; } RNavega The last filter accompained by a blog post of some sort is from Rodrigo Navega, in the Love2D forum and the shader code.\nAfter the previous filters the mindset is similar, so showing the code should be simpler. We are still calculating the weights based on the distance from the edge, the difference here is that it outputs also an alpha value to be used in conjunction with the texture alpha.\n// The default size, in pixels, of the antialiasing filter. The default is 1.0 for a mathematically perfect // antialias. But if you want, you can increase this to 1.5, 2.0, 3.0 and such to force a bigger antialias zone // than normal, using more screen pixels. float SMOOTH_SIZE = locals.filter_width; float _HALF_SMOOTH = SMOOTH_SIZE / 2.0; vec2 uv_aa_dist( vec2 uv, vec2 res, out float alpha ) { vec2 pixels = uv * res; const vec2 nearest_edge = floor( pixels + 0.5 ); const vec2 edge_distance = (pixels - nearest_edge) * locals.camera_scale; const vec2 factor = clamp( edge_distance / vec2(_HALF_SMOOTH), -1.0, 1.0 ); pixels = (nearest_edge + 0.5 * factor ); const vec2 center_offset = abs(uv - vec2(0.5)); vec2 alpha_distance = ((center_offset - 0.5) * res * locals.camera_scale + _HALF_SMOOTH) / SMOOTH_SIZE; alpha_distance = clamp( alpha_distance, 0, 1 ); alpha = 1.0 - max(alpha_distance.x, alpha_distance.y); return pixels / res; } Remembering that we are using alpha blending as well, we can use this value to output a smoother edge. Also to note that the smooth size can be driven by the filter width in the demo, to experiment with the visuals.\nHere the factor is calculated based on the edge distance from the center texel and clamped between -1 and 1.\nKlems Klems decide to use a smoothstep based approach in the shader:\nvec2 uv_klems( vec2 uv, ivec2 texture_size ) { vec2 pixels = uv * texture_size + 0.5; // tweak fractional value of the texture coordinate vec2 fl = floor(pixels); vec2 fr = fract(pixels); vec2 aa = fwidth(pixels) * 0.75; fr = smoothstep( vec2(0.5) - aa, vec2(0.5) + aa, fr); return (fl + fr - 0.5) / texture_size; } It again use fwidth approach to calculate the factor, and use the fractional value to smoothstep between the 2 values that will be used near the edges.\nInigo Quilez (\u0026lt;3) From the comments in the Klems filter, the incredible Inigo comes up with another filter:\nvec2 uv_iq( vec2 uv, ivec2 texture_size ) { vec2 pixel = uv * texture_size; vec2 seam = floor(pixel + 0.5); vec2 dudv = fwidth(pixel); pixel = seam + clamp( (pixel - seam) / dudv, -0.5, 0.5); return pixel / texture_size; } The variable names are quite explicit, but the idea is to again start from the seam of the texture and modify the final uv based on the distance from the seam.\nBlocky This is very similar to IQ filter, just using different math:\n// https://www.shadertoy.com/view/ltfXWS // samples from a linearly-interpolated texture to produce an appearance similar to // nearest-neighbor interpolation, but with resolution-dependent antialiasing // // this function\u0026#39;s interface is exactly the same as texture\u0026#39;s, aside from the \u0026#39;res\u0026#39; // parameter, which represents the resolution of the texture \u0026#39;tex\u0026#39;. // basically calculates the lengths of (a.x, b.x) and (a.y, b.y) at the same time vec2 v2len(in vec2 a, in vec2 b) { return sqrt( a * a + b * b ); } vec2 uv_blocky( in vec2 uv, in ivec2 res ) { vec2 pixels = uv * res; // enter texel coordinate space. vec2 seam = floor(pixels + .5); // find the nearest seam between texels. // here\u0026#39;s where the magic happens. scale up the distance to the seam so that all // interpolation happens in a one-pixel-wide space. pixels = (pixels - seam) / v2len(dFdx(pixels), dFdy(pixels)) + seam; pixels = clamp(pixels, seam - .5, seam + .5); // clamp to the center of a texel. return pixels / res;// convert back to 0..1 coordinate space. } BGolus: Anti-Aliasing Ben Golus also wrote his own implementation here with two variations as well!\nvec2 uv_aa_linear( vec2 uv, vec2 res, float width ) { vec2 pixels = uv * res; vec2 pixels_floor = floor(pixels + 0.5); vec2 pixels_fract = clamp( (pixels - pixels_floor) / fwidth(pixels) / width, -0.5, 0.5); return (pixels_floor + pixel_fract) / res; } vec2 uv_aa_smoothstep( vec2 uv, vec2 res, float width ) { vec2 pixels = uv * res; vec2 pixels_floor = floor(pixels + 0.5); vec2 pixels_fract = fract(pixels + 0.5); vec2 pixels_aa = fwidth(pixels) * width * 0.5; pixels_fract = smoothstep( vec2(0.5) - pixels_aa, vec2(0.5) + pixels_aa, pixels_fract ); return (pixels_floor + pixels_fract - 0.5) / res; } They both use a width to define how big is the area covered by the anti-aliasing on the edges, and in the demo it is a tweakable value (default to 1.5).\nThe first variation it scales using fwidth but also modifies the value with the width, giving the possibility of a more fine tuned control over the edge appearance (pixels - pixels_floor) / fwidth(pixels) / width. This is a unique feature of his filters!\nThe second variation has both the smoothstep as a way of filtering the UV based on the distance from the center of the texel, and a fwidth tunable with the width again as value to move the sampling coordinate. Note that if width is 0, pixel_fract is 0.5, thus being the center of the texel, and in the final conversion back it is taken in consideration.\nFat Pixel This is the shader that was mentioned by Casey Muratori in his video and for me started all this research culminating in this article! The original source code is here but again I post the code straight away:\nvec2 uv_fat_pixel( vec2 uv, ivec2 texture_size ) { vec2 pixel = uv * texture_size; vec2 fat_pixel = floor(pixel) + 0.5; // subpixel aa algorithm (COMMENT OUT TO COMPARE WITH POINT SAMPLING) fat_pixel += 1 - clamp((1.0 - fract(pixel)) * locals.texels_per_pixel, 0, 1); return fat_pixel / texture_size; } Using the same texels_per_pixel number calculated as for the Cole Cecil shader, we basically move the UV based on the distance to the edge. Note that texels_per_pixel if camera and scene have the same size it becomes just the camera zoom - so we are choosing the weight from edge scaled by the zoom factor of the camera.\nTODO: The Maister There is a final filter that I wanted to check, but I will come at it later. I wanted to write about it briefly because there is the most mathematical approach and thus can shed some light also an all the other filters!\nThe most mathematical approach is by Hans Kristian in which he does an analysis of the problem from a signal theory point of view. His blog is full of great posts, so go check it out!\nThe posts that we will see are the following:\npseudo-bandlimited-pixel-art-filtering-in-3d-a-mathematical-derivation why-coverage-based-pixel-art-filtering-is-terrible They are an in-depth signal based analysis of the problem, even though the author kindly added source code as well.\nI will check the shader code in the future, for now the blog post is what really gives a hand at understanding the other filters.\nThe Testbed With the accompanying source code there is included also some graphics to already test the idea presented here. I\u0026rsquo;ve used two textures, that were free to use made available by the amazing Luis Zuno found through his itch.io page.\nIt should be pretty straightforward to modify the shaders and test your own ideas, so I hope you will come up with new filters!\nAll the filters are in DataDrivenRendering/data/articles/PixelArtFiltering/pixel_art_filtering.hfx file.\nThe source is in my github repository both in data/articles/PixelArtfiltering and source/articles/PixelArtFiltering where you can find the shaders and the code. There are basically only two files (pixel_art_filtering.hfx and pixel_art_filtering_app.cpp) that you need to modify to experiment with this concepts.\nScene Setup The scene is composed of an animated sprite and a background.\nThere are different options to zoom in and out and translate, to test all possible scenarios.\nI\u0026rsquo;ve added also a simple CRT filter to be toggled after the scene is rendered.\nAlso, sampling is always bilinear.\nYou can also toggle premultiplied alpha, so you see that not having any alpha blending will result in shimmering edges, another big problem not really mentioned around.\nLinks and Previous Work Here I want to link all the previous posts about the subject for quick reference:\nhttps://csantosbh.wordpress.com/2014/01/25/manual-texture-filtering-for-pixelated-games-in-webgl/ https://csantosbh.wordpress.com/2014/02/05/automatically-detecting-the-texture-filter-threshold-for-pixelated-magnifications/ https://colececil.io/blog/2017/scaling-pixel-art-without-destroying-it/ https://themaister.net/blog/2018/08/25/pseudo-bandlimited-pixel-art-filtering-in-3d-a-mathematical-derivation/ https://love2d.org/forums/viewtopic.php?t=89442 https://github.com/RNavega/PixelArt-Antialias-Love2D/blob/master/main.lua Also ShaderToy is a source of many other filters included in the demo.\nhttps://www.shadertoy.com/view/MllBWf https://www.shadertoy.com/view/ltfXWS https://www.shadertoy.com/view/4dlXzB https://www.shadertoy.com/view/ltcGDX https://www.shadertoy.com/view/ltBGWc https://www.shadertoy.com/view/ltBfRD Conclusions We saw different sampling filters that resulted in different ways of leveraging the bilinear filtering with some custom coordinates to obtain a mix of sharpness for inside pixels and smoothness on the edges.\nWe also have now a easy to modify testbed to try different ideas and hopefully make your pixels (and your eyes) happy!\nAs always, please comment, modify, improve, share the article and the code!\n","date":1618250216,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618250216,"objectID":"524bd59104ae64caf2e459e03963ef02","permalink":"https://jorenjoestar.github.io/post/pixel_art_filtering/","publishdate":"2021-04-12T19:56:56+02:00","relpermalink":"/post/pixel_art_filtering/","section":"post","summary":"An example of non integer scaling of pixel art. Overview Pixel art is living a beautiful renaissance these last years, with more and more studios embracing its visuals and more artists going back to this unique way of describing visuals in a game.\nOne of the most annoying problem we are seeing, especially since we don\u0026rsquo;t have all the CRT TVs that \u0026lsquo;apply their filters\u0026rsquo;, is the pixel \u0026lsquo;shimmering\u0026rsquo; happening when scaling or moving a pixel art image with non-integer values.","tags":[],"title":"Pixel Art Filtering","type":"post"},{"authors":[],"categories":[],"content":"Overview Since growing up I\u0026rsquo;ve always been fascinated by stars, and being exposed to anime like Sainy Seiya and Hokuto No Ken just fueled the passion. My 4th year of high-school had a full year course on \u0026lsquo;geographical astronomy\u0026rsquo; - an in depth look at our planet and the stars from a scientific perspective. Many years has passed (20+!) and I\u0026rsquo;ve never dwelved into these kind of topic.\nThen few years ago, while researching for rendering un Just Cause 4 I stumbled upon a couple of papers about realistic rendering of stars. I did a working prototype in Unity but did not understood many things, and I had no time to look back into this.\nThen came Christmas time, with lockdown and such I finally had an excuse to dwelve deeper into this topic. Also I am searching for little rendering demos I can use to test and cleanup my libraries to write code.\nA HUGE shout to the author of SGHimmel - code that contains a much deeper and precise implementation.\nMine is more a starting point and a small subset of what is needed to render realistic stars - the ones visible with naked eye.\nThere is a seminal paper that put all these informations in one place:\nA Physically Based Night Sky Model. This paper contains all the stars rendering informations (and much more, like Moon and Sun) and it is the real deal.\nA second paper also expanded that and gave us the SGHimmel code:\nSingle Pass Rendering of Day and Night Sky Phenomena.\nMy only contribution is to distill the very complex informations in a few files of code, and maybe help someone else to start looking into this amazing world.\nAstronomy Where do we start ?\nCatalogs We need DATA. In Astronomy, and for stars, there are the so called \u0026lsquo;catalogs\u0026rsquo; - a collection of data relative to stars normally collected by hand (!) by astronomers from different sources. A list of catalogs can be found here:\nhttps://heasarc.gsfc.nasa.gov/docs/cgro/db-perl/W3Browse/w3table.pl?MissionHelp=star_catalog\n\u0026hellip; catalogs types and story ?\nAmongst all the catalogs the most beginner friendly to use is the Yale Bright Stars Catalog. This catalog was created around 1908 and different was updated until \u0026lsquo;recently\u0026rsquo;. It contains all the stars that are visible with naked eye from Earth - 9100 objects - normally visible if the have a visual magnitude of more than 6.5. The online version is both a binary based one and a text based one. I decided to use the binary version, but possibly it will change in the future.\nStar Entry Now that we chose a catalog let\u0026rsquo;s see what we really need to properly place and visualize a star. Using the binary version shows the minimum necessary data that can be used, and in this case following this link:\nRight Ascension and Declination Spectral Type Visual Magnitude Proper Motion Before dwelving into this, an incredibly important thing to consider is WHEN the catalog is compiled - more specifically what time reference point is used in the catalog.\nEpoch, Julian Dates and J2000 TLDR: convert from Gregorian Calendar to Julian Date to properly rotate stars.\nLong explanation: Between different astronomers different epochs were used in different calendars, thus referencing different catalogs had problems in understanding which reference system was used. As some of you may know, depending on your culture you could use different calendars as well!\nFor astronomy related things, a common \u0026rsquo;time reference point\u0026rsquo; was decided by the International Astronomical Union, and this is the Julian Calendar with the precise moment called J2000.\nDifferent \u0026lsquo;reference points\u0026rsquo; were used, and in 1984 the IAU switched from the J1950 to the J2000 epoch.\nSpecifically an epoch is a moment in time that is used as central reference point to calculate positions and motions of celestial objects.\nJ2000 thus is the date of January 1, 2000 at 12:00 Terrestrial Time in the Gregorian Calendar at the Greenwich meridian, and all the positional data in the catalog is relative to this moment.\nThere are plenty of conversions between Gregorian Calendar and Julian Calendar, and in the code provided there will be some links also to some pages with the math involved.\nRight Ascension and Declination TLDR: stars \u0026rsquo;latitude and longitude\u0026rsquo; to place them.\nLong explanation:\nNow that we have a reference time, we can finally use the Right Ascension and Declination data.\nA very simple explanation of Right Ascension and Declination is that they are the celestial equivalent to latitude and longitude on earth, but they reference the celestial sphere - an ideal sphere centered in the Earth center and not following the Earth axis inclination.\nJost Burgi Celestial Sphere. Source: Wikipedia. Both Right Ascension and Declination are relative to the Celestial Equator - an ideal equator that has a different inclination than the natural Earth equator (due to its tilt axis). Right Ascension thus is the eastward angular distance relative to the Celestial Equator, expressed in hours, minutes and seconds. Declination instead is the north/south angle relative to the Celestial Equator, expressed in degrees (in the range -90, 90).\nAll this combined defines the Equatorial Coordinate System, used to locate celestial objects.\nRight Ascension and Declination relative to J2000 are then the celestial \u0026rsquo;latitude and longitude\u0026rsquo; to locate a celestial object around the Earth, relative to a reference system that is not tilted with the axis but more \u0026lsquo;absolute\u0026rsquo;.\nSpectral Type TLDR: convert from Spectral Type to RGB color.\nLong explanation:\nTo help organize stars they needed to categorize them based on some parameters. In 1817 already Josepth Von FraunHofer started analyzing the spectrum of the visible stars and in the following years, but it is with the work of Annie Cannon, that catalogued hundreds of thousands of stars, that spectrum-based classification became more common.\nThere are two main spectral classifications, the Morgan-Keenan System and the UBV or Johnson-Morgan-system.\nInterestingly enough the Yale Bright Star Catalog contains the MK Spectral Type in the binary format, and the BV index in the text format.\nI ended up creating a list of MK types to colors in the code, starting from these:\nSpectral Type to temperature https://www.handprint.com/ASTRO/specclass.html\nTemperature to color http://www.vendian.org/mncharity/dir3/blackbody/UnstableURLs/bbr_color.html\nAs an alternative I could have parse the text version and using the BV indices, possibly I\u0026rsquo;ll do that and cross reference colors to see if the are exact.\nThere is also this incredibly useful post about all this:\nhttps://stackoverflow.com/questions/21977786/star-b-v-color-index-to-apparent-rgb-color\nThe key information here is that once we read the Spectral Type of each star, we have a table that converts it to an RGB value.\nVisual Magnitude TLDR: magnitude shows how visible is a star.\nLong explanation:\nThis is the part in which I still don\u0026rsquo;t feel confident about what is really happening, but I\u0026rsquo;ll try to give the better explanation of what I understood. It will possibly be subject to changes in the code!\nIn the seminal paper a correlation between the Magnitude and the Glaring is done, with the Glaring coming from another very important paper on visual perception, defining Glare as the sum of the flare and bloom optical phenomena happening to our eyes here.\nWe are trying to create a correlation between the Magnitude of a star and how big is seen in the screen, and this is a pretty accurate description of what happens to us when seeing \u0026lsquo;brighter stars\u0026rsquo;.\nWe have 2 ways to achieve this:\nOutput a pixel color intensity that works with tonemapping and engages the bloom Make the star bigger based on magnitude. The second approach is the one used here, but I would like to experiment also with the other option.\nIn the second paper (Single Pass \u0026hellip;) there are some equations that correlate pixel intensity with visual magnitude AND \u0026lsquo;how big\u0026rsquo; the stars appear to the same magnitude.\nWe will see more in detail in the code how to use those.\nProper Motion TLDR: adjust right ascension and declination with this per-year changes.\nLong explanation:\nThe final data relative to the stars is the proper motion. Proper motion can be simply defined as the yearly move of a star in Equatorial Coordinate System. Most of the stars visible with naked eye are so distant that their motion is not as diverse as the J2000 position specified, but few (the closest ones) need a more precise calculation.\nTaking in account proper motion will give the most precise star positioning of all.\nCoding After this lenghty introduction in astronomy we can finally see the code! The repository is still DataDrivenRendering - but all the code and data is contained under the StarApplication folders in source/articles and data/articles! I am working on improving my framework so I can experiment faster and faster.\nAs already wrote before, one of the biggest problem was retrieving the data and understanding its meaning!\nProject Structure Data: https://github.com/JorenJoestar/DataDrivenRendering/tree/master/data/articles/StarRendering\nSource: https://github.com/JorenJoestar/DataDrivenRendering/tree/master/source/Articles/StarRendering\nAll the relevant code is in star_map_application.h/.cpp.\nStar Data Parsing We chose the Yale Bright Star Catalog, and there are two versions here (http://tdc-www.harvard.edu/software/catalogs/bsc5.html): one binary and one text. I chose to use the binary one, even though I could change idea and revise this code and article.\nThe binary is pretty easy to parse, with a caveat: you need an alignment of 1 to correctly parse the data! The parsing structures are just 2, one for the header and one for each star entry:\nhttp://tdc-www.harvard.edu/software/catalogs/bsc5.header.html http://tdc-www.harvard.edu/software/catalogs/bsc5.entry.html Once we parse from the file we have all our stars with right ascension, declination and visual magnitude ready for us!\nConstellation Data Parsing Constellations are another set of data that needs to be relative to a specific catalog. I found this website that presents the constellation lines in a text format:\nhttp://cdsarc.u-strasbg.fr/viz-bin/Cat?VI/49 https://github.com/hemel-waarnemen-com/Constellation-lines\nThe problem is that the file contains the constellations as a series of line (line strips) that you need to continuously draw like a pen not leaving the paper. I decided to convert this in a list of segments, so I have to parse the text file and make the conversion.\nThere are a couple of caveats here:\nConstellations can be present more than once, the they have 2 non contiguous lines. Parsing is done 2 times, first to calculate offsets of final segments (especially for the constellations with more lines), second to actually parse the data. This is also an interesting use of the hydra_lexer - backbone of the HFX language. In this demo it is already used but in following ones I\u0026rsquo;ll update it more and more.\n// Read constellation file char* constellation_data = hydra::file_read_into_memory( \u0026#34;..\\\\data\\\\articles\\\\StarRendering\\\\constellations_lines.txt\u0026#34;, nullptr, false, *allocator ); // Allocate raw memory and entries for the data parsed. // Not elegant, but functioning. DataBuffer data_buffer; data_buffer_init( \u0026amp;data_buffer, 10000, 10000 * 4 ); Lexer lexer; lexer_init( \u0026amp;lexer, constellation_data, \u0026amp;data_buffer ); // First parse: calculate offsets and total size of indices array. uint32_t data_size = 0; bool parsing = true; // An example entry: // Ant 4 4273 4104 3871 3765 // Hash is used as line comment. // while ( parsing ) { Token token; lexer_next_token( \u0026amp;lexer, token ); switch ( token.type ) { case Token::Token_Hash: { // Skip the line lexer_goto_next_line( \u0026amp;lexer ); break; } case Token::Token_Identifier: { // Ant 4 4273 4104 3871 3765 // Read name char name[ 4 ]; name[ 0 ] = toupper( token.text.text[ 0 ] ); name[ 1 ] = toupper( token.text.text[ 1 ] ); name[ 2 ] = toupper( token.text.text[ 2 ] ); name[ 3 ] = 0; int32_t constellation_index = Constellations::get_index( \u0026amp;constellations, name ); // Read segment count lexer_expect_token( \u0026amp;lexer, token, Token::Token_Number ); uint32_t count = atoi( token.text.text ); constellations.entries[ constellation_index ].count += count - 1; // This is segments count data_size += count - 1; // Segments count // Just advance the token to the next line. for ( uint32_t i = 0; i \u0026lt; count; ++i ) { lexer_next_token( \u0026amp;lexer, token ); } break; } case Token::Type::Token_EndOfStream: { parsing = false; break; } } } The most interesting part for me is seeing the parsing loop and having a lexer/tokenizer as a personal tool is a MUST! Constellations contains a map from the name to the entry. The second parse just reads the actual star numbers and puts them in the correct place. Not sure it is interesting code to read here. We now have a list of segments, and thus 2 points, for each constellation, in a contiguous block of memory.\nNext is\u0026hellip;\nTemperature to Color In the binary data we parsed from the Catalog we have the Spectral Type of a star, a letter+number identification system to classify a star. To properly calculate the color of a star we need to do the following:\nRead the star Spectral Type Convert the Spectral Type to Temperature Convert Temperature to RGB color Taking the data form the links in the MK part of the article, we parse the Temperature to Color data from a file containing the black bodies color data.\nAgain some non-interesting parsing code, but the entries are like this:\n1000 K 2deg 0.6499 0.3474 2.472e+06 1.0000 0.0337 0.0000 255 51 0 #ff3300 1000 K 10deg 0.6472 0.3506 2.525e+06 1.0000 0.0401 0.0000 255 56 0 #ff3800\nIt uses 2 different CIE specifications for colors, and we use the 10deg (the second one) of each entry.\nWe then have a table (hand written from the links above) that links Spectral Types to temperature:\nstruct Range { uint32_t min; uint32_t max; }; // Morgan-Keenan classification // https://starparty.com/topics/astronomy/stars/the-morgan-keenan-system/ // Letters are for star categories. // Numbers (0..9) are for further subdivision: 0 hottest, 9 colder. static const uint32_t k_max_star_types = \u0026#39;z\u0026#39; - \u0026#39;a\u0026#39;; // Temperature ranges (in Kelvin) of the different MK spectral types. static const Range k_star_temperature_ranges[ k_max_star_types ] = { // A0-A9 B C D E F G { 7300, 10000 }, { 10000, 30000 }, { 2400, 3200 }, { 100000, 1000000 }, { 0, 0 }, { 6000, 7300 }, { 5300, 6000 }, { 0, 0 }, { 0, 0 }, // J K L M O P Q R S T { 0, 0 }, { 3800, 5300 }, { 1300, 2100 }, { 2500, 3800 }, { 0, 0 }, { 30000, 40000 }, { 0, 0 }, { 0, 0 }, { 0, 0 }, { 2400, 3500 }, { 600, 1300 }, // U V W X Y { 0, 0 }, { 0, 0 }, { 25000, 40000 }, { 0, 0 }, { 0, 600 } }; We finally have all conversion from Spectral Types to Color! Ole\u0026rsquo;!\nGregorian/Julian date conversion Again some code that is mostly taking formulas from the net! In the Catalog we are using the Right Ascension and Declination of each star is expressed relative to the Julian Date 2000.\nThe only really interesting thing here is the fact that you need to use a double - a float will loose the difference for hour and less in the days!\n// // From https://en.wikipedia.org/wiki/Julian_day // // Gregorian Calendar Date to Julian Day Number conversion // This is the reference Julian Date used in current astronomy. static const int32_t j2000 = 2451545; // // Julian Day Number calculations. // https://en.wikipedia.org/wiki/Julian_day // https://aa.quae.nl/en/reken/juliaansedag.html // https://core2.gsfc.nasa.gov/time/julian.txt // http://www.cs.utsa.edu/~cs1063/projects/Spring2011/Project1/jdn-explanation.html static int32_t calculate_julian_day_number( int32_t year, int32_t month, int32_t day ) { // Formula coming from Wikipedia. int32_t a = ( month - 14 ) / 12; int32_t jdn = ( 1461 * (year + 4800 + a)) / 4 + ( 367 * ( month - 2 - 12 * a ) ) / 12 - ( 3 * ( ( year + 4900 + a ) / 100 ) ) / 4 + day - 32075; // Other formula found online: /*int m, y, leap_days; a = ( ( 14 - month ) / 12 ); m = ( month - 3 ) + ( 12 * a ); y = year + 4800 - a; leap_days = ( y / 4 ) - ( y / 100 ) + ( y / 400 ); int32_t jdn2 = day + ( ( ( 153 * m ) + 2 ) / 5 ) + ( 365 * y ) + leap_days - 32045;*/ return jdn; } // // Julian Date // static double calculate_julian_date( int32_t year, int32_t month, int32_t day, int32_t hour, int32_t minute, int32_t second ) { int32_t jdn = calculate_julian_day_number( year, month, day ); double jd = jdn + (( hour - 12.0 ) / 24.0) + (minute / 1440.0) + (second / 86400.0); return jd; } // // Julian centuries since January 1, 2000, used to rotate the stars. // static double calculate_julian_century_date( int32_t year, int32_t month, int32_t day, int32_t hour, int32_t minute, int32_t second ) { double jd = calculate_julian_date( year, month, day, hour, minute, second ); return ( jd - j2000 ) / 36525.0; } NOTE!!! The Julian date we are calculating is RELATIVE TO J2000! Super important!\nStar Placement This is the real deal. I myself used the seminal papers on the subject that express the formulat to calculate the placement of a star in the Celestial Sphere.\nThere are two component on this:\nConversion from Right Ascension and Declination to Equatorial Coordinates. The most important thing to remember here is that the data coming from the catalog is expressed at J2000 date. Let\u0026rsquo;s convert RA and D:\n// // Convert to euclidean coordinates // static void convert_to_euclidean( float right_ascension, float declination, float radial_distance, float\u0026amp; out_x, float\u0026amp; out_y, float\u0026amp; out_z ) { const float cosd = cosf( declination ); out_x = radial_distance * sinf( right_ascension ) * cosd; out_y = radial_distance * cosf( right_ascension ) * cosd; out_z = radial_distance * sinf( declination ); } This will put the star in place at the date of J2000 or January 1, 2000 at 12:00 Terrestrial Time in the Gregorian Calendar! We need to calculate the rotation from J2000 to our current time and location.\nLatitude, Longitude and Date to Rotation This is the missing link. As already noted above, the data coming from the Catalog is the position of the stars at J2000.\nThere are some conversions to do from latitude and longitude, and these formulas are a mix coming from the papers I mentioned at the beginnig and simple conversion.\nfloat longitude_radians = glm_rad( longitude ); float latitude_radians = glm_rad( latitude ); // Calculate rotation matrix based on time, latitude and longitude // T is time in julian century, as used in the paper. double T = calculate_julian_century_date( year, month, day, hour, minute, second ); double local_mean_sidereal_time = 4.894961f + 230121.675315f * T + longitude_radians; // Exploration of different rotations versors rotation_y = glms_quatv( latitude_radians - GLM_PI_2f, { 0, 1, 0 } ); versors rotation_z = glms_quatv( -local_mean_sidereal_time, { 0, 0, 1 } ); static bool rotation_order_invert = false; versors final_rotation = rotation_order_invert ? glms_quat_mul( rotation_y, rotation_z ) : glms_quat_mul( rotation_z, rotation_y ); if ( apply_precession ) { versors precession_rotation_z = glms_quatv( 0.01118f, { 0, 0, 1 } ); versors precession = glms_quat_mul( glms_quat_mul( precession_rotation_z, glms_quatv( -0.00972, {1, 0, 0} ) ), precession_rotation_z ); final_rotation = glms_quat_mul( final_rotation, precession ); } mat4s star_rotation_matrix = glms_quat_mat4( final_rotation ); The ugliness in this code is that I had some reference system problems somewhere, so I put some variables to understand what was happening. This is true code, you see everything :)\nStarting from latitude and longitude and date we arrive at a rotation matrix to apply to the stars!\nApplying precession is taking in consideration the precession and nutation phenomena, again something that the brillian paper \u0026lsquo;A Physically Based Night Sky\u0026rsquo; gives some formula. I admit not having still a deep understaning on how they arrived at that conclusion, but for sure I have a good starting point now (and hopefully you do as well!).\nThe final position of the star is star_rotation_matrix * vec4(position.xyz, 1), with the position calculated as the Equatorial Coordinate at J2000, and the Star Rotation as the additional rotation taking in consideration Latitude, Longitude and date.\nStar Rendering: Hydra for the win! We are using the new Hydra framework - this time having a 80% working Vulkan backend. I am working a bit on having HFX shader language extension as more and more something that I can rely to clearly prototype and explore ideas. The HFX file that renders everything defines also almost everything, from the shader to the vertex layout to the resource types used.\nThere is a bit of magic as well here - something I found reading the second paper, \u0026lsquo;Single Pass Rendering of Day and Night Sky Phenomena\u0026rsquo; - so the math is coming from there. I am missing the scintillation, scattering and the daylight removal when it is day, even though for this demo is not important. From this you can see the difference between a demo and a feature in a game: when developing this as a feature, you should consider the interaction with all the other rendering systems, the tonemapping, luts, any kind of clouds, sun and moon rendering, and such. This is crucial!\nFrom a top down view of the rendering, we are basically drawing billboards that use the visual magnitude both for size and alpha. Overdraw fest!!!\nHere is the shader used. As you can see you can specify vertex layout and render states as well - something I LOVE to see with shaders. They are an integral part of the rendering!\nWith an HFX file now you can define totally a Vulkan Pipeline, so it is a great tool. The code has reload of shaders, so you can experiment faster!\nshader stars { layout { list Local { cbuffer ViewConstants ViewConstants; } vertex main3D { binding 0 32 instance attribute float4 Position 0 0 0 attribute float4 ColorData 0 1 16 } } render_states { state alpha { Cull None ZTest Always ZWrite Off BlendMode Alpha } } glsl to_screen { #pragma include \u0026#34;Platform.h\u0026#34; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 star_rotation_matrix; vec4 camera_up; vec4 camera_right; vec4 data; // x = min_radius, y = glare scale, z = radius scale, w = distance scale }; #if defined VERTEX layout (location = 0) in vec4 position; layout (location = 1) in vec4 color_data; layout (location = 0) out vec4 vTexCoord; layout (location = 1) out vec3 vColor; // Per vertex positions and uvs of a quad vec3 positions[6] = vec3[6]( vec3(-0.5,-0.5,0), vec3(0.5,-0.5,0), vec3(0.5, 0.5, 0), vec3(0.5, 0.5, 0), vec3(-0.5,0.5,0), vec3(-0.5,-0.5,0) ); vec2 uvs[6] = vec2[6]( vec2(0.0, 1.0), vec2(1.0, 1.0), vec2(1.0, 0.0), vec2(1.0, 0.0), vec2(0.0, 0.0), vec2(0.0, 1.0) ); const float _35OVER13PI = 0.85698815511020565414014334123662; void main() { // Calculate color based on magnitude // Following paper \u0026#34;Single Pass Rendering of Day and Night Sky Phenomena\u0026#34; float m = position.w; float m_a = 7.0f; // Average apparent magnitude float delta_m = pow(2.512, m_a - m); // Magic from the papers. Investigate the WHY of this. float i_t = delta_m * _35OVER13PI; i_t *= 4e-7 / (data.x * data.x); // resolution correlated i_t = min(1.167, i_t); // volume of smoothstep (V_T) // Day-Twilight-Night-Intensity Mapping (Butterworth-Filter) //float b = 1.0 / sqrt(1 + pow(sun.z + 1.14, 32)); //i_t *= b; if ( i_t \u0026lt; 0.01 ) return; float i_g = pow(2.512, m_a - (m + 0.167)) - 1; vec3 v_t = vec3(i_t); // v_k const float glare_scale = data.y; const float v_k = max(data.x, sqrt(i_g) * 2e-2 * glare_scale); // TODO: Scattering and Scintillation //v_t -= E_ext; vTexCoord.w = v_k / data.x; // vColor = mix( color_data.xyz, vec3( 0.66, 0.78, 1.00 ), 0.66 ); vColor *= v_t; vColor = max(vec3(0.0), vColor); const uint vertex_index = gl_VertexID % 6; vTexCoord.xy = positions[vertex_index].xy * vec2(-1, 1); float particle_size = v_k * data.z; vec3 scaled_billboard = vTexCoord.x * particle_size * camera_right.xyz + vTexCoord.y * particle_size * camera_up.xyz; vec4 final_position = star_rotation_matrix * vec4(position.xyz, 1) + vec4(scaled_billboard.xyz, 1); gl_Position = view_projection_matrix * vec4(final_position.xyz, 1.0f); } #endif // VERTEX #if defined FRAGMENT layout (location = 0) in vec4 vTexCoord; layout (location = 1) in vec3 vColor; layout (location = 0) out vec4 outColor; void main() { float x = vTexCoord.x; float y = vTexCoord.y; float zz = (1 - x * x - y * y); if ( zz \u0026lt; 0.0 ) discard; float k = vTexCoord.w; float l = length(vec2(x, y)); const float radius_scale = data.w; const float glare_scale = data.y; float t = 1 - smoothstep(0.0, 1.0, l * k / radius_scale); float g = 1 - pow(l, glare_scale / 64.0); float intensity = max(t, g); outColor = vec4(intensity * vColor, intensity); } #endif // FRAGMENT } pass stars_to_screen { stage = final resources = Local vertex_layout = main3D vertex = to_screen fragment = to_screen render_states = alpha } } A simplified version of the rendering code is here, but I like how this is becoming. Feedback is really appreciated :)\nThe instance buffer contains the Euclidean positions of each star with the visual magnitude as well. We are drawing straight into the swapchain, and I use the abstraction of \u0026lsquo;resource list\u0026rsquo; similar to the \u0026lsquo;descriptor sets\u0026rsquo; coming from Vulkan. They are just a \u0026hellip;list of resources.\nCommandBuffer* gpu_commands = update.gpu_commands; gpu_commands-\u0026gt;clear( sort_key, 0, 0, 0, 1 ); gpu_commands-\u0026gt;clear_depth_stencil( sort_key++, 1.0f, 0 ); // Draw the stars! //////////////////////////// gpu_commands-\u0026gt;bind_pass( sort_key++, update.gpu_device-\u0026gt;get_swapchain_pass() ); gpu_commands-\u0026gt;set_scissor( sort_key++, nullptr );\t// Default framebuffer/render target sizes gpu_commands-\u0026gt;set_viewport( sort_key++, nullptr ); gpu_commands-\u0026gt;bind_vertex_buffer( sort_key++, star_instance_buffer, 0, 0 ); gpu_commands-\u0026gt;bind_pipeline( sort_key++, star_rendering_pipeline ); gpu_commands-\u0026gt;bind_resource_list( sort_key++, \u0026amp;star_resource_list, 1, nullptr, 0 ); gpu_commands-\u0026gt;draw( sort_key++, TopologyType::Triangle, 0, 6, 0, star_count_to_render ); And that\u0026rsquo;s it for the rendering! We basically draw the Celestial Sphere - we miss taking in consideration the night/day transition depending where we are, or the moon and sun.\nConclusion We had a dive into how to render stars using real-life astronomical data. The real deal is finding the proper data, and trying to understand how to use it. There are still some things that I don\u0026rsquo;t understand myself, but at least if you are curious about this topic this could be a good starting point.\nI hope to have time to understand things deeper soon and correct any error.\nHydra library is also evolving to something more and more usable for demo, I am trying to keep the code relatively small and clear, I\u0026rsquo;ll continue with other demos about this. I like the idea of something very concise - so you can focus only on the details you need.\nNext will be the Atmospheric Scattering demo from the amazing Sebastien Hillaire.\nHappy new year and may the stars shine upon your path!\n","date":1608980079,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608980079,"objectID":"b743e28a20c7aa5361fe911a0799697d","permalink":"https://jorenjoestar.github.io/post/realistic_stars/","publishdate":"2020-12-26T11:54:39+01:00","relpermalink":"/post/realistic_stars/","section":"post","summary":"Overview Since growing up I\u0026rsquo;ve always been fascinated by stars, and being exposed to anime like Sainy Seiya and Hokuto No Ken just fueled the passion. My 4th year of high-school had a full year course on \u0026lsquo;geographical astronomy\u0026rsquo; - an in depth look at our planet and the stars from a scientific perspective. Many years has passed (20+!) and I\u0026rsquo;ve never dwelved into these kind of topic.\nThen few years ago, while researching for rendering un Just Cause 4 I stumbled upon a couple of papers about realistic rendering of stars.","tags":[],"title":"Rendering Astronomic Stars","type":"post"},{"authors":[],"categories":[],"content":"After reading the great article by Sarah Drasner on productivity I wanted to share some other improvements that I use in my daily work and personal coding life.\nI am developing mostly rendering and other game-related code, so my OS is Windows 10. I have a background in using Linux-only for work at the beginning of my career, so bash customization and Vim were too useful to be overlooked!\nFor Windows I started using Cmder few years ago, but I should check also the revamped powershell. What I love about Cmder is that it gives you most of Unix/Linux scripts into Windows. And with that it comes also an \u0026lsquo;alias\u0026rsquo; file, in the path %CMDER_ROOT%\\config\\user_aliases.cmd.\nHere are some group of aliases I use to speed up my productivity!\nKnowing your aliases First of all some commands to know and edit your aliases.\nI tend to write aliases with acronyms for faster typing.\nma=cat %CMDER_ROOT%\\config\\user_aliases.cmd ea=vim %CMDER_ROOT%\\config\\user_aliases.cmd M.A. stands for My Aliases, while E.A. Edit Aliases.\nThis is the base - when I forget an alias I just type ma and I have all my list!\nJumping around Navigation is the first type of enhancement I recommend. This is HUGE for me and incredibly simple:\n..=cd .. ..2=cd ../.. ..3=cd ../../.. ..4=cd ../../../.. Navigate to parent folders in a much faster way!\nThe second one is actually\u0026hellip;jumps (thinking of ASM instructions)! When I identify some folders that I access often, I add these kind of lines:\njc= cd /D C:\\Coding jp= cd /D C:\\Users\\Gabriel\\Documents\\Visual Studio 2017\\Projects Again aliases. Jump Coding and Jump Projects.\nNotice the argument /D to use the absolute path - needed when you have paths in other folders.\nFile listing Again another simple trick, and you can add more variations to your needs. ls is the dir command of Linux/Unix, and is another foundation.\nls=ls --show-control-chars -F --color $* ll=ls --show-control-chars -F -l --color $* lr=ls --show-control-chars -F -lrt --color $* la=ls --show-control-chars -F -a --color $* Coloring is necessary to know what is a folder and what a file, something that should be enabled by default in my opinion. Also notice the \u0026lsquo;$*\u0026rsquo; at the end - it means to append all the argument that you want to pass after the alias!\nText editing I mainly use Sublime Text and occasionally VIM, so here are some aliases as well:\nvi=vim $* vimrc=vim %CMDER_ROOT%\\vendor\\msysgit\\share\\vim\\vimrc subl=\u0026#34;C:\\Program Files\\Sublime Text 3\\sublime_text.exe\u0026#34; $* With the alias subl you can open any file into Sublime Text. Very handy combination Cmder + Sublime Text!\nAlso quickly editing your vimrc file is a need for VIM users.\nGrepping I honestly completely forgot to add the color option\u0026hellip;learning through sharing. Thank you Sarah :)\nAdded the color option, these are the two variations of grep I use the most:\ngrep=grep --color $* gric=grep --color -Iir $* gril=grep --color -Iirl $* gric stands for Grep Ignore Case Ignore Binary Recursive - more or less. gril is like gric, but just lists file instead of content per file in the search. I use it to just check files.\nQuickly going through the options for grep:\n-I: let you ignore binary files. Speed up searching quite a bit. -i: ignore case. -r: recursively search directories. -l: only list files, not content. I\u0026rsquo;ll show you the difference.\nLet\u0026rsquo;s search for \u0026lsquo;blipbuffer\u0026rsquo; into my HydraNes/src folder:\ngrep --color -Iir blipbuffer *\nWe\u0026rsquo;ll loose color in the post, but this is the result:\nsrc/main.cpp: if (nes-\u0026gt;apu.blipBuffer-\u0026gt;samples_avail()) { src/main.cpp: int32 readSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;read_samples(sampleBuffer, kBufferSize, false); src/main.cpp: const int32 availableSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;samples_avail(); src/main.cpp: const int32 readSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;read_samples( bufferAddress, kBufferSize, false ); src/Nes.cpp: if ( !blipBuffer ) { src/Nes.cpp: blipBuffer = new Blip_Buffer(); src/Nes.cpp: blipBuffer-\u0026gt;clock_rate( CpuClockRate ); src/Nes.cpp: blipBuffer-\u0026gt;sample_rate( SampleRate ); src/Nes.cpp: externalApu-\u0026gt;output( blipBuffer ); src/Nes.cpp: blipBuffer-\u0026gt;clear(); src/Nes.cpp: blipBuffer-\u0026gt;clear(); src/Nes.cpp: blipBuffer-\u0026gt;end_frame( count ); src/Nes.cpp: // //blipBuffer-\u0026gt;end_frame( remainingCycles ); src/Nes.h:#include \u0026#34;BlipBuffer/blip_buf.h\u0026#34; src/Nes.h: Blip_Buffer* blipBuffer = nullptr; Instead using the list only option:\ngrep --color -Iirl blipbuffer *\nGives you this result:\nsrc/main.cpp src/Nes.cpp src/Nes.h Git This is another big one. Git can have very verbose commands, so aliases save a lot of typing! Again I add generic and very specific version of commands:\ngs=git status gl=git log --oneline --all --graph --decorate $* ga=git add \u0026#34;$*\u0026#34; gcm=git commit -m \u0026#34;$*\u0026#34; grmdir=git rm -r \u0026#34;$*\u0026#34; grmf=git rm \u0026#34;$*\u0026#34; gpso=git push -u origin \u0026#34;$*\u0026#34; gpsom=git push -u origin master gplo=git pull origin \u0026#34;$*\u0026#34; gplom=git pull origin master gru=git remote update gsr=git status -uno -u gt=git stash gts=git stash show -p gtl=git stash list gta=git stash apply gsps=git subtree push --previs= $* gspl=git subtree pull --previs= $* gspsh=git subtree push --prefix=source/hydra hydra master gspsl=git subtree pull --prefix=source/hydra hydra master See the difference between gpso and gpsom - the second one just using the master branch. gpsom and gplom are the ones I use the most and this again saves a lot of time.\nSame for the subtree commands, showing how I update my code using the common libraries names as hydra. In this case - and this is more a git concept - when working with subtree I use a remote alias, added with git remote add -f 'name' https://....git .\ngs is great to see what is the status of the current repository you are in. gl logs all the commits. ga adds the files and folders you write after the command.\nVisual Studio Compiler Some different aliases I use for Visual Studio:\nvs=\u0026#34;%VS140COMNTOOLS%..\\IDE\\devenv.exe\u0026#34; /edit \u0026#34;$*\u0026#34; ;= Needed to find MSBuild executable. vcvars=\u0026#34;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\u0026#34; msb=MSBuild $1 /property:Configuration=$2 /property:Platform=$3 msbd=MSBuild $1 /property:Configuration=Debug /property:Platform=x64 msbr=MSBuild $1 /property:Configuration=Release /property:Platform=x64 Again something as msb can be used to build code from a Visual Studio Solution. msbd and msbr are useful shortcuts for again commonly used configurations and platforms.\nConclusions There are many ways to improve productivity - and reducing the amount of stuff you have to write, for repetitive tasks, is very powerful. Hope this helps and again thanks to Sarah Drasner for the article that sparked the idea of writing this one!\nIf anybody wants to add more, comment, feedback please write to me! Gabriel\n","date":1586788980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586788980,"objectID":"c878b34d79c4a846d8504c72f6698bda","permalink":"https://jorenjoestar.github.io/post/productivity_terminal/productivity_terminal/","publishdate":"2020-04-13T10:43:00-04:00","relpermalink":"/post/productivity_terminal/productivity_terminal/","section":"post","summary":"After reading the great article by Sarah Drasner on productivity I wanted to share some other improvements that I use in my daily work and personal coding life.\nI am developing mostly rendering and other game-related code, so my OS is Windows 10. I have a background in using Linux-only for work at the beginning of my career, so bash customization and Vim were too useful to be overlooked!\nFor Windows I started using Cmder few years ago, but I should check also the revamped powershell.","tags":[],"title":"Improving Productivity in Terminals with Aliases","type":"post"},{"authors":[],"categories":[],"content":"Overview In the last articles we looked at progressively building tools to move rendering code towards data. We looked on how to create a simple lexer, a simple parser and a code generator. With those we were able to create a very simple language to augment shaders.\nWhy that ?\nThere are few reasons:\nShader languages misses any way of linking multiple programs Shader languages misses any way to define render states CGFX and Microsoft FX are mostly dead Ability to use ANY shader language - and just add the infrastructure Ability to generate permutations without manually creating them Hydra FX aims to add all the missing features and becoming just an augmentation to ANY shader language.\nThere is also a fundamental reason for me:\nDefine all the rendering informations needed as soon as possible!\nWith this reason then defining everything inside an HFX file, with the possibility of overriding some properties, it is paramount.\nThis is by far a new concept, and with the newer Graphics APIs (Vulkan and D3D12) it is becoming more and more of a need.\nIn this article we will see how we have all those features and add the missing ones. I will try to define everything in a more API-Independent way, so it can be adapted to any engine needs.\nWhy we need shader augmentation The augmentation that I have in mind is needed for different reasons, not only if you are targeting the newer APIs. Some of the reasons are the following:\nWrite the NECESSARY code. Nothing more. Logically group shaders in the same file. Describe a priori all the STATIC parts of rendering. Being more data driven, improve iteration time. Being more data driven, encourage rendering experimentation. Easiness of debugging. Encourages less hardcoded data. Write the NECESSARY code. Nothing more. As I wrote in previous articles writing code is our biggest power and liability. Wasting time writing useless code is like slowly typing with one finger, without any clue of a design nor knowledge of the subject. Of course this is very personal, but any time I have to reiterate some steps (in anything in my life) for no purpose but bad-design/bad-architecture/technical-debt it really makes me feel bad. Again, it is like playing Diablo and clicking all the time to attack, instead of knowing that you can hold the mouse button!\nFinally to the topic: shader augmentation means moving to data what many times is expressed in code. We can both have a data-driven rendering, or even generate code for us, or a combination of both. There is not right or wrong, and this will change in the future! The best solution is the one that solves your (incredibly well described) problem.\nAdding render states, vertex inputs, render graph informations let a simple text file to find its space into your awesome rendering quite easily.\nLogically group shaders in the same file. Having to write separated files it can be ok, but many times having everything in one file (well divided) will be easier to logically connects the shader themselves. Sometimes you can get lost into the combination of shaders quite easily. And anyway you NEED to define which shaders are used together since the dawn of time.\nSo put them into the same file!\nDescribe a priori all the STATIC parts of rendering. Knowing all the static parts of rendering can lead to offline analysis and build, statistics, and all kind of things you can think of. It also serves to really have knowledge of the combinational explosion of rendering before it arrives in your beloved renderer! Sometimes you can group shaders together and improve speed and usability by just analysing how similar some shaders are.\nBeing more data driven, improve iteration time. If you think of reloading assets, then a shader reload will also load all the render stage associated. If you want to bring it a step further, adding/removing passes, changing were in the render graph the shaders are used can be an incredible tool to quickly prototype, optimize, develop ideas.\nBeing more data driven, encourage rendering experimentation. You can also add some non-coding tools to augment a shader with all those data. And again, defining this in data let\u0026rsquo;s you check relationship with the rest of the renderer more easily.\nEasiness of debugging. Data-drivenness means that data is always available. In the example I am adding here, you can see how useful can be to even have a simple ImGui debug of a HFX file. Bring that to a realtime renderer, and you can quickly debug rendering problems without having to use external tools like Pix, RenderDoc and such. These are wonderful tools, but I always love to have a defense before arriving there.\nAn example is to debug on someone\u0026rsquo;s machine that does not have installed those tools.\nSame can be applied to performances, to quickly check performances in realtime.\nTooling is essential to any developer, and should be developed with the technology itself.\nEncourages less hardcoded data. Nothing wrong to hardcoding data, and many times is necessary and useful. But the question is: when is necessary ?\nHaving a common data format gives you the tools (see previous point) to analyze, compared to hardcoded.\nShader Augmentations We will take a HFX shader and will see all the augmentations. This is used in the Render Pipeline Article and renders the GLTF assets:\nshader PBR { properties { albedo_texture(\u0026#34;Albedo\u0026#34;, 2D) = \u0026#34;\u0026#34; normals_texture(\u0026#34;Normals\u0026#34;, 2D) = \u0026#34;\u0026#34; metal_roughness_texture(\u0026#34;MetalRoughness\u0026#34;, 2D) = \u0026#34;\u0026#34; emissive_texture(\u0026#34;Emissive\u0026#34;, 2D) = \u0026#34;\u0026#34; occlusion_texture(\u0026#34;Occlusion\u0026#34;, 2D) = \u0026#34;\u0026#34; scale(\u0026#34;Scale\u0026#34;, Float) = 16.0 } layout { vertex main3D { binding 0 16 vertex attribute float3 Position 0 0 0 attribute ubyte4n Color 0 1 12 } vertex main3DPosition { binding 0 12 vertex attribute float3 Position 0 0 0 } vertex main3DPositionNormal { binding 0 12 vertex binding 1 12 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } vertex gbuffer { binding 0 12 vertex binding 1 12 vertex binding 2 8 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float2 UV 2 2 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } list gbuffer { cbuffer ViewConstants ViewConstants; texture2D albedo; texture2D normals; texture2D metalRoughness; texture2D emissive; texture2D occlusion; sampler2D linear_sampler } } sampler_states { state linear_sampler { Filter MinMagMipLinear AddressU Clamp AddressV Clamp } } render_states { state main { Cull Back ZTest LEqual ZWrite On } state linesZTest { Cull None ZTest LEqual ZWrite Off BlendMode Alpha } } glsl GBuffer_V { #pragma include \u0026#34;Platform.h\u0026#34; layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 2) in vec2 UV; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; out vec2 uv; out vec3 worldPosition; void main() { vertexNormal = (inverse(transpose((instanceTransform))) * vec4(Normal,0)).rgb; uv = UV; vec4 world_pos = instanceTransform * vec4(Position.xyz, 1.0f); worldPosition = world_pos.xyz; gl_Position = view_projection_matrix * world_pos; } } glsl GBuffer_F { #pragma include \u0026#34;Platform.h\u0026#34; layout (location = 0) out vec4 Out_Color; layout (location = 1) out vec4 Out_Normals; layout (location = 2) out vec4 Out_Properties0; //layout (location = 3) out vec4 Out_WorldPosition; layout(binding=0) uniform sampler2D albedo; layout(binding=1) uniform sampler2D normals; layout(binding=2) uniform sampler2D metalRoughness; layout(binding=3) uniform sampler2D emissive; layout(binding=4) uniform sampler2D occlusion; in vec3 vertexNormal; in vec2 uv; in vec3 worldPosition; void generate_TB_basis( out vec3 vT, out vec3 vB, vec2 texST, vec3 base_normal, vec3 sigma_x, vec3 sigma_y, float flip_sign ) { vec2 dSTdx = dFdxFine ( texST ) , dSTdy = dFdyFine ( texST ) ; float det = dot ( dSTdx , vec2 ( dSTdy .y ,- dSTdy .x )); float sign_det = det \u0026lt;0 ? -1 : 1; // invC0 represents ( dXds , dYds ) ; but we don ’t divide by // determinant ( scale by sign instead ) vec2 invC0 = sign_det * vec2 ( dSTdy .y , - dSTdx .y ); vT = sigma_x * invC0 .x + sigma_y * invC0 .y; if( abs ( det ) \u0026gt; 0.0) vT = normalize ( vT ); vB = ( sign_det * flip_sign ) * cross ( base_normal , vT ); } void main() { // Calculate gradient base: vec3 base_normal = normalize(vertexNormal); vec3 position_derivate_x = dFdxFine( worldPosition ); vec3 position_derivate_y = dFdyFine( worldPosition ); vec3 sigma_x = position_derivate_x - dot( position_derivate_x, base_normal ) * base_normal; vec3 sigma_y = position_derivate_y - dot( position_derivate_y, base_normal ) * base_normal; float flip_sign = dot ( position_derivate_y, cross ( base_normal, position_derivate_x )) \u0026lt; 0 ? -1 : 1; vec3 tangent, bitangent; generate_TB_basis( tangent, bitangent, uv.xy, base_normal, sigma_x, sigma_y, flip_sign ); vec3 tangent_normal = texture(normals, uv.xy).xyz * 2 - 1; vec3 normal = tangent * tangent_normal.x + bitangent * tangent_normal.y + base_normal * tangent_normal.z; normal = normalize(normal); vec3 emissive_color = texture(emissive, uv.xy).rgb; //Out_Normals = vec4(vertexNormal, 1); //Out_Normals = vec4(tangent_normal * 0.5 + 0.5, 1); Out_Normals = vec4(normal, emissive_color.r); vec3 color = texture(albedo, uv.xy).xyz; float occlusion = texture(occlusion, uv.xy).r; Out_Color = vec4(color, occlusion); // G = Rougthness, B = Metalness vec2 roughness_metal = texture(metalRoughness, uv.xy).yz; Out_Properties0 = vec4(roughness_metal.xy, emissive_color.gb); // TODO: remove! This is to test world space reconstruction! //Out_WorldPosition = vec4(worldPosition, 1); } } glsl PositionOnly { #pragma include \u0026#34;Platform.h\u0026#34; #if defined VERTEX layout (location = 0) in vec3 Position; uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; void main() { gl_Position = view_projection_matrix * vec4(Position.xyz, 1.0f); } out vec4 vTexCoord; #endif // VERTEX #if defined FRAGMENT layout (location = 0) out vec4 Out_Color; void main() { Out_Color = vec4(1,1,1,1); } #endif // FRAGMENT } glsl PositionNormals { #pragma include \u0026#34;Platform.h\u0026#34; #if defined VERTEX layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; void main() { vertexNormal = Normal; gl_Position = view_projection_matrix * instanceTransform * vec4(Position.xyz, 1.0f); } #endif // VERTEX #if defined FRAGMENT layout (location = 0) out vec4 Out_Color; layout (location = 1) out vec4 Out_Normals; in vec3 vertexNormal; void main() { Out_Normals = vec4(vertexNormal * 0.5 + 0.5, 1); vec3 L = vec3(-0.7, 0.7, 0 ); float lambert_diffuse = max(0, dot(vertexNormal, L)); Out_Color = vec4(lambert_diffuse.xxx, 1); } #endif // FRAGMENT } pass GBuffer { resources = gbuffer render_states = main vertex_layout = gbuffer vertex = GBuffer_V fragment = GBuffer_F } pass PositionN { render_states = main vertex_layout = main3DPositionNormal vertex = PositionNormals fragment = PositionNormals } pass PositionOnly { render_states = main vertex_layout = main3DPosition vertex = PositionOnly fragment = PositionOnly } } 1: Linking Multiple Programs This is a pretty simple task, and the first one to be tackled.\nIn Vulkan all the Pipelines need all the shader used at creation, using an array of VkPipelineStageCreationInfo for graphics, compute and ray-tracing.\nIn D3D12, you have the ShaderBytecode used in the pipelines, but not as arrays (just member of the various creation structs).\nFrom a functionality perspective, they are EQUAL. It makes sense - a Pipeline is the description of all the static part of a GPU pipeline, and shaders are amongst the most important part of it.\nYou can see it in the \u0026lsquo;pass\u0026rsquo; section of the HFX file:\npass PositionOnly { vertex = PositionOnly fragment = PositionOnly ... } For a compute pipeline is even simpler, and dispatch size can be added as well:\npass DeferredCompute { compute = DeferredCompute dispatch = 32, 32, 1 ... } Even just with something like this it is easy to organize different shaders.\n2: Define Render States Following the previous point, Pipelines need also (almost) all the render states (depth/stencil, alpha, raster, \u0026hellip;) to be defined. This was one of the main features of CGFX and Microsoft\u0026rsquo;s FX - and still now is incredibly useful. Unity\u0026rsquo;s ShaderLab also incorporates render states.\nI decided to separate render states on their own group:\nrender_states { state main { Cull Back ZTest LEqual ZWrite On } state linesZTest { Cull None ZTest LEqual ZWrite Off BlendMode Alpha } } Here two different render states are defined. In this case a render states defines depth/stencil, blend and rasterization.\nA great addition to that is to add the possibility of inherit/override render states. For example in a Transparent pass, the blend state could be defined in the Render Pass data, and be inherited explicitly here.\nAlso very important is the definition of input assembly - how the vertices are fed into the vertex program:\nlayout { vertex main3D { binding 0 16 vertex attribute float3 Position 0 0 0 attribute ubyte4n Color 0 1 12 } vertex main3DPosition { binding 0 12 vertex attribute float3 Position 0 0 0 } vertex main3DPositionNormal { binding 0 12 vertex binding 1 12 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } vertex gbuffer { binding 0 12 vertex binding 1 12 vertex binding 2 8 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float2 UV 2 2 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } } Here we can see some instancing use case, just to show the flexibility of writing this code. The bytes offset could be removed as well.\n3: Use ANY Shader Language The best way to diffuse these augmentation is to change the less possible the shader languate itself. This is because you want to be portable, and when having different platform it can be paramount even to define shaders with different languages into the same file, and switch based on platforms. This is becoming less and less of a need (see HLSL working on Vulkan) but there could be some special cases. Would it be great to fix those special cases by writing platform specific shader fragments without any of your internal rendering code changing ?\nThe choise here is to use a keyword to identify the type of language and then simply write the code in that language. This is ideal to also incorporate code from previous codebases with a small amount of work.\nLet\u0026rsquo;s look at the GBuffer Vertex GLSL code:\nglsl GBuffer_V { #pragma include \u0026#34;Platform.h\u0026#34; layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 2) in vec2 UV; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; out vec2 uv; out vec3 worldPosition; void main() { vertexNormal = (inverse(transpose((instanceTransform))) * vec4(Normal,0)).rgb; uv = UV; vec4 world_pos = instanceTransform * vec4(Position.xyz, 1.0f); worldPosition = world_pos.xyz; gl_Position = view_projection_matrix * world_pos; } } The only modification I did, and it is sadly necessary in GLSL, is to add the \u0026lsquo;#pragma include\u0026rsquo; custom parsing to add the include in the HFX compiler.\n4: Resource Layouts as First Citizens A new addition of the new APIs, resource layouts are another great factor to take care of. Architecturally they can be implemented in different ways, but I like the idea of having them \u0026lsquo;in your face\u0026rsquo; since the beginning!\nIn the layout section, you can define resources like this:\nlist gbuffer { cbuffer ViewConstants ViewConstants; texture2D albedo; texture2D normals; texture2D metalRoughness; texture2D emissive; texture2D occlusion; sampler2D linear_sampler } The name will be used in the pass section to define which resource list is used. There can be multiple resource lists, normally they should be grouped per frequency (most frequent changes to least frequent ones) and can be separated by a comma for example.\nA small addition is to use externally specified resource list and code, like for starnest.hfx:\npass main { ... resources = \u0026#34;ShaderToy.Main\u0026#34;\t} This means that the pass named \u0026lsquo;main\u0026rsquo; will simply use the resources defined in \u0026lsquo;shadertoy.hfx\u0026rsquo; - resource list called main.\n5: Permutations This is the most tedious of the tasks, and also one of the most dangerous. Permutations explosion is a well known problem, and there are different ways of tackling this. If you don\u0026rsquo;t have a shader augmentation a good option is to write some scripts to help with generating the code.\nOtherwise if you have a shader augmentation and you define a \u0026lsquo;shader state\u0026rsquo;, you can define some \u0026lsquo;permutation flags\u0026rsquo;, and just add the defines when you compile shaders. Even in GLSL, you can do some easy string concatenation to add those defines, or use tools like GLSLang + SpirV to help.\nThis becomes a cartesian product of all the permutations/options groups and again can lead to a lot of created shader. I am still investigating the best approach and I will update this article with the results, because I want to include them into HFX but avoid that to become a huge file - and worst to include unused permutations.\nSo stay tuned as I will update this article with the solution I find!\n6: C++ Generated Helpers As finishing touch, there are some informations that can be exposed in a c++ file. \u0026hellip;\nIncluded code: \u0026lsquo;Shader Augmentation\u0026rsquo; The included code has a small application to compile and inspect HFX files. \u0026hellip;\nConclusions I tried to explain the reasons of the different shader augmentations and trying to focus more on the importance of not trying to create a new shading language, but instead empowering it with new informations.\nI can\u0026rsquo;t stress enough how important is to me to have an abstraction that is slightly on top of current shaders API - and create other systems to hide the complexities if needed.\nWith HFX, I would like to expand any language by adding all those features. I wish this could become a used tool by many in their project, and really wish it will be the initial spark.\nNext in line is a revisiting of higher level of rendering, to arrive to explore different rendering techniques with the easiness that the data-driven approach should give.\nAs always please comment, give me feedback, share and enjoy!\nThanks for reading! Gabriel\n","date":1584375097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584375097,"objectID":"d878607d4270e839a6401b74363024f4","permalink":"https://jorenjoestar.github.io/post/shader_augment_for_pipelines/shader_augment_for_pipelines/","publishdate":"2020-03-16T12:11:37-04:00","relpermalink":"/post/shader_augment_for_pipelines/shader_augment_for_pipelines/","section":"post","summary":"Overview In the last articles we looked at progressively building tools to move rendering code towards data. We looked on how to create a simple lexer, a simple parser and a code generator. With those we were able to create a very simple language to augment shaders.\nWhy that ?\nThere are few reasons:\nShader languages misses any way of linking multiple programs Shader languages misses any way to define render states CGFX and Microsoft FX are mostly dead Ability to use ANY shader language - and just add the infrastructure Ability to generate permutations without manually creating them Hydra FX aims to add all the missing features and becoming just an augmentation to ANY shader language.","tags":[],"title":"Augmenting shader languages for modern rendering APIs","type":"post"},{"authors":[],"categories":[],"content":"Overview Model used in the demo. Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ We finally arrived in the Rendering Pipeline realm. Some can write that it is useless, some can hate it. Many have some sort of abstraction for it since ages, and others have to now that new APIs like Vulkan and DX12 have it as an explicit part of their design (finally!).\nAfter we built a basic Material System in the previous article (https://jorenjoestar.github.io/post/writing_shader_effect_language_3/) we can add another layer on top of it and built a complete Rendering Frame.\nIn this article I will talk about a simplified version of Render Graph that I call Render Pipeline and came into my mind in the canteen of Codemasters after thinking:\nWhat is the biggest dependency in Rendering ?\nThe answer is simple:\nRender Targets!\nRender Targets or Frame Buffers is just an intermediate buffer in which we can draw something and use it later. Basically a Read/Write texture! It is not easy to shuffle around a Render Target, and having knowledge of which one are you using can make a huge difference for your rendering tech. Textures and Render Targets are the biggest memory lord in any rendering application, thus knowing where you are spending your memory can be really powerful.\nFrom a pure understanding of rendering techniques, having a clear visualization of this aspect makes a HUGE difference!\nOnce I started using to describe a frame of rendering with the Render Target Dependencies I never looked back. As always, knowledge is power.\nRender Pipeline Thinking First of all, let\u0026rsquo;s start defining some general concepts to describe the problem we are trying to solve.\nThe problem we are trying to solve is:\nHow to describe the inter-frame dependencies of Render Targets in a frame ?\nThe dependencies are who writes and/or read from/to a Render Target. That is exactly what is described in a Render Pipeline. Enter the Render Pipeline.\nA Render Pipeline is a list of Passes that read and writes Render Targets.\nThat\u0026rsquo;s it. Done! See you next article!\nOf course I am kidding - but this is the gist of it. The implications, however, are profound.\nNext logical question is:\nHow can we read and write from/to a Render Target ?\nLet\u0026rsquo;s list how we can write to a Render Target\nGraphics - binding some geometry, render states and Render Targets Compute - write anything to the Render Target Even a so called \u0026lsquo;post-process\u0026rsquo; is just a fullscreen triangle with a shader.\nAnd to read\u0026hellip;well any shader that takes reads a texture!\nIt is incredible to think that with this simple building blocks you can describe almost everything to render!\nFor example, let\u0026rsquo;s try to express some common rendering techniques using only those concepts.\nDeferred Rendering We can define the following simple steps:\nMeshes uses their materials (shaders + textures + constants) as input and write into GBuffer Render Target + depth. A Compute/Post-process shader will read the Gbuffer Render Target and depth (to reconstruct the pixel position), a light list of some sort and outputs a texture with the result. Transparent objects are drawn into this new Render Target using their materials. And so on\u0026hellip; Exponential Variance Shadow Mapping in a Forward Rendering Pipeline Meshes writes into a depth-only render target using the light as \u0026lsquo;camera/point of view\u0026rsquo;. Compute or Postprocess converts the depth-only render target into a EVSM one. Meshes uses their materials and the EVSM shadow map to render into a \u0026lsquo;main\u0026rsquo; Render Target. \u0026lt;diagram 2 goes here\u0026gt;\nOther Rendering Concepts To give a full description of the frame we need to add other concepts that will help us. These are the less strict ones - and just a personal way of seeing things.\nRender View The concept of \u0026lsquo;Render View\u0026rsquo; is just a way or representing a camera and a list of visible objects from it. We will see how we use it later, but a simple example of Render View would be the \u0026lsquo;Sun Shadow\u0026rsquo; render view - representing the sun (as a camera) and a list of visible objects from it. The \u0026lsquo;Main\u0026rsquo; render view of course represent the main camera and visible objects. This, combined with render managers becomes a powerful combination to describe what needs to be rendered.\nRender Manager If you think from an ECS mentality, this would be a \u0026lsquo;system\u0026rsquo;. Each render manager is responsible to render one or more render \u0026lsquo;aspects/entities\u0026rsquo; into a Render Pass. A render manager can subscribe to any \u0026lsquo;graphics\u0026rsquo; pass and render from there.\nFor example, a \u0026lsquo;static geometry\u0026rsquo; render manager could setup an instancing buffer for the gbuffer-generation pass and draw all objects.\nRender Pipeline Implementation After we defined the basic concepts let\u0026rsquo;s see an actual implementation of the Render Pipeline. We will see the code of each component and arrive at the actual data definition (in json).\nThe code has changed a bit since last article, with the inclusion of CGLM as math library and other high-level rendering code, included in hydra_rendering.h/.cpp.\nRender View First element is the Render View:\n//\r// Render view is a \u0026#39;contextualized\u0026#39; camera - a way of using the camera in the render pipeline.\r//\rstruct RenderView {\rCamera camera;\rarray( RenderScene ) visible_render_scenes;\r}; // struct RenderView Using STB\u0026rsquo;s array (the macro is just an aid to know it is not just a pointer) we have a list of visible render scenes from that camera. It should be pretty straighforward.\nRender Manager Next is Render Manager:\n//\rstruct RenderManager {\rstruct RenderContext {\rDevice* device;\rconst RenderView* render_view;\rCommandBuffer* commands;\rRenderScene* render_scene_array;\ruint16_t start;\ruint16_t count;\ruint16_t stage_index;\r}; // struct RenderContext\rvirtual void render( RenderContext\u0026amp; render_context ) = 0;\r}; // struct RenderManager The base class is really just a \u0026lsquo;render\u0026rsquo; method. Here the RenderContext is interesting, and it gives access to all you need to render:\nDevice - used to map/unmap resources. RenderView - access to camera (and more, but that\u0026rsquo;s for the next article!). CommandBuffer - the actual draw commands are written here. RenderScene - the RenderScene from start to start + count. In this very simple demo, we have just 2 render managers: Line Renderer and Scene Renderer. The most interesting one is the second: Line Renderer has commands to draw lines that will be mapped into a GPU buffer and uses instancing to draw them.\nvoid LineRenderer::render( RenderContext\u0026amp; render_context ) {\rDevice\u0026amp; device = *render_context.device;\r// Update camera matrix\rconst Camera\u0026amp; camera = render_context.render_view-\u0026gt;camera;\rMapBufferParameters cb_map = { lines_cb, 0, 0 };\rfloat L = 0, T = 0;\rfloat R = device.swapchain_width, B = device.swapchain_height;\rconst float ortho_projection[4][4] =\r{\r{ 2.0f / ( R - L ), 0.0f, 0.0f, 0.0f },\r{ 0.0f, 2.0f / ( T - B ), 0.0f, 0.0f },\r{ 0.0f, 0.0f, -1.0f, 0.0f },\r{ ( R + L ) / ( L - R ), ( T + B ) / ( B - T ), 0.0f, 1.0f },\r};\rLocalConstants* cb_data = (LocalConstants*)device.map_buffer( cb_map );\rif ( cb_data ) {\rcb_data-\u0026gt;view_projection = camera.view_projection;\rmemcpy( \u0026amp;cb_data-\u0026gt;projection, \u0026amp;ortho_projection, 64 );\rcb_data-\u0026gt;resolution = { device.swapchain_width * 1.0f, device.swapchain_height * 1.0f, 1.0f / device.swapchain_width, 1.0f / device.swapchain_height };\rdevice.unmap_buffer( cb_map );\r}\rif ( current_line_index ) {\rconst uint32_t mapping_size = sizeof( LinVertex ) * current_line_index;\rMapBufferParameters map_parameters_vb = { lines_vb, 0, mapping_size };\rLinVertex* vtx_dst = (LinVertex*)device.map_buffer( map_parameters_vb );\rif ( vtx_dst ) {\rmemcpy( vtx_dst, \u0026amp;s_line_buffer[0], mapping_size );\rdevice.unmap_buffer( map_parameters_vb );\r}\rCommandBuffer* commands = render_context.commands;\rcommands-\u0026gt;begin_submit( 2 );\rShaderInstance\u0026amp; shader_instance = line_material-\u0026gt;shader_instances[3];\rcommands-\u0026gt;bind_pipeline( shader_instance.pipeline );\rcommands-\u0026gt;bind_resource_list( shader_instance.resource_lists, shader_instance.num_resource_lists, nullptr, 0 );\rcommands-\u0026gt;bind_vertex_buffer( lines_vb, 0, 0 );\r// Draw using instancing and 6 vertices.\rconst uint32_t num_vertices = 6;\rcommands-\u0026gt;draw( TopologyType::Triangle, 0, num_vertices, current_line_index / 2 );\rcommands-\u0026gt;end_submit();\rcurrent_line_index = 0;\r}\r} Easy to notice how, with a Vulkan/DX12 interface, there are few less commands to write. Binding a pipeline sets everything considered \u0026lsquo;static\u0026rsquo; - render states, shaders - and with just resource lists (that sets textures and constants) and vertex/index buffers we have everything needed to render.\nNOTE: HFX has gone some improvements and now supports render states and vertex declarations/formats. I\u0026rsquo;ll write about it in the next post - but this has become crucial.\nShader Resources Management This is another personal preference - but not necessary at all. Two concepts are really useful to me to be explicit and centralized: resources and bindings.\nResources are all referenced in a \u0026lsquo;Shader Resource Database\u0026rsquo;:\n//\r// Struct used to retrieve textures, buffers and samplers.\r//\rstruct ShaderResourcesDatabase {\rstruct BufferStringMap {\rchar* key;\rBufferHandle value;\r}; // struct BufferStringMap\rstruct TextureStringMap {\rchar* key;\rTextureHandle value;\r}; // struct TextureStringMap\rstruct SamplerStringMap {\rchar* key;\rSamplerHandle value;\r}; // struct SamplerStringMap\rBufferStringMap* name_to_buffer = nullptr;\rTextureStringMap* name_to_texture = nullptr;\rSamplerStringMap* name_to_sampler = nullptr;\rvoid init();\rvoid terminate();\rvoid register_buffer( char* name, BufferHandle buffer );\rvoid register_texture( char* name, TextureHandle texture );\rvoid register_sampler( char* name, SamplerHandle sampler );\rBufferHandle find_buffer( char* name );\rTextureHandle find_texture( char* name );\rSamplerHandle find_sampler( char* name );\r}; // struct ShaderResourcesDatabase Simply put, any resource used by rendering is here. Both Materials, Pipelines and Render Managers register and use the database to create the resource lists used in rendering.\nNext and more convoluted is the shader resources lookup class:\n//\r// Struct to link between a Shader Binding Name and a Resource. Used both in Pipelines and Materials.\r//\rstruct ShaderResourcesLookup {\renum Specialization {\rFrame, Pass, View, Shader\r}; // enum Specialization\rstruct NameMap {\rchar* key;\rchar* value;\r}; // struct NameMap\rstruct SpecializationMap {\rchar* key;\rSpecialization value;\r}; // struct SpecializationMap\rNameMap* binding_to_resource = nullptr;\rSpecializationMap* binding_to_specialization = nullptr;\rNameMap* binding_to_sampler = nullptr;\rvoid init();\rvoid terminate();\rvoid add_binding_to_resource( char* binding, char* resource );\rvoid add_binding_to_specialization( char* binding, Specialization specialization );\rvoid add_binding_to_sampler( char* binding, char* sampler );\rchar* find_resource( char* binding );\rSpecialization find_specialization( char* binding );\rchar* find_sampler( char* binding );\rvoid specialize( char* pass, char* view, ShaderResourcesLookup\u0026amp; final_lookup );\r}; // struct ShaderResourcesLookup This class specify the binding between a shader resource and an actual resource. As a simple example to clarify, a shader could have an \u0026lsquo;albedo\u0026rsquo; texture defined in the code, but the actual texture is defined by the material. Or for a Render Stage, like a Post-Processing one, its input could be defined in the shader code as \u0026lsquo;input 0, input 1\u0026hellip;\u0026rsquo; and the render pipeline creates the binding.\nWith those in place, we can finalize any resource used by any shader/material/pipeline.\nThe actual usage is into the Shader Instance class. Let\u0026rsquo;s have a quick look.\n//\rstruct ShaderInstance {\rvoid load_resources( const PipelineCreation\u0026amp; pipeline, PipelineHandle pipeline_handle, ShaderResourcesDatabase\u0026amp; database, ShaderResourcesLookup\u0026amp; lookup, Device\u0026amp; device );\rPipelineHandle pipeline;\rResourceListHandle resource_lists[k_max_resource_layouts];\ruint32_t num_resource_lists;\r}; // struct ShaderInstance This class is what actually contains the resource lists and pipeline used to render anything. Not very happy with the name - any suggestion welcome. A material contains a list of those - one for each pass - and is used to draw. Again with the new Vulkan/DX12 mentality, Pipeline + Resource Lists + Geometry is all you need to render almost.\nThe magic happens when creating the resource lists:\nvoid ShaderInstance::load_resources( const PipelineCreation\u0026amp; pipeline_creation, PipelineHandle pipeline_handle, ShaderResourcesDatabase\u0026amp; database, ShaderResourcesLookup\u0026amp; lookup, Device\u0026amp; device ) {\rusing namespace hydra::graphics;\rResourceListCreation::Resource resources_handles[k_max_resources_per_list];\rfor ( uint32_t l = 0; l \u0026lt; pipeline_creation.num_active_layouts; ++l ) {\r// Get resource layout description\rResourceListLayoutDescription layout;\rdevice.query_resource_list_layout( pipeline_creation.resource_list_layout[l], layout ); We know that a pipeline can have 1 or more resource lists, thus we just iterate through them. Next we look into each resource of the current list:\n// For each resource\rfor ( uint32_t r = 0; r \u0026lt; layout.num_active_bindings; r++ ) {\rconst ResourceBinding\u0026amp; binding = layout.bindings[r];\r// Find resource name\r// Copy string_buffer char* resource_name = lookup.find_resource( (char*)binding.name );\rswitch ( binding.type ) {\rcase hydra::graphics::ResourceType::Constants:\rcase hydra::graphics::ResourceType::Buffer:\r{\rBufferHandle handle = resource_name ? database.find_buffer( resource_name ) : device.get_dummy_constant_buffer();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\r... same for textures For each binding coming from the shader (think \u0026lsquo;albedo\u0026rsquo; for a PBR shader) we search for the actual resource name (\u0026lsquo;WoodBeamAlbedo\u0026rsquo;) and query the database to find it. After we did that, we can create the list:\n}\r}\rResourceListCreation creation = { pipeline_creation.resource_list_layout[l], resources_handles, layout.num_active_bindings };\rresource_lists[l] = device.create_resource_list( creation );\r}\rnum_resource_lists = pipeline_creation.num_active_layouts;\rpipeline = pipeline_handle;\r} With this mechanism we added another explicit connection between resources.\nIt is finally time to see the actual render pipeline!\nRender Stage/Pass This is the CORE of everything, and it must work with all both geometrical stages and post-process ones. You can either create a base virtual class or doing something like here. Important is understanding the concept!\n//\r// Encapsulate the rendering of anything that writes to one or more Render Targets.\r//\rstruct RenderStage {\renum Type {\rGeometry, Post, PostCompute, Swapchain, Count\r};\rType type = Count; Simply we define the types:\nGeometry - uses render manager with meshes to draw. Post - fullscreen triangle + shader. PostCompute - any compute shader execution basically! Swapchain - special case of binding the window framebuffer and render the last time. Next is the most important part: dependencies!\narray( TextureHandle ) input_textures = nullptr;\rarray( TextureHandle ) output_textures = nullptr;\rTextureHandle depth_texture; When we create the pipeline, we save all inputs and outputs textures. Depth/Stencil is a put in its own part.\nfloat scale_x = 1.0f;\rfloat scale_y = 1.0f;\ruint16_t current_width = 1;\ruint16_t current_height = 1; Here we handle scaling. When using scale, we use the framebuffer\u0026rsquo;s window width/height to calculate the Render Target size of the output ones. When using the current width/height we instead define a specific size (like for a shadow map).\nRenderPassHandle render_pass; hydra::graphics low level rendering needs this handle to actually handle the drawing.\nMaterial* material = nullptr;\ruint8_t pass_index = 0; This is for PostProcesses : material and pass index to retrieve the \u0026lsquo;shader instance\u0026rsquo; containing the pipeline and the resource lists.\nRenderView* render_view = nullptr; RenderView used by this stage. For example the \u0026lsquo;Sun Shadow Render Stage\u0026rsquo; will use the \u0026lsquo;Shadow Render View\u0026rsquo; to dispatch all its objects to each render manager.\nfloat clear_color[4];\rfloat clear_depth_value;\ruint8_t clear_stencil_value;\ruint8_t clear_rt : 1;\ruint8_t clear_depth : 1;\ruint8_t clear_stencil : 1;\ruint8_t resize_output : 1;\ruint8_t pad : 4; If the stage needs to clear its output(s), these will tell what to do.\nuint64_t geometry_stage_mask; // Used to send render objects to the proper stage. Not used by compute or postprocess stages. This creates a link between render managers and stages. An object is rendered only if its stage mask equals at least one stage. Why that ? Because when defining a render view, we have a list of objects visible from that camera, and we need a way of dispatching those objects to their respective managers.\nFor example a \u0026lsquo;dynamic render object\u0026rsquo; could have appear both on the gbuffer pass and an \u0026lsquo;object special effect\u0026rsquo; pass - both visible from the main camera.\nThis ideas comes from the AMAZING talk by Bungie:\nhttp://advances.realtimerendering.com/destiny/gdc_2015/Tatarchuk_GDC_2015__Destiny_Renderer_web.pdf\nA render manager is what they call a feature renderer - named differently because this version is much more basic!\narray( RenderManager* ) render_managers; Render Managers can register to stages even if they don\u0026rsquo;t have objects, for example a \u0026lsquo;Lighting Manager\u0026rsquo; would want to submit a list of visible light in a certain pass.\n// Interface\rvirtual void init();\rvirtual void terminate();\rvirtual void begin( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void render( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void end( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void load_resources( ShaderResourcesDatabase\u0026amp; db, Device\u0026amp; device );\rvirtual void resize( uint16_t width, uint16_t height, Device\u0026amp; device );\rvoid register_render_manager( RenderManager* manager );\r}; // struct RenderStage This is the final interface. Load resources is used for PostProcesses - they have a material and need to load its resources.\nRender Pipeline We arrived at the last piece of the puzzle!\n//\r// A full frame of rendering using RenderStages.\r//\rstruct RenderPipeline {\rstruct StageMap {\rchar* key;\rRenderStage* value;\r};\rstruct TextureMap {\rchar* key;\rTextureHandle value;\r};\rvoid init( ShaderResourcesDatabase* initial_db );\rvoid terminate( Device\u0026amp; device );\rvoid update();\rvoid render( Device\u0026amp; device, CommandBuffer* commands );\rvoid load_resources( Device\u0026amp; device );\rvoid resize( uint16_t width, uint16_t height, Device\u0026amp; device );\rStageMap* name_to_stage = nullptr;\rTextureMap* name_to_texture = nullptr;\rShaderResourcesDatabase resource_database;\rShaderResourcesLookup resource_lookup;\r}; // struct RenderPipeline This is literally IT! This class contains all the stages and resources needed to render. Most of the time it will just iterate over the stages and execute something per stage.\nResource database contains all the resources used actually - and the lookup instead is only for the PostProcess stages.\nRender Pipeline Description We really have all the part to render a frame! Let\u0026rsquo;s look at the data defining the pipeline. We will define a simple-silly-non-effective PBR deferred rendering. Probably the worst shaders you saw, but it will still work.\nFirst we define the Render Targets:\n{\r\u0026#34;name\u0026#34;: \u0026#34;PBR_Deferred\u0026#34;,\r\u0026#34;RenderTargets\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferAlbedo\u0026#34;,\r\u0026#34;format\u0026#34;: \u0026#34;R8G8B8A8_UNORM\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferNormals\u0026#34;,\r\u0026#34;format\u0026#34;: \u0026#34;R16G16B16A16_SNORM\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferProperties0\u0026#34;,\r\u0026#34;format\u0026#34;: \u0026#34;R8G8B8A8_UNORM\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;MainDepth\u0026#34;,\r\u0026#34;format\u0026#34;: \u0026#34;D24_UNORM_S8_UINT\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;BackBufferColor\u0026#34;,\r\u0026#34;format\u0026#34;: \u0026#34;R16G16B16A16_FLOAT\u0026#34;\r}\r], by default they will have the same size as the window framebuffer, unless otherwise written (scale_x/y, width/height).\nNext are the actual render stages. The first is the GBufferOpaque one:\n\u0026#34;RenderStages\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferOpaque\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;Geometry\u0026#34;,\r\u0026#34;render_view\u0026#34;: \u0026#34;main\u0026#34;,\r\u0026#34;depth_stencil\u0026#34;: \u0026#34;Main\u0026#34;,\r\u0026#34;inputs\u0026#34;: [\r],\r\u0026#34;outputs\u0026#34;: {\r\u0026#34;rts\u0026#34;: [ \u0026#34;GBufferAlbedo\u0026#34;, \u0026#34;GBufferNormals\u0026#34;, \u0026#34;GBufferProperties0\u0026#34; ],\r\u0026#34;depth\u0026#34;: \u0026#34;MainDepth\u0026#34;,\r\u0026#34;flags\u0026#34;: \u0026#34;Common\u0026#34;,\r\u0026#34;clear_color\u0026#34;: \u0026#34;000000ff\u0026#34;,\r\u0026#34;clear_depth\u0026#34;: 1.0,\r\u0026#34;clear_stencil\u0026#34;: 0\r}\r}, As you see it outputs to 3 Render Targets + Depth. It also specify clear color, depth and stencil.\nNext is the silliest compute shader to calculate light:\n{\r\u0026#34;name\u0026#34;: \u0026#34;DeferredLights\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;PostCompute\u0026#34;,\r\u0026#34;material_name\u0026#34;: \u0026#34;SimpleFullscreen\u0026#34;,\r\u0026#34;material_pass_index\u0026#34;: 2,\r\u0026#34;inputs\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferAlbedo\u0026#34;,\r\u0026#34;sampler\u0026#34;: \u0026#34;Point\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;gbuffer_albedo\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferNormals\u0026#34;,\r\u0026#34;sampler\u0026#34;: \u0026#34;Point\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;gbuffer_normals\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;GBufferProperties0\u0026#34;,\r\u0026#34;sampler\u0026#34;: \u0026#34;Point\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;gbuffer_properties0\u0026#34;\r},\r{\r\u0026#34;name\u0026#34;: \u0026#34;MainDepth\u0026#34;,\r\u0026#34;sampler\u0026#34;: \u0026#34;Point\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;depth_texture\u0026#34;\r}\r],\r\u0026#34;outputs\u0026#34;: {\r\u0026#34;images\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;BackBufferColor\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;destination_texture\u0026#34;\r}\r],\r\u0026#34;flags\u0026#34;: \u0026#34;Common\u0026#34;\r}\r}, It will read all the previously generated textures and run a compute shader to calculate the final lighting. Worth noting \u0026lsquo;material\u0026rsquo; and \u0026lsquo;material pass index\u0026rsquo; - to retrieve the shader from the material. If you open SimpleFullscreen.hfx and go to the third defined pass, you will see the code.\nNext is an example of reusing a Render Target to add informations (like transparent objects). It will add debug rendering on top of the other objects and write in the BackBufferColor render target. The absence of clear parameters dictates that we don\u0026rsquo;t want to clear.\n{\r\u0026#34;name\u0026#34;: \u0026#34;DebugRendering\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;Geometry\u0026#34;,\r\u0026#34;render_view\u0026#34;: \u0026#34;main\u0026#34;,\r\u0026#34;inputs\u0026#34;: [\r],\r\u0026#34;outputs\u0026#34;: {\r\u0026#34;rts\u0026#34;: [ \u0026#34;BackBufferColor\u0026#34; ],\r\u0026#34;depth\u0026#34;: \u0026#34;MainDepth\u0026#34;,\r\u0026#34;flags\u0026#34;: \u0026#34;Common\u0026#34;\r}\r}, Last step is the swapchain. It is simply using a simple shader to write to the window framebuffer as the last step of the frame.\n{\r\u0026#34;name\u0026#34;: \u0026#34;Swapchain\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;Swapchain\u0026#34;,\r\u0026#34;mask\u0026#34;: \u0026#34;FRAMEBUFFER\u0026#34;,\r\u0026#34;material_name\u0026#34;: \u0026#34;Swapchain\u0026#34;,\r\u0026#34;render_view\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;depth_stencil\u0026#34;: \u0026#34;Post\u0026#34;,\r\u0026#34;inputs\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;BackBufferColor\u0026#34;,\r\u0026#34;sampler\u0026#34;: \u0026#34;Point\u0026#34;,\r\u0026#34;binding\u0026#34;: \u0026#34;input_texture\u0026#34;\r}\r],\r\u0026#34;outputs\u0026#34;: {\r\u0026#34;rts\u0026#34;: [\r],\r\u0026#34;depth\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;flags\u0026#34;: \u0026#34;Common\u0026#34;,\r\u0026#34;clear_color\u0026#34;: \u0026#34;000000ff\u0026#34;\r}\r}\r]\r} Visualization With all this defined, we can arrive to have something incredibly useful as this (included in the demo!):\nRender Pipeline To me this is the quintessence of rendering: visualization. Seeing things helps me understanding much better. Debugging broken features, studying features, understanding dependencies, shuffling things around becomes MUCH easier.\nDemo and code The demo loads a model, apply a silly directional light and gives you some controls, and uses the render pipeline. It was setup during the night just to show something usable, but it is far from ideal!\nIn the code provided there is everything I am talking here.\nAnd now some links to libraries/resources used.\n3 models are included from the free GLTF library: https://github.com/KhronosGroup/glTF-Sample-Models\nTinyGLTF by Syoyo Fujita:\nhttps://github.com/syoyo/tinygltf\nThe always present-always amazing ImGui by Omar: https://github.com/ocornut/imgui\nWith the NodeEditor by Michał Cichoń: https://github.com/thedmd/imgui-node-editor\nFor the PBR rendering, kudos to the GREAT INFORMATIONS from Google Filament and Romain Guy.\nLastly, this is not anywhere near production ready, but I am still happy to share it as a knowledge building block for others. I am thinking of making some videos for this - if you are interested let me know (both in English and Italian).\nConclusions We arrived at defining the Render Pipeline - a way of describing how a frame is rendered. It is a very simplified version of the RenderGraph/FrameGraph - as seen in many talks - and this is something I\u0026rsquo;ve used in my home projects (and current indie game) with great success. No mention of adding resource barriers, sharing memory, async compute and more. The whole purpose of this article was instead to focus on the more high level architecture side.\nWhat is next ?\nI would write about the improvements on the HFX shader effect and would like to cleanup and make that library more robust. Then there is the Vulkan backend to be wrote and many examples to be done. Examples could be amazing to be tutorial and develop the technology more. Then there is talking deeper about dispatching rendering draws, render managers and such - another interesting and very unique subject in Rendering Engine architectures. In all the companies I\u0026rsquo;ve worked, I always found completely different solutions!\nPlease comment, share, send feedback! I am happy to answer any question and very happy to share this article. Thanks for reading!\nGabriel\n","date":1571064229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582383529,"objectID":"7c26dedfc7a8e73a1db210a4acccdded","permalink":"https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/","publishdate":"2019-10-14T10:43:49-04:00","relpermalink":"/post/data_driven_rendering_pipeline/","section":"post","summary":"Overview Model used in the demo. Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ We finally arrived in the Rendering Pipeline realm. Some can write that it is useless, some can hate it. Many have some sort of abstraction for it since ages, and others have to now that new APIs like Vulkan and DX12 have it as an explicit part of their design (finally!).\nAfter we built a basic Material System in the previous article (https://jorenjoestar.","tags":[],"title":"Data Driven Rendering: Pipelines","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In Part 2 of this series we added Resource Layouts and Properties to the HFX language, trying to arrive at a point in which we can describe the rendering of a Shader Effect almost entirely. In this article I would like to explore further adds to HFX, especially a proper Material System to be used in conjunction with the HFX language. I also separated the code a little bit for clarity and added the usage of STB array and hash maps. With that I would like to develop a Material System that is robust and easy to use, even though I am (DISCLAIMER!) far from it!\nI will first talk about the theory and thoughts behind those changes, and then go through the code changes and addition.\nMaterial System thoughts Following a nomenclature from the amazing guys at our_machinery, we are adding a Tier 1 Shader System - something that builds on top of the graphics API created in the previous article.\nFirst of all, a great series of article is again on their website:\nhttps://ourmachinery.com/post/the-machinery-shader-system-part-1/ https://ourmachinery.com/post/the-machinery-shader-system-part-2/ https://ourmachinery.com/post/the-machinery-shader-system-part-3/ We are building a Material System based on a graphics-API that exposes the following concepts:\nBuffer Texture Pipeline (that includes shaders) Render Pass Resource List Layout Resource List We are using a Vulkan/D3D12 interface here, and these concepts map 1 to 1 with that. One of the big changes from a typical low-level graphics API is both the \u0026lsquo;missing\u0026rsquo; concept of Shader as a resource, and the addition of Render Pass as resource. A new concept is the one of Resource List Layout and Resource List. The Vulkan names are Descriptor Set Layout and Descriptor Set, but even though they reflect more the underlying driver nature of the term, I changed to Resource List just to have it clearer as a concept.\nThe King here is the Pipeline: it is a structure that contains all the immutable data of a pipeline. That includes our missing shaders, all the render states (DepthStencil, AlphaBlend, \u0026hellip;) and a Layout of the resources to be used by the shader.\nPart of the dynamic pipeline states are normally the geometry and the resource lists. Note that I am using the plural here: each pipeline can have 1 or more resource lists!!. This is a good to organize your resources based on update frequencies, something coming from the numerous talks about Approaching Zero Driver Overhead.\nRemembering the simple interface of our API, now we have the following:\nstruct Device {\r...\rBufferHandle create_buffer( const BufferCreation\u0026amp; creation );\rTextureHandle create_texture( const TextureCreation\u0026amp; creation );\rPipelineHandle create_pipeline( const PipelineCreation\u0026amp; creation );\rSamplerHandle create_sampler( const SamplerCreation\u0026amp; creation );\rResourceListLayoutHandle create_resource_list_layout( const ResourceListLayoutCreation\u0026amp; creation );\rResourceListHandle create_resource_list( const ResourceListCreation\u0026amp; creation );\rRenderPassHandle create_render_pass( const RenderPassCreation\u0026amp; creation );\r...\r}; If you look at the OpenGL implementation (the only I wrote for now :( ) you will find that Shaders are considered resources, but it is more for convenience of the attach/linking OpenGL need than anything else.\nLet\u0026rsquo;s finally introduce the new concept for this article!\nShader Effect A Shader Effect is the blueprint of static data needed to draw something on the screen.\nIt needs to include shaders (included in a Pipeline Description), properties (coming from the HFX file) and find its location into a render graph.\nA Shader Effect is 1 to 1 with a Binary HFX file.\nAs a convenience we will add also informations about the local constants. When creating a Shader Effect, we can define properties, and we put all the numerical properties into one buffer.\nThis is the current code:\nstruct ShaderEffect {\r//\r//\rstruct PropertiesMap {\rchar* key;\rhfx::ShaderEffectFile::MaterialProperty* value;\r}; // struct PropertiesMap\rstruct Pass {\rPipelineCreation pipeline_creation;\rchar name[32];\rPipelineHandle pipeline_handle;\ruint32_t pool_id;\r}; // struct Pass\rPass* passes;\ruint16_t num_passes = 0;\ruint16_t num_properties = 0;\ruint32_t local_constants_size = 0;\rchar* local_constants_default_data = nullptr;\rchar* properties_data = nullptr;\rPropertiesMap* name_to_property = nullptr;\rchar name[32];\rchar pipeline_name[32];\ruint32_t pool_id;\r}; // struct ShaderEffect You see both a pipeline name and an array of passes with a name. These are to insert the pass into a very primordial render graph, that I wrote just because I didn\u0026rsquo;t want to hardcode the frame structure, especially because next article will be EXACTLY on this topic!\nHaving defined the Shader Effect, we can now move into the next big actor.\nMaterial A Material is an instance of a Shader Effect.\nGiven a HFX file, we generate a new file (HMT, Hydra Material) that will contain all the informations. The concept of Material is really unique values for the properties of a Shader Effect.\nThat is basically it.\nFor example, if a shader contains a property like an albedo texture, the material answer the question \u0026ldquo;which albedo texture?\u0026rdquo;. This is done for every property.\nLet\u0026rsquo;s have a look at our new material:\n{\r\u0026#34;name\u0026#34;: \u0026#34;SimpleFullscreen\u0026#34;,\r\u0026#34;effect_path\u0026#34;: \u0026#34;SimpleFullscreen.hfx\u0026#34;,\r\u0026#34;properties\u0026#34;: [\r{\r\u0026#34;scale\u0026#34;: 16.0,\r\u0026#34;albedo\u0026#34;: \u0026#34;AngeloCensorship.png\u0026#34;,\r\u0026#34;modulo\u0026#34;: 2.0\r}\r],\r\u0026#34;bindings\u0026#34;: [\r{\r\u0026#34;LocalConstants\u0026#34;: \u0026#34;LocalConstants\u0026#34;,\r\u0026#34;destination_texture\u0026#34;: \u0026#34;compute_output_texture\u0026#34;,\r\u0026#34;input_texture\u0026#34;: \u0026#34;compute_output_texture\u0026#34;,\r\u0026#34;albedo_texture\u0026#34;: \u0026#34;albedo\u0026#34;\r}\r]\r} As you can see there is a name, the effect path, the properties and the bindings. These will be explained in the next section. Properties are just a name-value list, coming from the Shader Effect itself (the .bhfx file).\nThe texture is my horrible drawing after reading the fantastic rendering guide by Angelo Pesce and how he censored the parts that were internal to Roblox!\nShader Resource Database and Lookup A concept that I saw only in the our_machinery posts, but I personally adopted since a couple of years, is a way of automating a daunting task: setting shader resources.\nI still need to finish the correct implementation of those, but the concepts are simple. A Shader Resource Database is a database of resources that can be searched using a Shader Resources Lookup. The name of the binding is the shader related name, while the value is the name into the database. Of course you can use hashes instead of names, and compile them into a binary version of this, but this is not important now.\nOne interesting bit (sadly not implemented here, sorry!) is the binding specialization. This is done so that resources can be specialized in the database. This is done per pass and it let you write only one binding list for all the Material, and then gather the proper resource based on the specialization. For example if there is a binding for a pass-dependent resource, writing a generic version can specialize the shader pass correctly. Or using special keywords in the bindings, you can retrieve input/output textures from the render pass in which the shader is rendered!\nFor now though it is more a manual written list, but it will be developed further.\nWhere is my code ? Having introduced the new concept, let\u0026rsquo;s look at the changes that happened in the last weeks of night coding. As said before, in general I separated the code in header/cpp for clarity and building performances (after a good talk on Twitter, https://twitter.com/GabrielSassone/status/1179810419617275905?s=20).\nApplications First big changes was separating the code from the previous articles in an application: namely CustomShaderLanguageApplication in CustomShaderLanguage.h/cpp and MaterialSystemApplication in MaterialSystem.h/cpp.\nThe first contains all the application code that uses HDF and HFX, with code generation and HFX compilation. The second contains both the new Material System and the application that uses it. I would love to say that is an usable app, but I really touched my limits in non designing clearly when night coding. Personal note: I hope this could be the spark to create a FX Composer successor, open source and free for all!\nSTB As part of this experiment I wanted to try something different. Instead of re-writing array and hash maps with templates, I wanted to give a try to the STB libraries: namely stb_ds.h and stb_image.h. Arrays and Hash Maps are now included in hydra_lib.h to be used across the code.\nHydra Graphics The device added render passes and the support for multiple resources layout. It also creates FBOs for color passes and supports resize, especially thanks to the Render Pipeline.\nPrimitive Render Graph (called Render Pipeline) I use the term I used since the inception in 2010, and honestly it is more true to what it does. It is not a graph but more a list of Render Stages with input/outputs defined clearly. In the next article I will develop more on this, but for now I needed some structure like this to be explicit.\nIn the application there are 3 pipelines, one for a single pass ShaderToy shader, one for a silly compute to framebuffer shader(that for now loads a texture and outputs it to the framebuffer), and one for just a render to window.\nI use this in my indie project, with a fully custom and data driven (written in json) pipeline that includes compute deferred lighting and shadows, shadow passes, various post-process passes and such, everything very easy to debug and very easy to modify/add/delete. There is a mechanism to send the correct draw calls to the correct pass through the usage of render systems, but again this will be a topic for the next article!\nIn the included code, there is also a small but powerful tool: a pipeline explorer. For now it will just show the render targets for each stage, and in these simple examples does not matter much. In the next article we will dive deep into the Render Pipeline/Graph subject and then all of this will make sense!\nBreak: a simple Resource Manager While being a very important topic, this is not the focus of this article. Anyway I wanted a Resource Manager that would be helpful to handle resource creation and loading. This includes also resource compilation, something that normally happens at build time, but in our exercise can be triggered at run-time.\nThe resource manager is a class that simply manages resources using factories and manages dependencies between resources. We have only 3 resources for now:\nTextures Shader Effects Materials Resources A resource is a class that both has data and let the dependency with other data be clear. The resource\u0026rsquo;s data is actually a pointer to actual raw data used by other systems, in this case rendering. Let\u0026rsquo;s see its definition:\nstruct Resource {\rstruct ResourceReference {\ruint8_t type;\rchar path[255];\r}; // struct ResourceReference\rstruct Header {\rchar header[7];\ruint8_t type; // ResourceType::enum\rsize_t data_size;\ruint16_t num_external_references;\ruint16_t num_internal_references;\r}; // struct Header\rstruct ResourceMap {\rchar* key;\rResource* value;\r};\rHeader* header;\rchar* data;\rvoid* asset;\rResource::ResourceReference* external_references;\r// External\rResourceMap* name_to_external_resources;\r// Interal\r}; // struct Resource A resource is loaded from a binary file and contains a header and some data coming from the file, and an asset containing a system specific pointer.\nWe added 3 system specific resources (Texture, Shader Effect and Material) but the class handled is always resource. To access the system specific data, asset member is used.\nA resource contains also a map to the external resources loaded within it - to handle external references.\nCompilation Starting from a source file (.hfx, .png, .hmt) using the specific factory, the resource manager compiles the code to a binary resource. This means both converting the source format to a binary representation but also adding external dependencies to the file. These dependencies will be loaded when loading the resource, and before it.\nLoading Loading happens by first loading all the dependent resources and then using the specific factory to load the system specific asset. This is a very semplicistic resource management - synchronous only, single threaded, not optimized - so really was an exercise in having something running for both compiling a resource and managing dependencies. The whole point is the separation between a source and human-readable format to a binary one and encapsulate this.\nAfter this (very!) small break on resource management, let\u0026rsquo;s continue to the actual code for the materials!\nMaterial System implementation After all this thory let\u0026rsquo;s look at the code!\nShader Effect The main parts of a Shader Effect are Passes and Properties. Passes are the most important one, as they contain all the informations to create an actual Pipeline, called Pipeline Creation.\nRemembering the Vulkan/DX12 interface, we cannot create singularly a shader, but we need all the pipeline data (depth stencil, alpha blend, \u0026hellip;) to actually create the shaders too.\nThe gist here is to access all those informations in a hiearchical way, basically reading them from the RenderPipeline and then overwriting with what is defined in the HFX file.\nRight now there is almost nothing if not the shaders, so the creation is quite simple:\nfor ( uint16_t p = 0; p \u0026lt; shader_effect_file.header-\u0026gt;num_passes; p++ ) {\rhfx::ShaderEffectFile::PassHeader* pass_header = hfx::get_pass( shader_effect_file, p );\ruint32_t shader_count = pass_header-\u0026gt;num_shader_chunks;\rmemcpy( effect-\u0026gt;passes[p].name, pass_header-\u0026gt;stage_name, 32 );\rPipelineCreation\u0026amp; pipeline_creation = effect-\u0026gt;passes[p].pipeline_creation;\rShaderCreation\u0026amp; creation = pipeline_creation.shaders;\rbool compute = false;\r// Create Shaders\rfor ( uint16_t i = 0; i \u0026lt; shader_count; i++ ) {\rhfx::get_shader_creation( pass_header, i, \u0026amp;creation.stages[i] );\rif ( creation.stages[i].type == ShaderStage::Compute )\rcompute = true;\r}\rcreation.name = pass_header-\u0026gt;name;\rcreation.stages_count = shader_count;\reffect-\u0026gt;passes[p].pipeline_creation.compute = compute;\r// Create Resource Set Layouts\rfor ( uint16_t l = 0; l \u0026lt; pass_header-\u0026gt;num_resource_layouts; l++ ) {\ruint8_t num_bindings = 0;\rconst ResourceListLayoutCreation::Binding* bindings = get_pass_layout_bindings( pass_header, l, num_bindings );\rResourceListLayoutCreation resource_layout_creation = { bindings, num_bindings };\rpipeline_creation.resource_list_layout[l] = context.device.create_resource_list_layout( resource_layout_creation );\r}\rpipeline_creation.num_active_layouts = pass_header-\u0026gt;num_resource_layouts;\r// Create Pipeline\reffect-\u0026gt;passes[p].pipeline_handle = context.device.create_pipeline( pipeline_creation );\rif ( effect-\u0026gt;passes[p].pipeline_handle.handle == k_invalid_handle ) {\rinvalid_effect = true;\rbreak;\r}\r} When we will have a proper RenderPipeline, we will get the basic pipeline creation from there, and overwrite the shaders and states that will be defined in the HFX.\nThere are 3 main steps here:\nCreate Shaders Create Resource Set Layouts Create Pipelines These are simple operations that rely heavily on the device. The objective of the HFX is to embed most information possible to create a complete pipeline.\nAnother important step is to populate the properties map:\nstring_hash_init_arena( effect-\u0026gt;name_to_property );\rfor ( uint32_t p = 0; p \u0026lt; effect-\u0026gt;num_properties; ++p ) {\rhfx::ShaderEffectFile::MaterialProperty* property = hfx::get_property( effect-\u0026gt;properties_data, p );\rstring_hash_put( effect-\u0026gt;name_to_property, property-\u0026gt;name, property );\r} We are using the STB String Hashmap here with the property that are inside the shader effect file. Those will contain the type, name for UI and the pointer to a default value. The default value will be used based on the type of course.\nWe are also saving the local constant buffer size, so that we can allocate some memory in the Material and alter its property using the UI.\nWe will see the importance of this next.\nMaterial struct ShaderInstance {\rPipelineHandle pipeline;\rResourceListHandle resource_lists[hydra::graphics::k_max_resource_layouts];\ruint32_t num_resource_lists;\r}; // struct ShaderInstance\rstruct Material {\rShaderInstance* shader_instances = nullptr;\ruint32_t num_instances = 0;\rShaderResourcesLookup lookups; // Per-pass resource lookup. Same count as shader instances.\rShaderEffect* effect = nullptr;\rBufferHandle local_constants_buffer;\rchar* local_constants_data = nullptr;\rconst char* name = nullptr;\rStringBuffer loaded_string_buffer;\ruint32_t num_textures = 0;\ruint32_t pool_id = 0;\rTexture** textures = nullptr;\r}; // struct Material This is the glue to actually render something on the screen. As a recap, we need 3 informations to render something:\nPipeline (shaders + render states) Resources (handles to buffers and textures) Geometry (in this case a fullscreen quad) Material gives all those informations.\nA Shader Instance is defined for each pass, and actually contains the Pipeline Handle and the List of Resource Lists to be used. This is one of the new concepts for Vulkan/DX12: you can use one of more lists of resources to render, and normally it is better to group them by frequency.\nFinally, a list of textures is saved to be modified by the editor.\nTo understand more the process, let\u0026rsquo;s look at the loading code of a Material.\nvoid* MaterialFactory::load( LoadContext\u0026amp; context ) {\rusing namespace hydra::graphics;\r// 1. Read header from file\rMaterialFile material_file;\rmaterial_file.header = (MaterialFile::Header*)context.data;\rmaterial_file.property_array = (MaterialFile::Property*)(context.data + sizeof( MaterialFile::Header ));\rmaterial_file.binding_array = (MaterialFile::Binding*)(context.data + sizeof( MaterialFile::Header ) + sizeof( MaterialFile::Property ) * material_file.header-\u0026gt;num_properties); We are using the data from the material file to access properties and bindings. Properties are both numerical and path to textures, bindings are name to retrieve resources from the database. We will look into that later.\n// 2. Read shader effect\rResource* shader_effect_resource = string_hash_get( context.resource-\u0026gt;name_to_external_resources, material_file.header-\u0026gt;hfx_filename );\rShaderEffect* shader_effect = shader_effect_resource ? (ShaderEffect*)shader_effect_resource-\u0026gt;asset : nullptr;\rif ( !shader_effect ) {\rreturn nullptr;\r}\r// 3. Search pipeline\rRenderPipeline* render_pipeline = string_hash_get( context.name_to_render_pipeline, shader_effect-\u0026gt;pipeline_name );\rif ( !render_pipeline ) {\rreturn nullptr;\r} Access the Shader Effect through the resource dependencies, and the Render Pipeline from the map.\n// 4. Load material\rchar* material_name = material_file.header-\u0026gt;name;\ruint32_t pool_id = materials_pool.obtain_resource();\rMaterial* material = new (materials_pool.access_resource(pool_id))Material();\rmaterial-\u0026gt;loaded_string_buffer.init( 1024 );\rmaterial-\u0026gt;pool_id = pool_id;\r// TODO: for now just have one lookup shared.\rmaterial-\u0026gt;lookups.init();\r// TODO: properly specialize.\r// For each pass\r//for ( uint32_t i = 0; i \u0026lt; effect-\u0026gt;num_pipelines; i++ ) {\r// PipelineCreation\u0026amp; pipeline = effect-\u0026gt;pipelines[i];\r// //final ShaderBindings specializedBindings = bindings.specialize( shaderTechnique.passName, shaderTechnique.viewName );\r// //shaderBindings.add( specializedBindings );\r//}\rmaterial-\u0026gt;effect = shader_effect;\rmaterial-\u0026gt;num_instances = shader_effect-\u0026gt;num_passes;\rmaterial-\u0026gt;shader_instances = new ShaderInstance[shader_effect-\u0026gt;num_passes];\rmaterial-\u0026gt;name = material-\u0026gt;loaded_string_buffer.append_use( material_name );\rmaterial-\u0026gt;num_textures = material_file.header-\u0026gt;num_textures;\rmaterial-\u0026gt;textures = (Texture**)hydra::hy_malloc( sizeof( Texture* ) * material-\u0026gt;num_textures ); Here is the meaty part. We create the Material, initialize a StringBuffer used to store all the names found in the file, init the db-\u0026gt;resource lookup and create the ShaderInstance array.\n// Init memory for local constants\rmaterial-\u0026gt;local_constants_data = (char*)hydra::hy_malloc( shader_effect-\u0026gt;local_constants_size );\r// Copy default values to init to sane valuess\rmemcpy( material-\u0026gt;local_constants_data, material-\u0026gt;effect-\u0026gt;local_constants_default_data, material-\u0026gt;effect-\u0026gt;local_constants_size ); We cached the constant data size to allocate its memory, and we copy the default values in it. This memory will be overwritten by the other numerical properties and used to initialize the local constant buffer.\n// Add properties\ruint32_t current_texture = 0;\rfor ( size_t p = 0; p \u0026lt; material_file.header-\u0026gt;num_properties; ++p ) {\rMaterialFile::Property\u0026amp; property = material_file.property_array[p];\rhfx::ShaderEffectFile::MaterialProperty* material_property = string_hash_get( material-\u0026gt;effect-\u0026gt;name_to_property, property.name );\rswitch ( material_property-\u0026gt;type ) {\rcase hfx::Property::Texture2D:\r{\rconst char* texture_path = material-\u0026gt;loaded_string_buffer.append_use( property.data );\rResource* texture_resource = string_hash_get( context.resource-\u0026gt;name_to_external_resources, texture_path );\rTexture* texture = (Texture*)texture_resource-\u0026gt;asset;\rtexture-\u0026gt;filename = texture_path;\rrender_pipeline-\u0026gt;resource_database.register_texture( property.name, texture-\u0026gt;handle );\rmaterial-\u0026gt;textures[current_texture] = texture;\r++current_texture;\rbreak;\r}\rcase hfx::Property::Float:\r{\rmemcpy( material-\u0026gt;local_constants_data + material_property-\u0026gt;offset, property.data, sizeof( float ) );\rbreak;\r}\r}\r} When cycling through the properties, we are copying the numerical properties into the newly allocated memory (local_constant_data) and we load the textures from the dependencies.\n// Add bindings\rfor ( size_t b = 0; b \u0026lt; material_file.header-\u0026gt;num_bindings; ++b ) {\rMaterialFile::Binding\u0026amp; binding = material_file.binding_array[b];\rchar* name = material-\u0026gt;loaded_string_buffer.append_use( binding.name );\rchar* value = material-\u0026gt;loaded_string_buffer.append_use( binding.value );\rmaterial-\u0026gt;lookups.add_binding_to_resource( name, value );\r} We populate the resource lookups.\nBufferCreation checker_constants_creation = {};\rchecker_constants_creation.type = BufferType::Constant;\rchecker_constants_creation.name = s_local_constants_name;\rchecker_constants_creation.usage = ResourceUsageType::Dynamic;\rchecker_constants_creation.size = shader_effect-\u0026gt;local_constants_size;\rchecker_constants_creation.initial_data = material-\u0026gt;local_constants_data;\rmaterial-\u0026gt;local_constants_buffer = context.device.create_buffer( checker_constants_creation );\rrender_pipeline-\u0026gt;resource_database.register_buffer( (char*)s_local_constants_name, material-\u0026gt;local_constants_buffer ); Generate the actual constant buffer.\n// Bind material resources\rupdate_material_resources( material, render_pipeline-\u0026gt;resource_database, context.device ); And finally search the bindings for the resources.\nstatic void update_material_resources( hydra::graphics::Material* material, hydra::graphics::ShaderResourcesDatabase\u0026amp; database, hydra::graphics::Device\u0026amp; device ) {\rusing namespace hydra::graphics;\r// Create resource list\r// Get all resource handles from the database.\rResourceListCreation::Resource resources_handles[k_max_resources_per_list];\r// For each pass\rfor ( uint32_t i = 0; i \u0026lt; material-\u0026gt;effect-\u0026gt;num_passes; i++ ) {\rPipelineCreation\u0026amp; pipeline = material-\u0026gt;effect-\u0026gt;passes[i].pipeline_creation;\rfor ( uint32_t l = 0; l \u0026lt; pipeline.num_active_layouts; ++l ) {\r// Get resource layout description\rResourceListLayoutDescription layout;\rdevice.query_resource_list_layout( pipeline.resource_list_layout[l], layout );\r// For each resource\rfor ( uint32_t r = 0; r \u0026lt; layout.num_active_bindings; r++ ) {\rconst ResourceBinding\u0026amp; binding = layout.bindings[r];\r// Find resource name\r// Copy string_buffer char* resource_name = material-\u0026gt;lookups.find_resource( (char*)binding.name );\rswitch ( binding.type ) {\rcase hydra::graphics::ResourceType::Constants:\rcase hydra::graphics::ResourceType::Buffer:\r{\rBufferHandle handle = resource_name ? database.find_buffer( resource_name ) : device.get_dummy_constant_buffer();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\rcase hydra::graphics::ResourceType::Texture:\rcase hydra::graphics::ResourceType::TextureRW:\r{\rTextureHandle handle = resource_name ? database.find_texture( resource_name ) : device.get_dummy_texture();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\rdefault:\r{\rbreak;\r}\r}\r}\rResourceListCreation creation = { pipeline.resource_list_layout[l], resources_handles, layout.num_active_bindings };\rmaterial-\u0026gt;shader_instances[i].resource_lists[l] = device.create_resource_list( creation );\r}\rmaterial-\u0026gt;shader_instances[i].num_resource_lists = pipeline.num_active_layouts;\rmaterial-\u0026gt;shader_instances[i].pipeline = material-\u0026gt;effect-\u0026gt;passes[i].pipeline_handle;\r}\r} For each Pass, Resource Layout and Binding, we search the Database to retrieve the actual resource and create the Resource List.\nThis can be improved - having a global database of resources and a \u0026rsquo;local\u0026rsquo; one based on material resources.\n// 5. Bind material to pipeline\rfor ( uint8_t p = 0; p \u0026lt; shader_effect-\u0026gt;num_passes; ++p ) {\rchar* stage_name = shader_effect-\u0026gt;passes[p].name;\rhydra::graphics::RenderStage* stage = string_hash_get( render_pipeline-\u0026gt;name_to_stage, stage_name );\rif ( stage ) {\rstage-\u0026gt;material = material;\rstage-\u0026gt;pass_index = (uint8_t)p;\r}\r}\rreturn material;\r} Finally, and this is hacky, we assing the current material and pass index to the found stage. Once we have the real Render Pipeline/Graph working, we will use another dispatching mechanism.\nRendering of a Material After all of this we finally have created a Material. But how can we render it ? The magic here happens in a Render Pipeline! A Render Pipeline is a list of Render Stages and some resources with it. In this case resources are the render targets and the buffers that are shared amongst Stages (and Render Systems in the future). Resources are inside a Shader Resources Database and they can be retrieved using a Shader Resource Lookup.\nEach Render Stage has defined a list of input and output textures plus some resize data. This data is needed to recreate textures when a resize event arrives if needed, or change size if an option is changed (like a Shadow Map resolution option). As everthing in this articles, this is primordial and simple, but I think is a very good start, especially from a mindset perspective.\nIn this simple scenario we render 1 material only, and normally it simply 1 Material Pass for each Render Stage Pass, rendering either using a fullscreen quad or through compute.\nThere are 2 pipelines, both simple and used as a test, one is for a ShaderToy shader that I use as test, the other as a compute only pipeline. They are both hardcoded and created at the beginning of the Material Application, but as said before, it should be data-driven and reloadable to have great rendering power.\nRendering of a Pipeline The code is simple:\nvoid RenderPipeline::render( CommandBuffer* commands ) {\rfor ( size_t i = 0; i \u0026lt; string_hash_length( name_to_stage ); i++ ) {\rRenderStage* stage = name_to_stage[i].value;\rstage-\u0026gt;begin( commands );\rstage-\u0026gt;render( commands );\rstage-\u0026gt;end( commands );\r}\r} We cycle through each stage and render.\nvoid RenderStage::begin( CommandBuffer* commands ) {\rcommands-\u0026gt;begin_submit( 0 );\rcommands-\u0026gt;begin_pass( render_pass );\rcommands-\u0026gt;set_viewport( { 0, 0, (float)current_width, (float)current_height, 0.0f, 1.0f } );\rif ( clear_rt ) {\rcommands-\u0026gt;clear( clear_color[0], clear_color[1], clear_color[2], clear_color[3] );\r}\rcommands-\u0026gt;end_submit();\r// Set render stage states (depth, alpha, ...)\r} Before rendering anything this code will bind the correct FBO/Render Targets, clear and set viewport and set render states. After this we are ready to render the actual stage. In this simple implementation we have only 3 type of stages: Compute, Post and Swapchain.\nThey are very simple and similar, like this:\ncommands-\u0026gt;begin_submit( pass_index );\rcommands-\u0026gt;bind_pipeline( shader_instance.pipeline );\rcommands-\u0026gt;bind_resource_list( \u0026amp;shader_instance.resource_lists[0], shader_instance.num_resource_lists );\rcommands-\u0026gt;draw( graphics::TopologyType::Triangle, 0, 3 );\rcommands-\u0026gt;end_submit(); Set the pipeline, bind all the different resource lists and issue the draw (in this case a full screen triangle).\nIncluded in the code Material application I just added a simple Material Application to render the content of one of those simple shaders.\nHonestly not very happy about the code quality - and you can see why trying to add big features like memory management or multi-threading is a no-go.\nThe application let you switch between materials by right clicking on the .hmt file. The whole purpose is to explore with the given code a couple of materials and their dependencies.\nStarnest is a shader by the amazing Pablo Roman Andrioli, so all credits are to him! I wanted something beautiful to show in this simple example from ShaderToy.\nConclusions and some thoughts We added a simple material system based on our HFX language. Interestingly enough code generation is used much less - if almost nothing - instead of serializing data into files and using them. As stated in the other articles, the goal is to have a parsing and code generation knowledge under your belt, and understand when it is time to use it! We also introduced a lot of connections to other topics that are lengthy enough - like resource management - that need more time and dedication to properly be explored. I am continuing working on this until it will become my rendering explorer - a tool I can use to easily explore ideas, much like ShaderToy but in an even more powerful way. How ? In the next article we will explore the final piece of the puzzle, and then we will probably start iterating and improving on what we have! We will see how we can describe a frame and the rendering dependencies in an easy way, especially if done since the beginning, and how much having that knowledge upfront is GREAT to work on rendering.\nI am honestly not happy about the overall architecture though - here you have an example of exploring code - code written to explore a specific subject, and after venturing more into it you want to rewrite it. To properly rewrite it you need to create solid foundations - namely Memory Management, Multi-Threading, Basic Data Structures, \u0026hellip; and choose to pick your battles!\nThis is a huge lesson: pick your battles, choose what to concentrate on. These articles are more towards code generation and rendering, but defining the constraints of the articles helps in narrowing down what to do. If, as I would like, you want to use this code to evolve into something like a \u0026lsquo;desktop\u0026rsquo; Shadertoy, then you can\u0026rsquo;t ignore all the foundational topics. On the other end if you just quickly want to experiment with those topics, this should suffice.\nI have two paths here: rewriting most of this code with a solid foundations, and delaying a RenderPipeline/Graph article, or finishing with this architecture and then re-write everything with the \u0026lsquo;desktop Shadertoy\u0026rsquo;. Again, pick your battles :)\nAs always, please comment, feedback, share! I really hope soon there will be some rendering joy!\nGabriel\n","date":1571064229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576767529,"objectID":"f35b15163ee0411fa4b054c437381030","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_3/","publishdate":"2019-10-14T10:43:49-04:00","relpermalink":"/post/writing_shader_effect_language_3/","section":"post","summary":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In Part 2 of this series we added Resource Layouts and Properties to the HFX language, trying to arrive at a point in which we can describe the rendering of a Shader Effect almost entirely. In this article I would like to explore further adds to HFX, especially a proper Material System to be used in conjunction with the HFX language. I also separated the code a little bit for clarity and added the usage of STB array and hash maps.","tags":[],"title":"Writing a Shader Effect Language Part 3: Materials","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In Part 1 of this series we created a simple language to work as \u0026lsquo;shader effect\u0026rsquo; - a shader language superset to make our life easier, by adding missing features. The fact that there is not an industry standard for a shader effect language leads to either hand-crafted (and secret) languages, or to hardcoded permutations, or to other gray-area solutions.\n(Personal though: part of me would like to help in contributing to the creation of a standard through these articles and code.)\nWhat is the goal of this article ?\nThe goal is to enrich the HFX language to generate more code possible and/or bake data for us, namely:\nShader constants generation Shader resource bindings Render states (depth stencil, blend, rasterization) Render pass hints for a future framegraph We will see Render States and Render Pass hints in a following article, because this is an already lengthy article!\nI hope that by now the way of adding an identifier, parsing it and generating code is clearer. In this article we will focus more on the features than anything else, even though I will put a lot of code still. But before that, we need to have a big addition to our example: a rendering API! We will use this as target of our code generation, and it will be an amazing example to see something working.\nMaybe this will spark a new FX Composer ?\nThis article will be divided in 2 parts. Part 1 of this article will talk about the rendering API. Part 2 will be about the extended HFX language. If you are not interested in that, jump to part 2 of this article.\nPart 1: adding a low-level rendering API Writing articles on rendering without some sort of API to use is tricky. Creating a language to speed up data driven rendering, either for generating code and/or for baking data needs a target API. The main idea is to have an abstract API to map more easily rendering concepts instead of losing ourselves in specific API needs.\nThe search for an abstract API The first thing to do is to search for an existing abstract API. I have few criteria in mind:\nSimple and clear interface Compact and clear code Vulkan and D3D12 interface With those in mind, I found 2 alternatives: BGFX and Sokol.\nI am an honest fan of both, they are brilliant, robust and well written. But for the purpose of these articles, sadly they miss my search criteria. There is also a huge disclaimer here: I used them too little, so it is possible I overlooked the usage of them. I will be more than glad to use either instead of my toy API! I respect the developers and the library a lot, they are doing an amazing job! But we are handcrafting something, and to properly do that I personally need to know deeply the code. And I am not.\nBGFX is very complete, but the interface is a little confusing for me, possibly because I never used it but just read the code few times. The main reason I chose not to use it is because the interface is missing the resource interface like Vulkan and D3D12 (DescriptorSets, \u0026hellip;), otherwise it would have been an amazing choice.\nSokol is also very good, I love the code and the simple interface. Two main problems here: again no Vulkan/D3D12 resource interface, and in this case a different target: it does not support compute shaders.\nAgain, I want to make it clear: I am not saying these are not good libraries. They are amazing. They just don\u0026rsquo;t fit my search criteria, plus I LOVE to work on rendering architecture. Well actually, it is my favourite job!\nSo kudos to them (I also wrote to Andre Weissflog to ask for compute shader support, but it is not in his plans for now) but we are making a different choice.\nIf you ever find anything that I write useful guys, please let me know!\nHydra Graphics: design principles Small trivia: the name comes from my first ever graphics engine written in 2006 (I think), after devouring 3D Game Engine Design by Dave Eberly. I already knew I would write many engines and I would learn and grow stronger from every of them, so I chose the name Hydra from the Greek mythology monster. The other name would have been Phoenx engine, but I remember finding already some tech with that name.\nAnyway, design principles! I really loved the interface of Sokol, and often I used something similar by myself. I opted for a pair of header/implementation files as the only needed files.\nThe backend is OpenGL for now, just because I have a working implementation in my indie project that works with pretty complex rendering, and I can use that as reference.\nInterface Rendering in general is a matter of creating, modifying and combining resources. There are mainly 2 classes that do all the rendering work:\nDevice Command Buffer The Device is responsible for creation, destruction, modification and query of the resources. The Command Buffer is responsible for the usage of resources for rendering.\nThe obvious fundamental concept is resource. A resource is handled externally through handles, can be created using creation structs and has both a common and an API-specific representation.\nBuffers are specialized in vertex/index/constant/\u0026hellip; depending on their creation parameters.\nThis is a small example on creation/usage/destruction of a resource. First, we can create a texture:\ngraphics::TextureCreation first_rt = {};\rfirst_rt.width = 512;\rfirst_rt.height = 512;\rfirst_rt.render_target = 1;\rfirst_rt.format = graphics::TextureFormat::R8G8B8A8_UNORM;\rTextureHandle render_target = gfx_device.create_texture( first_rt ); Next we can create a command buffer:\nCommandBuffer* commands = gfx_device.get_command_buffer( graphics::QueueType::Graphics, 1024 ); Skipping other creations, we bind resources and add the commands:\ncommands-\u0026gt;bind_pipeline( first_graphics_pipeline );\rcommands-\u0026gt;bind_resource_set( gfx_resources );\rcommands-\u0026gt;bind_vertex_buffer( gfx_device.get_fullscreen_vertex_buffer() );\rcommands-\u0026gt;draw( graphics::TopologyType::Triangle, 0, 3 ); At this point we can execute the command buffer to draw.\ngfx_device.execute_command_buffer( commands ); Updating a resource can be done like that:\nhydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\rLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters ); Everything uses structs to perform creation/updates. Nothing new, but I always loved this design.\nResource layout and resource lists I wanted to bring the Vulkan/D3D12 resource interface as first class citizens, and remove completely old concepts (like single constants, render states as single objects, single bind of a resource) and add new ones: resource layout, resource lists and command buffers. Well command buffers are not new, but finally you can draw only with those!\nIn Vulkan/D3D12 you can bind resources through the usage of sets: basically tables that contains the resources used. This is a welcomed difference from previous APIs, and I think it is a concept not too hard to grasp but very useful to have it explicit.\nThe first thing to define is the resource layout describes the layout of a set of resources. For example, if we have a material that uses Albedo and Normals textures and a constant buffer, the layout will contain all the informations about that (like the type, the GPU registers and so on). This though still does not contain the resources themselves! Enter resource list. A resource list is a list of actual resources relative to a layout. It sets resources using a layout.\nFrom now on, when we draw we can bind one or more resource lists.\nIn Vulkan lingo, the resource layout is called descriptor set layout, and a resource list is a descriptor set. Here are a couple of articles for the Vulkan side:\nOfficial Vulkan Documentation on Descriptor Layouts and Sets\nIntel API Without Secrets Part 6\nSimilarly in D3D12 there are Root Tables and Descriptor Tables. The concepts do no map 1 to 1 but they are pretty similar:\nD3D12 Descriptor Tables\nI tried to map these concepts using different words that would make more sense to me, so from Descriptor Set or Root Table it became Resource List and Resource Layout.\nPipelines Finally a pipeline is the complete description of what is needed by the GPU to draw something on the screen (or to use a Compute Shader for any other purpose). Basically a pipeline must fill all the informations for all the GPU stages like this (thanks to RenderDoc):\nRenderDoc Pipeline What once was setup individually now is all in one place (reflecting what happened behind the scene, into the driver). DepthStencil, AlphaBlend, Rasterization, Shaders, all must be defined here.\nIn the currrent implementation of the graphics-API a lot of states are still missing!.\nNow that we say the basic principles of the target rendering API, we can finally concentrate on the new freatures of HFX.\nPart 2: forging the HFX language features Our HFX language needs some properties to be added but first there is a change: HFX will generate a binary version to embed all the informations needed to create a shader.\nHFX evolution: what files are generated ? In the previous article, we used a single HFX file to generate multiple glsl files, ready to be used by any OpenGL renderer:\nShader Generation Remembering the article on Hydra Data Format, we instead were generating an header file. For our needs, we will generate an embedded HFX (binary HFX) AND a C++ header:\nBinary and Header Generation What is the next step for HFX ? For shader generation, we want ideally to load a HFX file without having to manually stick together the single shader files, and that is why the first step is to create embedded HFX files. This will contain all the information to create a shader, and this includes also the resource layouts.\nFor constant handling, we want to have UI generated and easy update on the gpu. We want to automate these things. This can be done in a more code-generated way or by generating data.\nIf we abstract the problem, all these articles are about understanding how you want to generate code or data to maximise iteration time, performances and control. By moving the HFX to being binary, we are effectively generating data used by the renderer. For the shader UI, we can do both: generate code or create data. We will see the generated code part here.\nLet\u0026rsquo;s see briefly the internals of the Embedded HFX file format:\nEmbedded HFX As a Recap, when parsing HFX we store some informations.\nFirst is the CodeFragment, including also (spoiler!) the addition of resources for the sake of this article:\n// C++\r//\rstruct CodeFragment {\rstruct Resource {\rhydra::graphics::ResourceType::Enum type;\rStringRef name;\r}; // struct Resource\rstd::vector\u0026lt;StringRef\u0026gt; includes;\rstd::vector\u0026lt;Stage\u0026gt; includes_stage; // Used to separate which include is in which shader stage.\rstd::vector\u0026lt;Resource\u0026gt; resources; // Used to generate the layout table.\rStringRef name;\rStringRef code;\rStage current_stage = Stage::Count;\ruint32_t ifdef_depth = 0;\ruint32_t stage_ifdef_depth[Stage::Count];\r}; // struct CodeFragment The rest is unchanged from the previous article. We have basically code and includes to bake the final shader. Remember, we are handling GLSL in these examples!\nNext is the Pass:\n// C++\r//\rstruct Pass {\rStringRef name;\rstruct ShaderStage {\rconst CodeFragment* code = nullptr;\rStage stage = Stage::Count;\r}; // struct ShaderStage\rStringRef name;\rstd::vector\u0026lt;ShaderStage\u0026gt; shader_stages;\r}; // struct Pass Nothing changed here. A pass is a container of one of more shaders. In general we will use the term shader state to describe the shaders that needs to be bound to the pipeline. Most common are the couple Vertex and Fragment shaders, or the Compute by itself.\nLast is the Shader itself:\n// C++\r//\rstruct Shader {\rStringRef name;\rstd::vector\u0026lt;Pass*\u0026gt; passes;\rstd::vector\u0026lt;Property*\u0026gt; properties;\r}; // struct Shader Being just a collection of passes. Again we are seeing the properties here, that I will talk later on in the article.\nThese will be used to \u0026lsquo;bake\u0026rsquo; data into a \u0026lsquo;bhfx\u0026rsquo; (binary HFX) file.\nBHFX layout In order to maximise efficiency, we are packing the data in the way we will use it. The file is divided in two main sections: common and passes. The overall layout is as follows:\nThe trick is to have the offset for each section easy to access.\nThe pass section contains several informations as following:\nAs we will see later we include shaders, resources layout and other data based on our target API (Hydra Graphics).\nWriting the BHFX file To write our file, we need to parse the HFX file. A quick code could be something like this:\n// C++\r//\r...\rchar* text = ReadEntireFileIntoMemory( \u0026#34;..\\\\data\\\\SimpleFullscreen.hfx\u0026#34;, nullptr );\rinitLexer( \u0026amp;lexer, (char*)text, data_buffer );\rhfx::initParser( \u0026amp;effect_parser, \u0026amp;lexer );\rhfx::generateAST( \u0026amp;effect_parser );\rhfx::initCodeGenerator( \u0026amp;hfx_code_generator, \u0026amp;effect_parser, 4096, 5 );\rhfx::compileShaderEffectFile( \u0026amp;hfx_code_generator, \u0026#34;..\\\\data\\\\\u0026#34;, \u0026#34;SimpleFullscreen.bhfx\u0026#34; ); Here we are parsing the file (generateAST) and then using that to compile our shader effect file. This is where the magic happens.\n// C++\r//\rvoid compileShaderEffectFile( CodeGenerator* code_generator, const char* path, const char* filename ) {\r// Create the output file\rFILE* output_file;\r// Alias the StringBuffer for better readability.\rStringBuffer\u0026amp; filename_buffer = code_generator-\u0026gt;string_buffers[0];\r// Concatenate name\rfilename_buffer.clear();\rfilename_buffer.append( path );\rfilename_buffer.append( filename );\rfopen_s( \u0026amp;output_file, filename_buffer.data, \u0026#34;wb\u0026#34; );\rif ( !output_file ) {\rprintf( \u0026#34;Error opening file. Aborting. \\n\u0026#34; );\rreturn;\r} Typical file creation preamble. Concatenate the file using the StringBuffer, and try to create it.\nRemember that overall the file structure is:\nFile header Pass offset list Pass sections Let\u0026rsquo;s start with the file header:\nconst uint32_t pass_count = (uint32_t)code_generator-\u0026gt;parser-\u0026gt;passes.size();\rShaderEffectFile shader_effect_file;\rshader_effect_file.num_passes = pass_count; fwrite( \u0026amp;shader_effect_file, sizeof(ShaderEffectFile), 1, output_file ); In this case we are writing straight to the file, because it is an in-order operation with the file layout. For the rest of the file writing we will need to use String Buffers to accumulate data out-of-order and then write the file in the correct order. Think of the Pass Offset List: to calculate the offsets we need to know the size of the passes. To know the size we need to finalize the pass data. To finalize the pass data we need to finalize shaders, and that means adding the includes.\nAgain for code clarity I use aliases like this:\nStringBuffer\u0026amp; code_buffer = code_generator-\u0026gt;string_buffers[1];\rStringBuffer\u0026amp; pass_offset_buffer = code_generator-\u0026gt;string_buffers[2];\rStringBuffer\u0026amp; shader_offset_buffer = code_generator-\u0026gt;string_buffers[3];\rStringBuffer\u0026amp; pass_buffer = code_generator-\u0026gt;string_buffers[4]; Let\u0026rsquo;s continue. We start tracking the pass section memory offset knowing that it will be after the header and the pass offset list:\npass_offset_buffer.clear();\rpass_buffer.clear();\r// Pass memory offset starts after header and list of passes offsets.\ruint32_t pass_offset = sizeof( ShaderEffectFile ) + sizeof(uint32_t) * pass_count; Now into the most interesting part. We will avoid talking about the resource layout part, that will be added later.\n// Pass Section:\r// ----------------------------------------------------------------------------------------\r// Shaders count | Name | Shader Offset+Count List | Shader Code 0, Shader Code 1\r// ----------------------------------------------------------------------------------------\rShaderEffectFile::PassHeader pass_header;\rfor ( uint32_t i = 0; i \u0026lt; pass_count; i++ ) {\rpass_offset_buffer.append( \u0026amp;pass_offset, sizeof( uint32_t ) );\rconst Pass\u0026amp; pass = code_generator-\u0026gt;parser-\u0026gt;passes[i];\rconst uint32_t pass_shader_stages = (uint32_t)pass.shader_stages.size();\rconst uint32_t pass_header_size = pass_shader_stages * sizeof( ShaderEffectFile::Chunk ) + sizeof( ShaderEffectFile::PassHeader );\ruint32_t current_shader_offset = pass_header_size; We start iterating the passes and calculate the shader offset. Shader Chunks (the actual shader code) are written after the Pass Header and the dynamic list of shader chunk offset and size. Next we will calculate the offsets of the single shaders AFTER we finalize the code - that means after the includes are added!\nshader_offset_buffer.clear();\rcode_buffer.clear();\rfor ( size_t s = 0; s \u0026lt; pass.shader_stages.size(); ++s ) {\rconst Pass::ShaderStage shader_stage = pass.shader_stages[s];\rappendFinalizedCode( path, shader_stage.stage, shader_stage.code, filename_buffer, code_buffer, true, constants_buffer );\rupdateOffsetTable( \u0026amp;current_shader_offset, pass_header_size, shader_offset_buffer, code_buffer );\r}\r// Update pass offset\rpass_offset += code_buffer.current_size + shader_offset; At this point we have code_buffer containing all the shaders of the pass one after another (null terminated) and we can update the pass offset for the next pass. We also calculated the single shader offsets with the updateOffsetTable method in shader_offset_buffer. We need to finalize the Pass Header and then we can merge the pass memory in one block and proceed to the next pass:\n// Fill Pass Header\rcopy( pass.name, pass_header.name, 32 );\rpass_header.num_shader_chunks = pass.num_shaders; This is a very IMPORTANT part. Merge in the pass_buffer all the pass section currently calculated: pass header, the single shader code offsets and the shader code itself.\npass_buffer.append( (void*)\u0026amp;pass_header, sizeof( ShaderEffectFile::PassHeader ) );\rpass_buffer.append( shader_offset_buffer );\rpass_buffer.append( code_buffer );\r} After we finished with all the passes, we have 2 buffers: one containing the pass offset list, the other the pass sections. We can write them off in the correct order finally and close the file:\nfwrite( pass_offset_buffer.data, pass_offset_buffer.current_size, 1, output_file );\rfwrite( pass_buffer.data, pass_buffer.current_size, 1, output_file );\rfclose( output_file );\r} We can see why we chose this format when looking at the code to actually create a shader state. First of all this is the struct to create a shader state:\n// hydra_graphics.h\r//\rstruct ShaderCreation {\rstruct Stage {\rShaderStage::Enum type = ShaderStage::Compute;\rconst char* code = nullptr;\r}; // struct Stage\rconst Stage* stages = nullptr;\rconst char* name = nullptr;\ruint32_t stages_count = 0;\r}; // struct ShaderCreation It is very simple, each stage has a code and type. A shader state can have one or more stages. This was already the case in OpenGL - compiling shaders and linking them - so the interface is similar - but it maps well to Vulkan/D3D12 as well, in which the Pipeline State, that describe almost everything the GPU needs to draw, needs an unique set of vertex/fragment/compute shaders. Anyway, we embed this data already in the binary HFX file, and thus we can easily create a shader state like this:\nstatic void compile_shader_effect_pass( hydra::graphics::Device\u0026amp; device, char* hfx_memory, uint16_t pass_index, hydra::graphics::ShaderHandle\u0026amp; out_shader ) {\rusing namespace hydra;\r// Get pass section memory\rchar* pass = hfx::getPassMemory( hfx_memory, pass_index );\rhfx::ShaderEffectFile::PassHeader* pass_header = (hfx::ShaderEffectFile::PassHeader*)pass;\rconst uint32_t shader_count = pass_header-\u0026gt;num_shader_chunks; graphics::ShaderCreation::Stage* stages = new graphics::ShaderCreation::Stage[shader_count];\r// Get individual shader code and type\rfor ( uint16_t i = 0; i \u0026lt; shader_count; i++ ) {\rhfx::getShaderCreation( shader_count, pass, i, \u0026amp;stages[i] );\r}\rgraphics::ShaderCreation first_shader = {};\rfirst_shader.stages = stages;\rfirst_shader.stages_count = shader_count;\rfirst_shader.name = pass_header-\u0026gt;name;\rout_shader = device.create_shader( first_shader );\rdelete stages;\r} Nothing really interesting here, but we read the file in memory and use the offsets we store to access the different sections of the file.\nTo access the Pass Section we first need to read its memory offset and then read from there. Remember from before that the offset is in the list AFTER the ShaderEffectFile header, and it is a single uint32:\nchar* getPassMemory( char* hfx_memory, uint32_t index ) {\r// Read offset form list after the ShaderEffectFile header.\rconst uint32_t pass_offset = *(uint32_t*)(hfx_memory + sizeof( ShaderEffectFile ) + (index * sizeof( uint32_t )));\rreturn hfx_memory + pass_offset;\r} From the pass offset, the list of shader chunks (that are defined as code offset and size) is right after the pass header\nvoid getShaderCreation( uint32_t shader_count, char* pass_memory, uint32_t index,\rhydra::graphics::ShaderCreation::Stage* shader_creation ) {\rchar* shader_offset_list_start = pass_memory + sizeof( ShaderEffectFile::PassHeader ); Read the single shader offset and access the memory there:\nconst uint32_t shader_offset = *(uint32_t*)(shader_offset_list_start + (index * sizeof( ShaderEffectFile::Chunk )));\rchar* shader_chunk_start = pass_memory + shader_offset; The baked informations are first the type (as a single char, but called hfx::ShaderEffectFile::ChunkHeader in case we change it) and the actual shader code is right after!\nshader_creation-\u0026gt;type = (hydra::graphics::ShaderStage::Enum)(*shader_chunk_start);\rshader_creation-\u0026gt;code = (const char*)(shader_chunk_start + sizeof( hfx::ShaderEffectFile::ChunkHeader ));\r} In this case I chose to bake the file instead of generating a header file - just cause I can reuse this code for every shader effect. I could have generated an header instead of the binary BHFX file, but then including it would mean that you need to recompile at every change. We will see some areas in which we can have both approaches!\nFinally done with the new embedded format, let\u0026rsquo;s see the new features!\nBrainstorming: what features are needed ? We already talked about the features at the beginning of the articles, but let\u0026rsquo;s write them again to refresh our memory:\nShader constants generation Shader resource bindings Render states (depth stencil, blend, rasterization) (in the next article) Render pass hints for a future framegraph (in the next article) There are few articles around this subject, but the most complete is from the amazing guys at OurMachinery, and in particular this article. These guys does (as always honestly) an amazing job in describing the problem we are facing and the solutions, and how enriching a shader language can make a huge difference in making better rendering (faster iteration time, less error prone, more artist friendly..) so I would suggest to read those articles (and in general any article/presentation/blog post they write!).\nWe will go through each feature in depth so get ready!\nConstants: artists, programmers, both ? Constants\u0026hellip;uniforms\u0026hellip;whatever name you choose, they represent the same concept: numerical properties.\nEven if they are a simple concept, still it is hard to make both rendering citizens happy: artists and programmers!\nArtists want tweakable UI, simple variables and fast iteration. Programmers want optimal layout, more CPU calculated variables possible, and ultimate control. How to make them both happy ?\nI brainstormed and designed for few days (well evenings) to solve this problem. One thought that came to me is that artists want to create a material interface, something they can tweak and change easily, and when you want to quickly prototype something, create and such, you don\u0026rsquo;t want to deal with low-level resource management and such. Let\u0026rsquo;s solve this first: give artists a simple way of creating a material interface!\nAfter searching for a bit, I chose to use a syntax very similar to Unity ShaderLab. Let\u0026rsquo;s see the HFX (finally!):\n// .HFX\r//\r// For the artist: create a material interface.\rproperties {\r// Using Unity ShaderLab syntax:\r// AORemapMin0(\u0026#34;AORemapMin0\u0026#34;, Range(0.0, 1.0)) = 0.0\rscale(\u0026#34;Scale\u0026#34;, Float) = 32.00\rmodulo(\u0026#34;Modulo\u0026#34;, Float) = 2.0\r} We added a new section in the language, named \u0026ldquo;properties\u0026rdquo;. Why this name ? Because properties contains both numerical properties and textures! The name makes sense in this way. Naming \u0026lsquo;constants\u0026rsquo; and having also textures, not.\nThere are 2 possible outputs from this, one that is pure code-generation and the other that is more data-driven. I will dwelve into the code-generation one and talk about the data-driven one in another post.\nThere are 3 parts for the generated code of the properties:\nProperties UI GPU-ready constant buffer API-dependant buffer For the Properties UI, we want to generate something like this:\n// C++\rstruct LocalConstantsUI {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rvoid reflectMembers() {\rImGui::InputScalar( \u0026#34;Scale\u0026#34;, ImGuiDataType_Float, \u0026amp;scale);\rImGui::InputScalar( \u0026#34;Modulo\u0026#34;, ImGuiDataType_Float, \u0026amp;modulo);\r}\rvoid reflectUI() {\rImGui::Begin( \u0026#34;LocalConstants\u0026#34; );\rreflectMembers();\rImGui::End();\r}\r}; // struct LocalConstantsUI For the GPU-ready constants, we want to have a both a GPU and a CPU representation like this:\n// C++\r//\rstruct LocalConstants {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rfloat pad_tail[2];\r}; // struct LocalConstants\r// GLSL\r//\rlayout (std140, binding=7) uniform LocalConstants {\rfloat scale;\rfloat modulo;\rfloat pad[2];\r} local_constants; And for the API-dependant buffer, we want to create code that takes care of everything for us. This is the real deal here - and something we will revisit in next articles to show some advanced features.\nvoid create( hydra::graphics::Device\u0026amp; device ) {\rusing namespace hydra;\rgraphics::BufferCreation constants_creation = {};\rconstants_creation.type = graphics::BufferType::Constant;\rconstants_creation.name = \u0026#34;LocalConstants\u0026#34;;\rconstants_creation.usage = graphics::ResourceUsageType::Dynamic;\r// NOTE: using LocalConstants struct - is the GPU ready one with padding and such!\rconstants_creation.size = sizeof( LocalConstants );\r// Struct is initialized with default values already, so it is safe to copy it to the GPU.\rconstants_creation.initial_data = \u0026amp;constants;\rbuffer = device.create_buffer( constants_creation );\r}\rvoid destroy( hydra::graphics::Device\u0026amp; device ) {\rdevice.destroy_buffer( buffer );\r}\rvoid updateUI( hydra::graphics::Device\u0026amp; device ) {\r// Draw UI\rconstantsUI.reflectUI();\r// TODO:\r// Ideally there should be a way to tell if a variable has changed and update only in that case.\r// Map buffer to GPU and upload parameters from the UI\rhydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\rLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters );\rif ( buffer_data ) {\rbuffer_data-\u0026gt;scale = constantsUI.scale;\rbuffer_data-\u0026gt;modulo = constantsUI.modulo;\rdevice.unmap_buffer( map_parameters );\r}\r} For the sake of the example this could be a possible implementation - but really depends on the rendering API. Let\u0026rsquo;s quickly check parsing and code-generation.\nConstants Parsing To parse the new property section, there is the new method void declarationProperties( Parser* parser ) that iterates through all properties, and inside that the void declarationProperty( Parser* parser, const StringRef\u0026amp; name ) one.\nWe are parsing the following HFX syntax:\n// Syntax\r//\ridentifier(string, identifier[(arguments)]) [= default_value] With this is an example:\n// HFX\r//\rproperties {\rscale(\u0026#34;Scale\u0026#34;, Float) = 32.0\r} We will add a simple backtracking to the parsing because of the optional parameters. Let\u0026rsquo;s check the code!\ninline void declarationProperty( Parser* parser, const StringRef\u0026amp; name ) {\rProperty* property = new Property();\r// Cache name\rproperty-\u0026gt;name = name;\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenParen ) ) {\rreturn;\r} We just parsed the property name and the \u0026lsquo;(\u0026rsquo;. Next is the string containing the UI name:\n// Advance to the string representing the ui_name\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_String ) ) {\rreturn;\r}\rproperty-\u0026gt;ui_name = token.text; Saved the ui name and then we have the type. Types can be Float, Int, Range, Texture, Vector, Color and we will simply parse their text and convert it to an enum that we will use in the code generation phase.\nif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Comma ) ) {\rreturn;\r}\r// Next is the identifier representing the type name\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r// Parse property type and convert it to an enum\rproperty-\u0026gt;type = propertyTypeIdentifier( token ); Now will come the most complicated part. We have optional \u0026lsquo;(\u0026rsquo; open parenthesis for the parameters if the type needs it. For the length of code and article, I skip this part and will add it in next article!\n// If an open parenthesis is present, then parse the ui arguments.\rnextToken( parser-\u0026gt;lexer, token );\rif ( token.type == Token::Token_OpenParen ) {\rproperty-\u0026gt;ui_arguments = token.text;\rwhile ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseParen ) ) {\r// TODO:\r// Parse parameters!\r}\r// Advance to the last close parenthesis\rnextToken( parser-\u0026gt;lexer, token );\rproperty-\u0026gt;ui_arguments.length = token.text.text - property-\u0026gt;ui_arguments.text;\r}\rif ( !checkToken( parser-\u0026gt;lexer, token, Token::Token_CloseParen ) ) {\rreturn;\r} At this point we can either be at the end of the property or we could have a \u0026lsquo;=\u0026rsquo; token to add a default value. Being that the Lexer class is small, we can backtrack by saving the current Lexer status:\n// Cache lexer status and advance to next token.\r// If the token is \u0026#39;=\u0026#39; then we parse the default value.\r// Otherwise backtrack by one token.\rLexer cached_lexer = *parser-\u0026gt;lexer; Now we can advance to the next token and:\nIf the token is \u0026lsquo;=\u0026rsquo;, parse the default value. If not, backtrack the position of the Lexer and finish the parsing. nextToken( parser-\u0026gt;lexer, token );\r// At this point only the optional default value is missing, otherwise the parsing is over.\rif ( token.type == Token::Token_Equals ) {\rnextToken( parser-\u0026gt;lexer, token );\rif ( token.type = Token::Token_Number ) {\r// Cache the data buffer entry index into the property for later retrieval.\rproperty-\u0026gt;data_index = parser-\u0026gt;lexer-\u0026gt;data_buffer-\u0026gt;current_entries - 1;\r}\relse {\r// TODO:\r// Handle vectors, colors and non single number default values\r}\r}\relse {\r*parser-\u0026gt;lexer = cached_lexer;\r}\rparser-\u0026gt;shader.properties.push_back( property );\r} An interesting point is that the numbers are parsed in a DataBuffer, and during the parsing of the token we will add the number to it. To retrieve it, we have the data_index field of the Property struct. Also here, for the sake of \u0026lsquo;brevity\u0026rsquo;, I am handling only floats and ints. Vectors, colors and texture property should be easy to add.\nFor vectors and colors we should parse a list of them and save them into the data buffer.\nFor textures we should just save the default value as text and use it in the code-generation part.\nCode Generation This should be pretty straight forward. We can iterate the properties and generate both a C++ struct and a HLSL/GLSL buffer. The only thing to be concerned is the padding: on the GPU normally the alignment is 16 bytes, so we can track that and insert padding when generating the code.\nIn the method void generateShaderResourceHeader( CodeGenerator* code_generator, const char* path ) we can see how we generate the different code for C++:\n// C++\r//\r// Beginning\rfprintf( output_file, \u0026#34;\\n#pragma once\\n#include \u0026lt;stdint.h\u0026gt;\\n#include \\\u0026#34;hydra_graphics.h\\\u0026#34;\\n\\n// This file is autogenerated!\\nnamespace \u0026#34; );\rfwrite( shader.name.text, shader.name.length, 1, output_file );\rfprintf( output_file, \u0026#34; {\\n\\n\u0026#34; );\r// Preliminary sections\rconstants_ui.append( \u0026#34;struct LocalConstantsUI {\\n\\n\u0026#34; );\rcpu_constants.append( \u0026#34;struct LocalConstants {\\n\\n\u0026#34; );\rconstants_ui_method.append(\u0026#34;\\tvoid reflectMembers() {\\n\u0026#34;);\rbuffer_class.append( \u0026#34;struct LocalConstantsBuffer {\\n\\n\\thydra::graphics::BufferHandle\\tbuffer;\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\tLocalConstants\\t\\t\\t\\t\\tconstants;\\n\\tLocalConstantsUI\\t\\t\\t\\tconstantsUI;\\n\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\tvoid create( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\tusing namespace hydra;\\n\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\t\\tgraphics::BufferCreation constants_creation = { graphics::BufferType::Constant, graphics::ResourceUsageType::Dynamic, sizeof( LocalConstants ), \u0026amp;constants, \\\u0026#34;LocalConstants\\\u0026#34; };\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\t\\tbuffer = device.create_buffer( constants_creation );\\n\\t}\\n\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\tvoid destroy( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\tdevice.destroy_buffer( buffer );\\n\\t}\\n\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\tvoid updateUI( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\t// Draw UI\\n\\t\\tconstantsUI.reflectUI();\\n\\t\\t// Update constants from UI\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\t\\thydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\t\\tLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters );\\n\\t\\tif (buffer_data) {\\n\u0026#34; );\r// For GPU the struct must be 16 bytes aligned. Track alignment\ruint32_t gpu_struct_alignment = 0;\rDataBuffer* data_buffer = code_generator-\u0026gt;parser-\u0026gt;lexer-\u0026gt;data_buffer;\r// For each property write code\rfor ( size_t i = 0; i \u0026lt; shader.properties.size(); i++ ) {\rhfx::Property* property = shader.properties[i];\rswitch ( property-\u0026gt;type ) {\rcase Property::Float:\r{\rconstants_ui.append(\u0026#34;\\tfloat\\t\\t\\t\\t\\t\u0026#34;);\rconstants_ui.append( property-\u0026gt;name );\rcpu_constants.append( \u0026#34;\\tfloat\\t\\t\\t\\t\\t\u0026#34; );\rcpu_constants.append( property-\u0026gt;name );\rif ( property-\u0026gt;data_index != 0xffffffff ) {\rfloat value = 0.0f;\rgetData( data_buffer, property-\u0026gt;data_index, value );\rconstants_ui.append( \u0026#34;\\t\\t\\t\\t= %ff\u0026#34;, value );\rcpu_constants.append( \u0026#34;\\t\\t\\t\\t= %ff\u0026#34;, value );\r}\rconstants_ui.append( \u0026#34;;\\n\u0026#34; );\rcpu_constants.append( \u0026#34;;\\n\u0026#34; );\rconstants_ui_method.append(\u0026#34;\\t\\tImGui::InputScalar( \\\u0026#34;\u0026#34;);\rconstants_ui_method.append( property-\u0026gt;ui_name );\rconstants_ui_method.append( \u0026#34;\\\u0026#34;, ImGuiDataType_Float, \u0026amp;\u0026#34; );\rconstants_ui_method.append( property-\u0026gt;name );\rconstants_ui_method.append( \u0026#34;);\\n\u0026#34; );\r// buffer_data-\u0026gt;scale = constantsUI.scale;\rbuffer_class.append(\u0026#34;\\t\\t\\tbuffer_data-\u0026gt;\u0026#34;);\rbuffer_class.append( property-\u0026gt;name );\rbuffer_class.append( \u0026#34; = constantsUI.\u0026#34; );\rbuffer_class.append( property-\u0026gt;name );\rbuffer_class.append( \u0026#34;;\\n\u0026#34; );\r++gpu_struct_alignment;\rbreak;\r}\r}\r}\r// Post-property sections\rconstants_ui.append( \u0026#34;\\n\u0026#34; );\rconstants_ui_method.append( \u0026#34;\\t}\\n\\n\u0026#34; );\rconstants_ui_method.append( \u0026#34;\\tvoid reflectUI() {\\n\\t\\tImGui::Begin( \\\u0026#34;LocalConstants\\\u0026#34; );\\n\u0026#34; );\rconstants_ui_method.append( \u0026#34;\\t\\treflectMembers();\\n\\t\\tImGui::End();\\n\\t}\\n\\n\u0026#34; );\rconstants_ui_method.append( \u0026#34;}; // struct LocalConstantsUI\\n\\n\u0026#34; );\r// Add tail padding data\ruint32_t tail_padding_size = 4 - (gpu_struct_alignment % 4);\rcpu_constants.append( \u0026#34;\\tfloat\\t\\t\\t\\t\\tpad_tail[%u];\\n\\n\u0026#34;, tail_padding_size );\rcpu_constants.append( \u0026#34;}; // struct LocalConstants\\n\\n\u0026#34; );\rbuffer_class.append( \u0026#34;\\t\\t\\tdevice.unmap_buffer( map_parameters );\\n\\t\\t}\\n\\t}\\n}; // struct LocalConstantBuffer\\n\\n\u0026#34; );\rfwrite( constants_ui.data, constants_ui.current_size, 1, output_file );\rfwrite( constants_ui_method.data, constants_ui_method.current_size, 1, output_file );\rfwrite( cpu_constants.data, cpu_constants.current_size, 1, output_file );\rfwrite( buffer_class.data, buffer_class.current_size, 1, output_file );\r// End\rfprintf( output_file, \u0026#34;} // namespace \u0026#34; );\rfwrite( shader.name.text, shader.name.length, 1, output_file );\rfprintf( output_file, \u0026#34;\\n\\n\u0026#34; );\rfclose( output_file ); This piece of code will generate a constant buffer from the properties:\n// GLSL\r//\rstatic void generateConstantsCode( const Shader\u0026amp; shader, StringBuffer\u0026amp; out_buffer ) {\rif ( !shader.properties.size() ) {\rreturn;\r}\r// Add the local constants into the code.\rout_buffer.append( \u0026#34;\\n\\t\\tlayout (std140, binding=7) uniform LocalConstants {\\n\\n\u0026#34; );\r// For GPU the struct must be 16 bytes aligned. Track alignment\ruint32_t gpu_struct_alignment = 0;\rconst std::vector\u0026lt;Property*\u0026gt;\u0026amp; properties = shader.properties;\rfor ( size_t i = 0; i \u0026lt; shader.properties.size(); i++ ) {\rhfx::Property* property = shader.properties[i];\rswitch ( property-\u0026gt;type ) {\rcase Property::Float:\r{\rout_buffer.append( \u0026#34;\\t\\t\\tfloat\\t\\t\\t\\t\\t\u0026#34; );\rout_buffer.append( property-\u0026gt;name );\rout_buffer.append( \u0026#34;;\\n\u0026#34; );\r++gpu_struct_alignment;\rbreak;\r}\r}\r}\ruint32_t tail_padding_size = 4 - (gpu_struct_alignment % 4);\rout_buffer.append( \u0026#34;\\t\\t\\tfloat\\t\\t\\t\\t\\tpad_tail[%u];\\n\\n\u0026#34;, tail_padding_size );\rout_buffer.append( \u0026#34;\\t\\t} local_constants;\\n\\n\u0026#34; );\r} Expert constants: an interesting problem A problem many times surfaces is that the material interface does not correspond to the buffer sent to the GPU, because the programmers will do the following:\nAdd system constants, that don\u0026rsquo;t need a UI Change order of the constants Change constants to more GPU friendly values, calculating some stuff on the CPU Pack constants into smaller ones This is an interesting topic and I\u0026rsquo;ll cover it in another article, but a simple solution would be to add a mapping between the GPU constants and the UI, so that we can separate the UI constants from the GPU ones.\nI\u0026rsquo;ll give a brief example but it would be too much for this article and will not be included in the source code.\nBasically we are trying to create a mapping between the material interface:\n// C++\rstruct LocalConstantsUI {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rvoid reflectMembers() {\rImGui::InputScalar( \u0026#34;Scale\u0026#34;, ImGuiDataType_Float, \u0026amp;scale);\rImGui::InputScalar( \u0026#34;Modulo\u0026#34;, ImGuiDataType_Float, \u0026amp;modulo);\r}\rvoid reflectUI() {\rImGui::Begin( \u0026#34;LocalConstants\u0026#34; );\rreflectMembers();\rImGui::End();\r}\r}; // struct LocalConstantsUI And the GPU constants:\n// C++\rstruct LocalConstants {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rfloat pad_tail[2];\r}; // struct LocalConstants We could enhance HFX with some syntax to mark the derivate properties and just add the system ones in an explicit buffer layout, and add a layout section in the HFX:\n// HFX\rproperties {\r// Using Unity ShaderLab syntax:\rscale(\u0026#34;Scale\u0026#34;, Range(0.0, 100.0)) = 100.0\rmodulo(\u0026#34;Modulo\u0026#34;, Float) = 2.0\r}\rlayout {\rCBuffer LocalConstants {\rfloat4x4 world_view_projection; // \u0026#39;System\u0026#39; variable\rfloat scale01 = (scale); // Silly normalized version of scale interface property\rfloat modulo;\rfloat pad[2];\r}\r} we could completely override the automatic constant buffer generation from the properties. With this we can:\nAdd a system variable like world_view_projection Flag the property scale as UI only, by saying that property scale01 uses it. I think that with this syntax both artists and programmers can be happy together! I will try to work on this on a later article.\nResource bindings: Vulkan and D3D12 mentality As stated multiple times, the shift in mentality is towards the new APIs, and that includes the concept of resource lists. The problem is that we don\u0026rsquo;t want artists to have to handle this kind of things - especially if you want to quickly prototype things! But at the same time, we want programmers to have the possibility to optimize the shaders the artists gave them. What is the solution? Simple: creating an optional resource layout section and automatically generate it if not present, so that artists (and not only) can happily create amazing tech and THEN worry about these details!\nAutomatic Resource Layout The easiest way to handle resource layout is to make them SIMPLE. Remember the K.I.S.S. principle. In this case it means that we can create a Resource List for each pass, that will contain:\nOne constant/uniform buffer containing all the properties All the textures used by the shader How can we achieve that ?\nWe already saw how we can generate the constant buffer from the properties in the previous section. For textures we have a couple of options.\nList of Textures Being in automation land, there are 2 ways to add texture dependencies:\nUse reflection mechanism from the target shader language Parse identifiers in the current finalized shader For the sake of fun we will look into the second of course! If we go back to void declarationGlsl( Parser* parser ), we can add a new method to parse the keyword:\n// Parse hash for includes and defines\rif ( token.type == Token::Token_Hash ) {\r// Get next token and check which directive is\rnextToken( parser-\u0026gt;lexer, token );\rdirectiveIdentifier( parser, token, code_fragment );\r}\relse if ( token.type == Token::Token_Identifier ) { \u0026lt;------------ New Code!\r// Parse uniforms to add resource dependencies if not explicit in the HFX file.\rif ( expectKeyword( token.text, 7, \u0026#34;uniform\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, token );\runiformIdentifier( parser, token, code_fragment );\r}\r} In this way it will search for the identifier uniform and search for the other identifiers. This is GLSL centric of course.\ninline void uniformIdentifier( Parser* parser, const Token\u0026amp; token, CodeFragment\u0026amp; code_fragment ) {\rfor ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase \u0026#39;i\u0026#39;:\r{\rif ( expectKeyword( token.text, 7, \u0026#34;image2D\u0026#34; ) ) {\r// Advance to next token to get the name\rToken name_token;\rnextToken( parser-\u0026gt;lexer, name_token );\rCodeFragment::Resource resource = { hydra::graphics::ResourceType::TextureRW, name_token.text };\rcode_fragment.resources.emplace_back( resource );\r}\rbreak;\r}\rcase \u0026#39;s\u0026#39;:\r{\rif ( expectKeyword( token.text, 9, \u0026#34;sampler2D\u0026#34; ) ) {\r// Advance to next token to get the name\rToken name_token;\rnextToken( parser-\u0026gt;lexer, name_token );\rCodeFragment::Resource resource = { hydra::graphics::ResourceType::Texture, name_token.text };\rcode_fragment.resources.emplace_back( resource );\r}\rbreak;\r}\r}\r}\r} Should be pretty straight-forward: if you find the identifier for texture, add a resource dependency with type and name to the current code fragment! Is this the ideal solution ? Probably not. But I wanted to show what we can achieve once we have fun with parsing, including the understanding on when to say NO to it!\nManual Resource Layout Now that the effect can work without too much programmer time, it is time to give back to programmers the control they want. In the previous paragraph about Expert Constants we talked about adding a new section, called layout. In this section we can specify the resource list for each pass manually, and later on in the pass we can reference this lists as used by the pass.\nGoing on a more complete solution, layouts should be included and merged when including other HFX files. This is something we want and we\u0026rsquo;ll look in another post, we can start simple by defining something local:\n// HFX\r//\r// For the developer\rlayout {\rlist LocalCompute {\rcbuffer LocalConstants;\rtexture2Drw(rgba8) destination_texture;\r}\rlist Local {\rtexture2D input_texture;\r}\r} This is a rather simple layout, but let\u0026rsquo;s see it. First of all, for each \u0026rsquo;list\u0026rsquo; keyword we define a single list with a unique name. With that, we can reference in the pass which list to use.\nThe code that does the parsing is (at this point) pretty straight-forward, both in void declarationResourceList( Parser* parser, ResourceList\u0026amp; resource_list ) and void resourceBindingIdentifier( Parser* parser, const Token\u0026amp; token, ResourceBinding\u0026amp; binding ). I will not go over it, but basically it will parse the resource lists and add them to the shader. The parsing itself will read the text and create the ResourceSetLayoutCreation::Binding and add it to the list of the resources.\nWe then add a new identifier in the pass to choose which resource list to be used:\n// HFX\r//\rpass FillTexture {\rresources = LocalCompute, ...\rdispatch = 32, 32, 1\rrender_pass = compute\rcompute = ComputeTest\r}\rpass ToScreen {\rresources = Local\rrender_pass = fullscreen\rvertex = ToScreen\rfragment = ToScreen\r} The parsing will happen in void declarationPassResources( Parser* parser, Pass\u0026amp; pass ).\nAdding Resource Layout data to binary HFX So after this amazing journey we are ready to embed those informations into the BHFX and use it right away into the rendering API.\nThe big difference is if the hfx file contains a layout section. If it is not present, then all the informations will be gathered automatically and will be added with the writeAutomaticResourcesLayout method.\nFirst we will add the LocalConstant buffer created from the properties:\nstatic void writeAutomaticResourcesLayout( const hfx::Pass\u0026amp; pass, StringBuffer\u0026amp; pass_buffer, uint32_t\u0026amp; pass_offset ) {\rusing namespace hydra::graphics;\r// Add the local constant buffer obtained from all the properties in the layout.\rhydra::graphics::ResourceSetLayoutCreation::Binding binding = { hydra::graphics::ResourceType::Constants, 0, 1, \u0026#34;LocalConstants\u0026#34; };\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding ); Then we will cycle through all the shader stages and write the resources into the memory:\nfor ( size_t s = 0; s \u0026lt; pass.shader_stages.size(); ++s ) {\rconst Pass::ShaderStage shader_stage = pass.shader_stages[s];\rfor ( size_t p = 0; p \u0026lt; shader_stage.code-\u0026gt;resources.size(); p++ ) {\rconst hfx::CodeFragment::Resource\u0026amp; resource = shader_stage.code-\u0026gt;resources[p];\rswitch ( resource.type ) {\rcase ResourceType::Texture:\r{\rcopy( resource.name, binding.name, 32 );\rbinding.type = hydra::graphics::ResourceType::Texture;\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding ) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding );\rbreak;\r}\rcase ResourceType::TextureRW:\r{\rcopy( resource.name, binding.name, 32 );\rbinding.type = hydra::graphics::ResourceType::TextureRW;\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding ) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding );\rbreak;\r}\r}\r}\r}\r} If instead there is a layout section, the method writeResourcesLayout is called and will be pretty straight-forward:\nstatic void writeResourcesLayout( const hfx::Pass\u0026amp; pass, StringBuffer\u0026amp; pass_buffer, uint32_t\u0026amp; pass_offset ) {\rusing namespace hydra::graphics;\rfor ( size_t r = 0; r \u0026lt; pass.resource_lists.size(); ++r ) {\rconst ResourceList* resource_list = pass.resource_lists[r];\rconst uint32_t resources_count = (uint32_t)resource_list-\u0026gt;resources.size();\rpass_buffer.append( (void*)resource_list-\u0026gt;resources.data(), sizeof(ResourceBinding) * resources_count );\rpass_offset += sizeof( ResourceBinding ) * resources_count;\r}\r} And this will be put at the end of the current pass section:\npass_buffer.append( (void*)\u0026amp;pass_header, sizeof( ShaderEffectFile::PassHeader ) );\rpass_buffer.append( shader_offset_buffer );\rpass_buffer.append( code_buffer );\rif ( automatic_layout ) {\rwriteAutomaticResourcesLayout( pass, pass_buffer, pass_offset );\r}\relse {\rwriteResourcesLayout( pass, pass_buffer, pass_offset );\r} Conclusions and what\u0026rsquo;s next We arrived at the end of this article, and we started seeing how we can use HFX as a more complete language to embed different rendering features. We saw how to embed shader code and resource lists so that the rendering API can create everything without hard-coded generation of resources. This also showed when it was useful to create data instead of code. On the contrary, the UI and the Constants are generated in a new header file - thus code generation. There are pros and cons to both approaches, but I hope that knowing how to generate code and create a custom language will let you play with the concepts and explore your own needs.\nAs next steps, there are some questions opened: how to reload shaders ? Can I add new material properties without recompiling code ?\nWe will also see a simple implementation of a frame-graph, that I use since my years in Codemasters and in my indie project. This will be much more data-driven than code-generated, but again, the purpose of these articles is to explore the concepts and understanding when to use what.\nAs always please comment, feedback, share!\nThanks for reading! Gabriel\n","date":1568176933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568176933,"objectID":"981a49821ad5e37fd95cd7c0c273e7a6","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_2/","publishdate":"2019-09-11T00:42:13-04:00","relpermalink":"/post/writing_shader_effect_language_2/","section":"post","summary":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In Part 1 of this series we created a simple language to work as \u0026lsquo;shader effect\u0026rsquo; - a shader language superset to make our life easier, by adding missing features. The fact that there is not an industry standard for a shader effect language leads to either hand-crafted (and secret) languages, or to hardcoded permutations, or to other gray-area solutions.\n(Personal though: part of me would like to help in contributing to the creation of a standard through these articles and code.","tags":[],"title":"Writing a Shader Effect Language Part 2","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In this article we will create a simple language that can encapsulate shader code (called code fragments) and output different files for each fragment. This is the initial step to switch from an engine that loads single files for each shader stage (vertex, fragment, compute, \u0026hellip;) to one that uses an effect file that contains more than one shader.\nWe will start by motivation, then will define the language itself (very simple), then we will look at the Parser and last the Code Generator.\nHave a good read!\nMotivation In the incredible quest of data-driven rendering, after we defeated the dragon of code generation another multiple headed dragon arises: an hydra! We have different options here: be the brave warrior in shiny armor that tries to cut all the heads of the hydra, built some machines that can fight for us and send them, or both built the machines AND fight.\nOur code is invaluable, like our energies fighting the hydra. We need to carefully balance them and see how can we use for the BEST.\nWriting manual code is good, it is generally what is done, but it is slow and error prone. Going data-driven can be fast, but can give you a sense of losing control (not personally, but I heard few people saying that). Only generating code can quickly become a recipe for disaster: so many particular use cases need attention, that the code could be come a different kind of mess.\nWe will try to go down the route of code generation mixed with data-driven. As I wrote in my previous articles, it is a fine line and can be good to know when to go in which direction!\nI will divide the article in 2 parts. The first part (this one) will contain the new Shader Code Generator to generate shader permutations and add include support to GLSL. The second will require a low-level rendering library and will show Code Generation of more CPU areas of Rendering, the real goal of all these articles!\nThe code is available here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nEffect file structure Looking at effects, the first thing to do is to define a file that will represent our shaders. My choice is to create a simple language to embed shaders code and generate the CPU code necessary to render it.\nWhy not using Json ? While it is an amazing data-format, I still want a bigger degree of control of what to parse and what to generate. The decision is based on the fact that by writing a parser for the language, I can automate some code-generation that would be more intricate with Json. Also, this series itself is a personal exploration on the topic, so using Json was not an option for this level of complexity.\nThe HFX Format HFX (Hydra Effects) is a new language we will define to write out shaders. The first iteration will be barebone - it will simply be a shader permutation generator - but it will be the foundation to extensions that will allow us to write CPU rendering code that we want to automate.\nIn defining the format, there will be few keywords that will be defined, but the general architecture will make straightforward to copy-paste shader code fragments from any language into the HFX language. We will use the following keywords (and concepts).\nShader The root of a shader effect. It will contain everything we are writing.\nGlsl/Hlsl These will define the actual shader code, enclosed fragments. Fragments can be composed and reused. For Glsl in particular, code fragments needs to be embedded in defines for each stage. More on that later.\nPass, Technique, Variant This is the central part for the effects to work. I\u0026rsquo;ve researched a bit, between Microsoft effects, Unity effects, Godot and Bungie and the concepts are very similar, but they seem to differ a little and also each implementation becomes very engine-specific of course. The presentation by Bungie is amazing and their system is by far the more extensive and complex, we will work on a much simpler shader effect system.\nLet\u0026rsquo;s define a pass as a combination of shader code for at least one stage of the shader pipeline. For example a single compute shader or a couple vertex-fragment shader.\nVariants and techniques are loose concept to help separating shader paths. For example a variant could be a different post-process shader, like different implementations of SSAO.\nA technique could be a whole set of passes that target a specific platform.\nNot having my mind set on those still, I will omit them for now, as they are concepts that are less central than the code generation, and can be very subjective opinion-wise. Possibly I\u0026rsquo;ll get them in part 2.\nProperties Final piece of the puzzle. This will define the resources used by the shader effect on a per-effect level. Keeping an eye on the newer rendering APIs (DX12 and Vulkan) this defines also the layout of the resources and how they are used. Possibly the most intense part from an automation possibility (and thus code-generation). We will define this in part 2 of this article.\nHigh level workflow From a high level perspective what will happen in all this code is enclosed in this code:\ntext = ReadEntireFileIntoMemory( \u0026#34;..\\\\data\\\\SimpleFullscreen.hfx\u0026#34;, nullptr );\rinitLexer( \u0026amp;lexer, (char*)text );\rhfx::Parser effect_parser;\rhfx::initParser( \u0026amp;effect_parser, \u0026amp;lexer );\rhfx::generateAST( \u0026amp;effect_parser );\rhfx::CodeGenerator hfx_code_generator;\rhfx::initCodeGenerator( \u0026amp;hfx_code_generator, \u0026amp;effect_parser, 4096 );\rhfx::generateShaderPermutations( \u0026amp;hfx_code_generator, \u0026#34;..\\\\data\\\\\u0026#34; ); We separated the Lexer from the Parser so we can reuse the lexer functionalities, thus we can reuse it from the previous example (parsing the HydraDataFormat files). Then we initialize the Parser and generate the AST. This will save all the passes and code fragments we defined in the HFX file. Finally we will get the parsing informations and give them to the code generator, that will write out the files for each pass and stage.\nLet\u0026rsquo;s dig into the example!\nParser: welcome HFX! In most rendering-API (OpenGL, Vulkan, Direct3D12, \u0026hellip;) shaders are compiled by compiling the individual stages (vertex, fragment, compute, geometry, \u0026hellip;) and in some APIs (especially the newer ones) are compiled into a Shader State.\nAs first step of this shader language, single shader files will be created by the shader generation method in our code.\nWe will define a simple fullscreen HFX with code fragments and passes.\nFirst, we define the root shader (SimpleFullscreen.hfx, under folder \u0026lsquo;data\u0026rsquo;):\nshader SimpleFullscreen { This is simply the container for all the code and passes that will define the shader effect.\nNow we need some actual code, so we can define a shader fragment. The keyword used in our language is glsl followed by a name and an open brace:\nglsl ToScreen { This will define a code fragment named ToScreen, that can be referenced from the passes. Next we use a glsl trick to signal our parser to use includes:\n#pragma include \u0026#34;Platform.h\u0026#34; This #pragma is actually ignored by the compiler, but will be used by the parser to actually add the include! BEWARE: this code will be included in BOTH vertex and fragment program! Anything outside of the VERTEX/FRAGMENT/COMPUTE macros will be, and this is done on purpose, like defining an interpolator struct only once or for common includes.\nNext we define the vertex program. BEWARE: vertex only code must be enclosed in VERTEX define!\n#if defined VERTEX\rout vec4 vTexCoord;\rvoid main() {\rvTexCoord.xy = vec2((gl_VertexID \u0026lt;\u0026lt; 1) \u0026amp; 2, gl_VertexID \u0026amp; 2);\rvTexCoord.zw = vTexCoord.xy;\rgl_Position = vec4(vTexCoord.xy * 2.0f + -1.0f, 0.0f, 1.0f);\r}\r#endif // VERTEX This code is a simple fullscreen triangle that does not require any vertex buffer, but uses the vertex id to draw. Nothing fancy.\nNext is the fragment program, and again enclosed in FRAGMENT define:\n#if defined FRAGMENT\rin vec4 vTexCoord;\rout vec4 outColor;\rlayout(binding=0) uniform sampler2D input_texture;\rvoid main() {\rvec3 color = texture2D(input_texture, vTexCoord.xy).xyz;\routColor = vec4(color, 1);\r}\r#endif // FRAGMENT\r} // glsl ToScreen This code simply reads a texture and outputs it to the screen.\nWe defined the code fragment ToScreen, containing both a vertex and a fragment program, and now we can actually generate the permutation that we need. The code for this in our effect file is:\npass ToScreen {\rvertex = ToScreen\rfragment = ToScreen\r} We are simply defining a pass with the vertex and fragment program defined in the ToScreen code fragment (yes I don\u0026rsquo;t like this term too).\nRunning the code generator on this simple effect file will generate the two files ToScreen.vert and ToScreen.frag.\nThese can be read directly into your favourite OpenGL renderer and used as is!\nThe Parser Now that we have defined the effect and we know what is the outcome of generating code from the effect file, let\u0026rsquo;s look into the different component of the parser and code generator needed.\nBy design, we chose the Lexer to know nothing about the language, so that we can use it between different languages. The entry point to parse the effect is the method generateAST:\nvoid generateAST( Parser* parser ) {\r// Read source text until the end.\r// The main body can be a list of declarations.\rbool parsing = true;\rwhile ( parsing ) {\rToken token;\rnextToken( parser-\u0026gt;lexer, token );\rswitch ( token.type ) {\rcase Token::Token_Identifier:\r{\ridentifier( parser, token );\rbreak;\r}\rcase Token::Type::Token_EndOfStream:\r{\rparsing = false;\rbreak;\r}\r}\r}\r} This code simply process the file - using the lexer - until the end of it, and reads only identifiers. It is the same as the previous article and the previous parser. What changes drastically is the identifier method! We will have 3 different set of identifiers, usable in different parts of the HFX file:\nMain identifiers, \u0026lsquo;shader\u0026rsquo;, \u0026lsquo;glsl\u0026rsquo;, \u0026lsquo;pass\u0026rsquo; Pass identifiers, \u0026lsquo;compute\u0026rsquo;, \u0026lsquo;vertex\u0026rsquo;, \u0026lsquo;fragment\u0026rsquo; Directive identifiers, \u0026lsquo;if defined\u0026rsquo;, \u0026lsquo;pragma include\u0026rsquo;, \u0026rsquo;endif\u0026rsquo; Let\u0026rsquo;s have a look at the code for parsing the main identifiers:\ninline void identifier( Parser* parser, const Token\u0026amp; token ) {\r// Scan the name to know which for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase \u0026#39;s\u0026#39;:\r{\rif ( expectKeyword( token.text, 6, \u0026#34;shader\u0026#34; ) ) {\rdeclarationShader( parser );\rreturn;\r}\rbreak;\r}\rcase \u0026#39;g\u0026#39;:\r{\rif ( expectKeyword( token.text, 4, \u0026#34;glsl\u0026#34; ) ) {\rdeclarationGlsl( parser );\rreturn;\r}\rbreak;\r}\rcase \u0026#39;p\u0026#39;:\r{\rif ( expectKeyword( token.text, 4, \u0026#34;pass\u0026#34; ) ) {\rdeclarationPass( parser );\rreturn;\r}\rbreak;\r}\r}\r}\r} This code simply defers the parsing of a particular identifier using the declaration method corresponding to the identifier. We will look into detail on each method.\nParsing \u0026lsquo;shader\u0026rsquo; We are parsing now the following part from the HFX file:\n// HFX\rshader SimpleFullscreen { This is the entry point of the effect itself. What should the parser do here ? Simply iterate through the main identifiers, \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;. Technically I could have separated the methods to have one with parsing shader only and the others parsing \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;, but did not want to complicate the code further.\nLet\u0026rsquo;s look at how we parse the identifier \u0026lsquo;shader\u0026rsquo;:\n// C++\rinline void declarationShader( Parser* parser ) {\r// Parse name\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r// Cache name string\rStringRef name = token.text;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r}\rwhile ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) {\ridentifier( parser, token );\r}\r} As the previous article\u0026rsquo;s code, this will get the tokens from the lexer and generate data if the syntax is correct. When we enter the method the Lexer will be just at the beginning of the name (SimpleFullscreen), so the code will parse the name, the open brace, and parse everything else until it encounter the close brace.\nThe method identifier will parse also identifiers \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;.\nParsing \u0026lsquo;glsl\u0026rsquo; This is the most complex parsing in the code. I will put both the HFX part and C++ code so hopefully it will be clearer what the parser is doing and why.\nAs a refresh and reference, this is the code fragment ToScreen defined in SimpleFullscreen.hfx:\n// HFX\rglsl ToScreen {\r#pragma include \u0026#34;Platform.h\u0026#34;\r#if defined VERTEX\rout vec4 vTexCoord;\rvoid main() {\rvTexCoord.xy = vec2((gl_VertexID \u0026lt;\u0026lt; 1) \u0026amp; 2, gl_VertexID \u0026amp; 2);\rvTexCoord.zw = vTexCoord.xy;\rgl_Position = vec4(vTexCoord.xy * 2.0f + -1.0f, 0.0f, 1.0f);\r}\r#endif // VERTEX\r#if defined FRAGMENT\rin vec4 vTexCoord;\rout vec4 outColor;\rlayout(binding=0) uniform sampler2D input_texture;\rvoid main() {\rvec3 color = texture2D(input_texture, vTexCoord.xy).xyz;\routColor = vec4(1, 1, 0, 1);\routColor = vec4(color, 1);\r}\r#endif // FRAGMENT\r} Let\u0026rsquo;s start from the beginning. When the parser finds the \u0026lsquo;glsl\u0026rsquo; keyword in the identifier method:\n// C++\rcase \u0026#39;g\u0026#39;:\r{\rif ( expectKeyword( token.text, 4, \u0026#34;glsl\u0026#34; ) ) {\rdeclarationGlsl( parser );\rreturn;\r}\rbreak;\r} It calls the method void declarationGlsl( Parser parser )*.\nThe lexer reading the HFX is after the glsl keyword when entering the method, just before the ToScreen identifier:\n// HFX\rglsl (Here!)ToScreen { Let\u0026rsquo;s see the C++ code step by step. First parsing the name \u0026lsquo;ToScreen\u0026rsquo;:\n// C++\rinline void declarationGlsl( Parser* parser ) {\r// Parse name\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r} as seen in other methods as well. We are defining a new code fragment, thus we need to initialize it. There is tracking of the #ifdef depths to manage when some code must be included in a code fragment and when not:\nCodeFragment code_fragment = {};\r// Cache name string\rcode_fragment.name = token.text;\rfor ( size_t i = 0; i \u0026lt; CodeFragment::Count; i++ ) {\rcode_fragment.stage_ifdef_depth[i] = 0xffffffff;\r}\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r} Next is simply arriving at the first token that contains all the glsl code:\n// Advance token and cache the starting point of the code.\rnextToken( parser-\u0026gt;lexer, token );\rcode_fragment.code = token.text; And now some more parsing craftmanship. We cannot use anymore the simple check to end parsing when encountering a closed brace, because there can be different structs defined that will break that mechanism. Instead we track the number of open braces and when we close the last one, we consider finished the parsing of the code fragment!\nuint32_t open_braces = 1;\r// Scan until close brace token\rwhile ( open_braces ) {\rif ( token.type == Token::Token_OpenBrace )\r++open_braces;\relse if ( token.type == Token::Token_CloseBrace )\r--open_braces; The only token that we care inside the code fragment is the hash, signalling either an include or a define, used for separating per-stage code. The parsing of the hash token will be done inside the directiveIdentifier method:\n// Parse hash for includes and defines\rif ( token.type == Token::Token_Hash ) {\r// Get next token and check which directive is\rnextToken( parser-\u0026gt;lexer, token );\rdirectiveIdentifier( parser, token, code_fragment );\r} Before diving deep into the directive identifiers, let\u0026rsquo;s finish the main parsing routine. We advance to the next token until we close all the braces, and then save the text length of all the code fragment:\nnextToken( parser-\u0026gt;lexer, token );\r}\r// Calculate code string length\rcode_fragment.code.length = token.text.text - code_fragment.code.text; Final step is to save the newly parsed code fragment into the parser data:\nparser-\u0026gt;code_fragments.emplace_back( code_fragment );\r} We can now dive deep into the parsing of directives, namely #if defined, #pragma include and #endif.\nParsing \u0026lsquo;#if defined\u0026rsquo; When we encounter the Hash token within the glsl part, we need to parse further to understand the other keywords. #if defined is the most important directive for us, because it will tell the parser which shader stage we are parsing currently and thus where to direct the text! It starts from a common/shared stage, for shared code, and when encounters a #if defined it can signal a stage specific code.\nNamely when parsing the following line in HFX:\n// HFX\r#(Here!)if defined VERTEX The parser needs to check 2 other identifiers. Remember that the parser is currently AFTER the Hash token, as beautifully written in the previous snippet! Let\u0026rsquo;s look at the code:\n// C++\rinline void directiveIdentifier( Parser* parser, const Token\u0026amp; token, CodeFragment\u0026amp; code_fragment ) {\rToken new_token;\rfor ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase \u0026#39;i\u0026#39;:\r{\r// Search for the pattern \u0026#39;if defined\u0026#39;\rif ( expectKeyword( token.text, 2, \u0026#34;if\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rif ( expectKeyword( new_token.text, 7, \u0026#34;defined\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\r// Use 0 as not set value for the ifdef depth.\r++code_fragment.ifdef_depth;\rif ( expectKeyword( new_token.text, 6, \u0026#34;VERTEX\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Vertex;\r}\relse if ( expectKeyword( new_token.text, 8, \u0026#34;FRAGMENT\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Fragment;\r}\relse if ( expectKeyword( new_token.text, 7, \u0026#34;COMPUTE\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Compute;\r}\r}\rreturn;\r}\rbreak;\r} Let\u0026rsquo;s dissect this code!\nStarting from the current token, just after the #(Hash), we need to check the correct composition of the keywords. We expect \u0026lsquo;if\u0026rsquo;, and then if found we go to the next token:\nif ( expectKeyword( token.text, 2, \u0026#34;if\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token ); We search for the \u0026lsquo;defined\u0026rsquo; identifier and if found we go to the next identifier:\nif ( expectKeyword( new_token.text, 7, \u0026#34;defined\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token ); The parser is currently here:\n#if defined (Here!)VERTEX And thus the last step is to check which shader stage is currently starting. This is done here:\nif ( expectKeyword( new_token.text, 6, \u0026#34;VERTEX\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Vertex;\r} In this central piece of code, we set the current stage to Vertex (because we found the keyword \u0026lsquo;VERTEX\u0026rsquo;) and we save the current ifdef depth. Why that ? Because when we will parse #endif, we will do the same for the open/close braces depth in the main glsl parser: we want to be sure that the defines are paired correctly and we are saving the per-stage code in the correct way! This will be more clear when we see the #endif parsing.\nMoving on, we will do the same for all the other keywords (\u0026lsquo;FRAGMENT\u0026rsquo; and \u0026lsquo;COMPUTE\u0026rsquo; for now):\nelse if ( expectKeyword( new_token.text, 8, \u0026#34;FRAGMENT\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Fragment;\r}\relse if ( expectKeyword( new_token.text, 7, \u0026#34;COMPUTE\u0026#34; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Compute;\r} And the parsing of #if defined is over!\nParsing \u0026lsquo;#pragma include\u0026rsquo; In HFX we are parsing the following:\n// HFX\r#pragma include \u0026#34;Platform.h\u0026#34; With the following code (inside directiveIdentifier method):\n// C++\rcase \u0026#39;p\u0026#39;:\r{\rif ( expectKeyword( token.text, 6, \u0026#34;pragma\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rif ( expectKeyword( new_token.text, 7, \u0026#34;include\u0026#34; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rcode_fragment.includes.emplace_back( new_token.text );\rcode_fragment.includes_stage.emplace_back( code_fragment.current_stage );\r}\rreturn;\r}\rbreak;\r} This is simply saving the filename after the include, that being surrounded by \u0026quot;\u0026quot; is classified as string, and is using the current stage to know which stage should include that file!\nParsing \u0026lsquo;#endif\u0026rsquo; Final part is the #endif identifier:\ncase \u0026#39;e\u0026#39;:\r{\rif ( expectKeyword( token.text, 5, \u0026#34;endif\u0026#34; ) ) {\rif ( code_fragment.stage_ifdef_depth[CodeFragment::Vertex] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\relse if ( code_fragment.stage_ifdef_depth[CodeFragment::Fragment] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\relse if ( code_fragment.stage_ifdef_depth[CodeFragment::Compute] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\r--code_fragment.ifdef_depth;\rreturn;\r}\rbreak;\r} This is mirroring the #if defined and simply goes back to set the current stage to common/shared and reset the per-stage ifdef depth.\nWe can now proceed to the final part of the parsing, the passes! This is the glue to generate the different files from the code fragments.\nParsing \u0026lsquo;pass\u0026rsquo; Reading the HFX file, we are now in the final part of the file:\n// HFX\rpass ToScreen {\rvertex = ToScreen\rfragment = ToScreen\r} A pass is simply a collection of code fragments associated with each shader stage (vertex, fragment, compute). When we parsed the fragments, we saved them in the parser to be retrieved.\nTo refresh our memory, this is the actual Pass struct in C++:\n// C++\rstruct Pass {\rStringRef name;\rconst CodeFragment* vs = nullptr;\rconst CodeFragment* fs = nullptr;\rconst CodeFragment* cs = nullptr;\r}; // struct Pass Going back to the main directive method, we call the declarationPass method when we encounter the \u0026lsquo;pass\u0026rsquo; identifier. We will parse the following line:\n// HFX\rpass ToScreen { With the following code (similar to everything else, it should be easier to read now):\n// C++\rinline void declarationPass( Parser* parser ) {\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\rPass pass = {};\r// Cache name string\rpass.name = token.text;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r} After we saved the pass name we can start reading the individual stages using the passIdentifier method:\nwhile ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) {\rpassIdentifier( parser, token, pass );\r} And then save the newly parsed pass.\nparser-\u0026gt;passes.emplace_back( pass );\r} For each identifier now, we will check which stage we are parsing. Currently we are here, after the open brace and all the whitespace:\n// HFX\rpass ToScreen {\r(Here!)vertex = ToScreen\rfragment = ToScreen\r} What is next is thus checking the identifier and filling the corresponding shader stage of the pass. I will post all the code of the method, because is similar to most code we seen and should be straightforward:\n// C++\rinline void passIdentifier( Parser* parser, const Token\u0026amp; token, Pass\u0026amp; pass ) {\r// Scan the name to know which stage we are parsing for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase \u0026#39;c\u0026#39;:\r{\rif ( expectKeyword( token.text, 7, \u0026#34;compute\u0026#34;) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.cs );\rreturn;\r}\rbreak;\r}\rcase \u0026#39;v\u0026#39;:\r{\rif ( expectKeyword( token.text, 6, \u0026#34;vertex\u0026#34; ) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.vs );\rreturn;\r}\rbreak;\r}\rcase \u0026#39;f\u0026#39;:\r{\rif ( expectKeyword( token.text, 8, \u0026#34;fragment\u0026#34; ) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.fs );\rreturn;\r}\rbreak;\r}\r}\r}\r} The real \u0026lsquo;magic\u0026rsquo; here is the \u0026lsquo;declarationShaderStage\u0026rsquo; method. This method parses the couple \u0026lsquo;identifier\u0026rsquo; \u0026lsquo;=\u0026rsquo; \u0026lsquo;identifier\u0026rsquo;, and searches the code fragment with the same name:\ninline void declarationShaderStage( Parser* parser, const CodeFragment** out_fragment ) {\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Equals ) ) {\rreturn;\r}\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r*out_fragment = findCodeFragment( parser, token.text );\r} After all the stages of the current pass are parsed, we save the pass and finish parsing the file!\nShader Permutation Generation The final step of this amazing journey is the simplest, and it is actually to generate the single files we need. In our case another specific class, CodeGenerator, will generate the different files from the parsed HFX file.\nAfter we\u0026rsquo;ve done with the parsing, we can call the generateShaderPermutations method that will generate files for each shader stage in each pass:\nvoid generateShaderPermutations( CodeGenerator* code_generator, const char* path ) {\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_1.clear();\rcode_generator-\u0026gt;string_buffer_2.clear();\r// For each pass and for each pass generate permutation file.\rconst uint32_t pass_count = (uint32_t)code_generator-\u0026gt;parser-\u0026gt;passes.size();\rfor ( uint32_t i = 0; i \u0026lt; pass_count; i++ ) {\r// Create one file for each code fragment\rconst Pass\u0026amp; pass = code_generator-\u0026gt;parser-\u0026gt;passes[i];\rif ( pass.cs ) {\routputCodeFragment( code_generator, path, CodeFragment::Compute, pass.cs );\r}\rif ( pass.fs ) {\routputCodeFragment( code_generator, path, CodeFragment::Fragment, pass.fs );\r}\rif ( pass.vs ) {\routputCodeFragment( code_generator, path, CodeFragment::Vertex, pass.vs );\r}\r}\r} The code should be straightforward, and the real action happens into the outputCodeFragment method. Let\u0026rsquo;s have a look at the code.\nFirst we define some data, like the file extensions for each shader stage or the defines to compile the code:\n// Additional data to be added to output shaders.\rstatic const char* s_shader_file_extension[CodeFragment::Count] = { \u0026#34;.vert\u0026#34;, \u0026#34;.frag\u0026#34;, \u0026#34;.compute\u0026#34;, \u0026#34;.h\u0026#34; };\rstatic const char* s_shader_stage_defines[CodeFragment::Count] = { \u0026#34;#define VERTEX\\r\\n\u0026#34;, \u0026#34;#define FRAGMENT\\r\\n\u0026#34;, \u0026#34;#define COMPUTE\\r\\n\u0026#34;, \u0026#34;\u0026#34; }; Then we start to write the file. We will use the string_buffer_0 to dynamically generate the path of the file without allocating memory:\nvoid outputCodeFragment( CodeGenerator* code_generator, const char* path, CodeFragment::Stage stage, const CodeFragment* code_fragment ) {\r// Create file\rFILE* output_file;\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_0.append( path );\rcode_generator-\u0026gt;string_buffer_0.append( code_fragment-\u0026gt;name );\rcode_generator-\u0026gt;string_buffer_0.append( s_shader_file_extension[stage] );\rfopen_s( \u0026amp;output_file, code_generator-\u0026gt;string_buffer_0.data, \u0026#34;wb\u0026#34; );\rif ( !output_file ) {\rprintf( \u0026#34;Error opening file. Aborting. \\n\u0026#34; );\rreturn;\r} And then use string_buffer_1 to instead generate the actual code into the file. First, and most important, we will add all the includes for this particular stage by opening the file, reading it into memory and adding it into the final code buffer.\nWe will still use string_buffer_0 to generate the path of the file:\ncode_generator-\u0026gt;string_buffer_1.clear();\r// Append includes for the current stage.\rfor ( size_t i = 0; i \u0026lt; code_fragment-\u0026gt;includes.size(); i++ ) {\rif ( code_fragment-\u0026gt;includes_stage[i] != stage \u0026amp;\u0026amp; code_fragment-\u0026gt;includes_stage[i] != CodeFragment::Common ) {\rcontinue;\r}\r// Open and read file\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_0.append( path );\rcode_generator-\u0026gt;string_buffer_0.append( code_fragment-\u0026gt;includes[i] );\rchar* include_code = ReadEntireFileIntoMemory( code_generator-\u0026gt;string_buffer_0.data, nullptr );\rcode_generator-\u0026gt;string_buffer_1.append( include_code );\rcode_generator-\u0026gt;string_buffer_1.append( \u0026#34;\\r\\n\u0026#34; );\r} After that is done we can copy the define needed for the current shader stage:\ncode_generator-\u0026gt;string_buffer_1.append( \u0026#34;\\t\\t\u0026#34; );\rcode_generator-\u0026gt;string_buffer_1.append( s_shader_stage_defines[stage] ); And finally the actual code:\ncode_generator-\u0026gt;string_buffer_1.append( \u0026#34;\\r\\n\\t\\t\u0026#34; );\rcode_generator-\u0026gt;string_buffer_1.append( code_fragment-\u0026gt;code ); Write to file and close it and we are done!\nfprintf( output_file, \u0026#34;%s\u0026#34;, code_generator-\u0026gt;string_buffer_1.data );\rfclose( output_file );\r} And this will generate the shader permutations for each pass with a single file, using the standard GLSL convention for files extensions.\nConclusions and next part We parsed our simple shader language to enhance and embed glsl code fragments into our codebase by generating single files that can be used into any OpenGL based renderer. We also laid out the foundation for a more powerful tool - namely code generation - even though there are some intermediate steps to be taken to arrive there. First of all, we will need a target rendering library (something like the amazing Sokol), so we can specialize our CPU rendering code. I already wrote something like Sokol but with a more Vulkan/D3D12 interface in mind, and I will use that. Still unsure if I will write a specific post on that.\nIn the next article we will add support for the new graphics library and develop the language more to generate code that will manage Constant buffers, automatically creating a CPU-side class, adding UI to edit it in realtime and possibly load/save the values.\nOf course, any feedback/improvements/suggestions on anything related here (article, code, etc) please let me know.\nStay tuned! Gabriel\n","date":1565111055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566011055,"objectID":"2335acd956430a4df447b79b03bb937a","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_1/","publishdate":"2019-08-06T13:04:15-04:00","relpermalink":"/post/writing_shader_effect_language_1/","section":"post","summary":"Overview Data Driven Rendering Series:\nhttps://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/ In this article we will create a simple language that can encapsulate shader code (called code fragments) and output different files for each fragment. This is the initial step to switch from an engine that loads single files for each shader stage (vertex, fragment, compute, \u0026hellip;) to one that uses an effect file that contains more than one shader.\nWe will start by motivation, then will define the language itself (very simple), then we will look at the Parser and last the Code Generator.","tags":[],"title":"Writing a Shader Effect Language Part 1","type":"post"},{"authors":[],"categories":[],"content":" UI using ImGUI, SDL and the code generated with this article. Motivation Following my previous article about Flatbuffers and data reflection the quest for Data-Driven Rendering continues! In this article I want to show how to write a very simple code-generator to help you automate writing of code in any language. The code is here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nThere is a balance that constantly needs to be found between code and data, and having a code-generator in my opinion helps tremendously in focus on the code that is necessary to be written. From a data perspective, normally the ‘baking’ pipeline is a series of DCC formats as source transformed into very project specific and optimized data. Code-wise, depending on the engine/technology you are using, ‘baking’ of the code is more uncommon. In a time in which iteration time has become almost more important than the tech itself, playing with this balance can be the key for any successful software. It could sound exaggerated, but I really believe in that. As always, both ImGui and SDL will be our sword and shields for this adventure. This will be the second step into data-driven rendering: code generation.\nAre we writing a compiler ? Short answer: yes!\nLong answer: we will be writing the simplest possible compiler that reads a source file and transform in a destination file, like Flatbuffers.\nThere are few links on both theory and practice that can help shed some light on the subject: The “Dragon Book” (called because of the dragon in the cover) is still THE to-go in compiler writing as far as I know. It is an intense book and explores writing a full compiler with depth, starting from Automata theory (just reminds me of how everything you study can be useful, I did 2 exams at University about that, wondering when I would use it! Hello prof Di Battista!) to full code examples:\nhttps://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811\nThis is for me the best website on the subject, very precise and readable and follows closely what is inside the Dragon Book:\nhttps://craftinginterpreters.com/\nAnd github page:\nhttps://github.com/munificent/craftinginterpreters\nMy interest was rekindled in 2015, when I was following the amazing Casey Muratori and his Handmade Hero. He generates code for introspection purposes, and really show a simple and effective way of generating code that works for you.\nWikipedia itself also contains a lot of good articles on the subject. The more you know about the it, the more you want to know. It is fascinating and very, very deep!\nCompiler 101 A real compiler is a very complex and fascinating subject/software so I will try to get the simplest possible approach giving my (flawed and incomplete) perspective.\nA compiler is a series of transformations applied to data (you can apply this definition to every software actually…).\nThe input data is a text, and normally the output is still text, but with very different meaning.\nThe raw depth of the subject is astonishing, consider that we are defining a grammar and thus a language, and how to express concepts into it.\nThe main steps are the following:\nLexer/scanner/tokenizer Parser Code generation We will define the code generator from a custom language called HDF (Hydra Definition Format) to C++. HDF will be a subset of Flatbuffers in this exercise, but once the concepts are clear it can be expanded to more stuff.\nLexer/Scanner/Tokenizer A lexer or scanner (or tokenizer) is a software that translates an input string into a list of Tokens based on Lexemes. A Lexeme is one or more characters that create a Token. Think of a keyword (like ‘if’, ‘class’, ‘static’ …).\nA Token is identified by a unique Lexeme and abstracts the Lexeme itself. It normally contains a type and some attributes, for example it can save where that lexeme is into the input text, the line. The final structure of the token can vary a bit.\nIn trying to find a simple definition for this step:\nThe act of Tokenizing is the act of abstracting the input text.\nFor example, given the following input text:\nstatic void amazing_method() {}; It will generate the list of tokens ‘keyword, identifier, identifier, open parenthesis, close parenthesis, open brace, close brace, semicolon’.\nThis IS abstracting the text!\nNormally a lexer/scanner is used by the parser to go through the code and retrieve a token and use it in some way. Let’s start seeing what a lexer could be!\nCode Let\u0026rsquo;s see the code used by the lexer.\nFirst thing will be to define the Token:\n// Lexer/Tokenizer code. It is abstract enough so is not grammar specific. // struct Token { enum Type { Token_Unknown, Token_OpenParen, Token_CloseParen, Token_Colon, Token_Semicolon, Token_Asterisk, Token_OpenBracket, Token_CloseBracket, Token_OpenBrace, Token_CloseBrace, Token_OpenAngleBracket, Token_CloseAngleBracket, Token_String, Token_Identifier, Token_Number, Token_EndOfStream, }; // enum Type Type type; StringRef text; }; // struct Token It is basically a enum with a StringRef. A StringRef is basically a substring - used to avoid allocations when parsing by simply saving where the Token is in the parsed text and how long it is.\nNext is the Lexer itself:\n// // The role of the Lexer is to divide the input string into a list of Tokens. struct Lexer { char* position = nullptr; uint32_t line = 0; uint32_t column = 0; bool error = false; uint32_t error_line = 0; }; // struct Lexer The most important variable is position - it saves where the Lexer is in the current text for parsing.\nFrom now on there will be only methods.\nFirst some character classification that will help the Lexer:\n// // All those methods are to classify a character. // inline bool IsEndOfLine( char c ) { bool Result = ((c == \u0026#39;\\n\u0026#39;) || (c == \u0026#39;\\r\u0026#39;)); return(Result); } inline bool IsWhitespace( char c ) { bool Result = ((c == \u0026#39; \u0026#39;) || (c == \u0026#39;\\t\u0026#39;) || (c == \u0026#39;\\v\u0026#39;) || (c == \u0026#39;\\f\u0026#39;) || IsEndOfLine( c )); return(Result); } inline bool IsAlpha( char c ) { bool Result = (((c \u0026gt;= \u0026#39;a\u0026#39;) \u0026amp;\u0026amp; (c \u0026lt;= \u0026#39;z\u0026#39;)) || ((c \u0026gt;= \u0026#39;A\u0026#39;) \u0026amp;\u0026amp; (c \u0026lt;= \u0026#39;Z\u0026#39;))); return(Result); } inline bool IsNumber( char c ) { bool Result = ((c \u0026gt;= \u0026#39;0\u0026#39;) \u0026amp;\u0026amp; (c \u0026lt;= \u0026#39;9\u0026#39;)); return(Result); } These should be quite straightforward.\nThen we have the most important method for the lexer: nextToken. This method will contain all the logic to go to the next token, and we will see it step by step.\nFirst is skipping all the whitespaces (empty characters, tabs, returns, etc) to arrive at the correct character in the text.\n// // This is the main method. Skip whitespaces and get next token. Save also the current position in the input string. // void nextToken( Lexer* lexer, Token\u0026amp; token ) { // Skip all whitespace first so that the token is without them. skipWhitespace( lexer ); The code for skipping the whitespace is pretty straight-forward. First it checks if it is a pure whitespace:\nvoid skipWhitespace( Lexer* lexer ) { // Scan text until whitespace is finished. for ( ;; ) { // Check if it is a pure whitespace first. if ( IsWhitespace( lexer-\u0026gt;position[0] ) ) { // Handle change of line if ( IsEndOfLine( lexer-\u0026gt;position[0] ) ) ++lexer-\u0026gt;line; // Advance to next character ++lexer-\u0026gt;position; Then it checks if it is a single line comment:\n} // Check for single line comments (\u0026#34;//\u0026#34;) else if ( (lexer-\u0026gt;position[0] == \u0026#39;/\u0026#39;) \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == \u0026#39;/\u0026#39;) ) { lexer-\u0026gt;position += 2; while ( lexer-\u0026gt;position[0] \u0026amp;\u0026amp; !IsEndOfLine( lexer-\u0026gt;position[0] ) ) { ++lexer-\u0026gt;position; } And last it checks for c-style multiline comments:\n} // Check for c-style multi-lines comments else if ( (lexer-\u0026gt;position[0] == \u0026#39;/\u0026#39;) \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == \u0026#39;*\u0026#39;) ) { lexer-\u0026gt;position += 2; // Advance until the string is closed. Remember to check if line is changed. while ( !((lexer-\u0026gt;position[0] == \u0026#39;*\u0026#39;) \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == \u0026#39;/\u0026#39;)) ) { // Handle change of line if ( IsEndOfLine( lexer-\u0026gt;position[0] ) ) ++lexer-\u0026gt;line; // Advance to next character ++lexer-\u0026gt;position; } if ( lexer-\u0026gt;position[0] == \u0026#39;*\u0026#39; ) { lexer-\u0026gt;position += 2; } } else { break; } } } After skipped all the whitespaces, we initialize the new token:\n// Initialize token token.type = Token::Token_Unknown; token.text.text = lexer-\u0026gt;position; token.text.length = 1; token.line = lexer-\u0026gt;line; We get the current character and advance the position, so we can analize it.\nchar c = lexer-\u0026gt;position[0]; ++lexer-\u0026gt;position; Here comes the character analisys using a simple switch.\nswitch ( c ) { case \u0026#39;\\0\u0026#39;: { token.type = Token::Token_EndOfStream; } break; case \u0026#39;(\u0026#39;: { token.type = Token::Token_OpenParen; } break; case \u0026#39;)\u0026#39;: { token.type = Token::Token_CloseParen; } break; case \u0026#39;:\u0026#39;: { token.type = Token::Token_Colon; } break; case \u0026#39;;\u0026#39;: { token.type = Token::Token_Semicolon; } break; case \u0026#39;*\u0026#39;: { token.type = Token::Token_Asterisk; } break; case \u0026#39;[\u0026#39;: { token.type = Token::Token_OpenBracket; } break; case \u0026#39;]\u0026#39;: { token.type = Token::Token_CloseBracket; } break; case \u0026#39;{\u0026#39;: { token.type = Token::Token_OpenBrace; } break; case \u0026#39;}\u0026#39;: { token.type = Token::Token_CloseBrace; } break; There are some special cases left. First parsing a string starting from a \u0026lsquo;\u0026quot;\u0026rsquo; character. It requires to scan the text until it finds another \u0026lsquo;\u0026quot;\u0026rsquo; to indicate the end of the string. It also supports multiple-line strings with the characters \u0026ldquo;\\\u0026rdquo; (double back-slash)\ncase \u0026#39;\u0026#34;\u0026#39;: { token.type = Token::Token_String; token.text.text = lexer-\u0026gt;position; while ( lexer-\u0026gt;position[0] \u0026amp;\u0026amp; lexer-\u0026gt;position[0] != \u0026#39;\u0026#34;\u0026#39; ) { if ( (lexer-\u0026gt;position[0] == \u0026#39;\\\\\u0026#39;) \u0026amp;\u0026amp; lexer-\u0026gt;position[1] ) { ++lexer-\u0026gt;position; } ++lexer-\u0026gt;position; } // Saves total string length token.text.length = lexer-\u0026gt;position - token.text.text; if ( lexer-\u0026gt;position[0] == \u0026#39;\u0026#34;\u0026#39; ) { ++lexer-\u0026gt;position; } } break; Then the final classification step: first is checking if the token is an identifier (a string literal that starts with a character and is followed by characters, underscores or numbers). If not a identifier, check to see if it is a number. This should be expanded to correctly parse numbers, but for now is not used.. If everything else fails, than we don\u0026rsquo;t recognize the token.\ndefault: { // Identifier/keywords if ( IsAlpha( c ) ) { token.type = Token::Token_Identifier; while ( IsAlpha( lexer-\u0026gt;position[0] ) || IsNumber( lexer-\u0026gt;position[0] ) || (lexer-\u0026gt;position[0] == \u0026#39;_\u0026#39;) ) { ++lexer-\u0026gt;position; } token.text.length = lexer-\u0026gt;position - token.text.text; } // Numbers else if ( IsNumber( c ) ) { token.type = Token::Token_Number; } else { token.type = Token::Token_Unknown; } } break; } } With this code we already have a working Lexer! I like to use the lexer in an abstract way - not knowing anything about the underlying language - so that it can be reused for different custom languages (Dr.Wily eyebrows movement goes here).\nIf you want to dive deeper into this, the amazing Crafting Interpreters contains a great page on scanning:\nhttps://www.craftinginterpreters.com/scanning.html\nAlso, some c-style parsing can be found here from the amazing Niklas Frykohlm:\nhttps://github.com/niklasfrykholm/nflibs/blob/master/nf_json_parser.c\nAnd another amazing parser from STB:\nhttps://github.com/nothings/stb/blob/master/stb_c_lexer.h\nParser So far we have abstracted the input text into a list of Tokens, and now we need to generate some more information before arriving at generating new code.\nAs far as I understood it, a parser reads the tokens and generates an Abstract Syntax Tree.\nSometimes, and in simpler parsers, the act of parsing itself can generates a new code if the language we are targeting is simple. Again, I prefer to separate Lexer and Parser to reuse the Lexer for different languages and separate the responsabilities!\nGiven a list of tokens and a grammar, a parser generates an Abstract Syntax Tree.\nIt gives meaning to the input text, and is responsible to check the syntax correctness.\nA simple definition for a grammar is the following:\nA grammar is a set of production rules that transforms a series of non-terminals into terminals.\nPutting everything in the perspective of data and transformations we can define:\nTerminals are finalized data Non-terminals are data that must be transformed Production rules are transformations of non-terminals to terminals Another definition of a parser than it could be :\nA parser is a software that transforms non-terminals in terminals following production rules.\nGrammar It is time to write the formal grammar (a context-free grammar) and see how it maps to code. It will be very simple — much simpler than many examples you find around — but it is a starting point. We will not deal with any expression, statements and such, not in the context of this code generator. I will point out some examples for more complex stuff, but I want to study more the subject for that to be more precise about the subject.\nEach line will be a production rule (a transformation), with the left-side being always a non-terminal. We are using regular expressions syntax here:\nalphabet → [a-zA-z] number →[0–9] identifier → alphabet (alphabet | number | “_”)* variable_declaration → identifier identifier “;” struct_declaration → “struct” identifier “{“ (variable_declaration)+ “}” “;” enum_declaration → “enum” identifier “{“ (identifier)+ “}” module → (struct_declaration | enum_declaration)+* First we define what an identifier is — a sequence of alpha-numerical characters that can contains also the underscore character.Notice that with the identifier production rule, the identifier cannot start with an underscore. A variable then is declared simply by two identifiers: the first for the type and the second for the name, following a semicolon. A struct is simply a list of variable declarations. Notice the “+” in the rule — this means that at least one element must be present. Enums are literally a name for the enum and a list of identifiers in curly braces. Finally the module is the root of our grammar. It will contain all the declarations we describe. See it as the data file we are writing to generate the code — one file is one module. Now that we defined a simple grammar, we can move to the theory behind the parser.\nPredictive Recursive Descent Parser The grammar we defined is a context-free-grammar. Depending on the type of grammar we can write different parsers. One of the most common type of parser (and easier to start with) is the Predictive Recursive Descent Parser, and that is what we will write given our grammar. You can dive into all the details of writing a context-free grammar, writing a Left-to-right Leftmost-derivation grammar (LL(k)) and such and be amazed by all the concepts behind.\nAgain, I am personally starting on this subject, so my knowledge is not deep.\nBack to the parser, the main characteristics of this parser are:\nDescent = top-down. Start from root and generate the Abstract Syntax Tree. Recursive = the parser has mutually recursive methods, one for each non-terminal. Predictive = no backtracking needed. For our simple grammar we do not need any backtracking. So the parser will start from the root (module non-terminal) and by sequentially reading all the tokens will generate a tree that represent our syntax.\nLet’s see some code!\nCode The central piece of code is the Parser. It uses the Lexer and saves the Types by parsing the input text.\n// // The Parser parses Tokens using the Lexer and generate an Abstract Syntax Tree. struct Parser { Lexer* lexer = nullptr; ast::Type* types = nullptr; uint32_t types_count = 0; uint32_t types_max = 0; }; // struct Parser Let\u0026rsquo;s have a look at the class Type. This class will let us identify correctly primitive types, enums, struct and commands - a special keyword I create to show a concept that can be used away from the canonical C/C++ languages. By saving a list of names and types we can successfully parse all the types listed above.\n// // Define the language specific structures. namespace ast { struct Type { enum Types { Types_Primitive, Types_Enum, Types_Struct, Types_Command, Types_None }; enum PrimitiveTypes { Primitive_Int32, Primitive_Uint32, Primitive_Int16, Primitive_Uint16, Primitive_Int8, Primitive_Uint8, Primitive_Int64, Primitive_Uint64, Primitive_Float, Primitive_Double, Primitive_Bool, Primitive_None }; Types type; PrimitiveTypes primitive_type; StringRef name; std::vector\u0026lt;StringRef\u0026gt; names; std::vector\u0026lt;const Type*\u0026gt; types; bool exportable = true; }; // struct Type } // namespace ast And now the actual code making the magic happens! Entry point for the parsing is generateAST. It simply goes through ALL the tokens until it reaches the end of the file. At this level of parsing, we parse only identifiers (keywords like \u0026lsquo;struct\u0026rsquo;, \u0026rsquo;enum\u0026rsquo;, \u0026hellip;).\nvoid generateAST( Parser* parser ) { // Read source text until the end. // The main body can be a list of declarations. bool parsing = true; while ( parsing ) { Token token; nextToken( parser-\u0026gt;lexer, token ); switch ( token.type ) { case Token::Token_Identifier: { identifier( parser, token ); break; } case Token::Type::Token_EndOfStream: { parsing = false; break; } } } } The method \u0026lsquo;identifier\u0026rsquo; searches for the language keywords and acts accordingly. The method \u0026rsquo;expectKeyword\u0026rsquo; simply checks that the keywords are the same.\ninline void identifier( Parser* parser, const Token\u0026amp; token ) { // Scan the name to know which for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) { char c = *(token.text.text + i); switch ( c ) { case \u0026#39;s\u0026#39;: { if ( expectKeyword( token.text, 6, \u0026#34;struct\u0026#34; ) ) { declarationStruct( parser ); return; } break; } case \u0026#39;e\u0026#39;: { if ( expectKeyword( token.text, 4, \u0026#34;enum\u0026#34; ) ) { declarationEnum( parser ); return; } break; } case \u0026#39;c\u0026#39;: { if ( expectKeyword( token.text, 7, \u0026#34;command\u0026#34; ) ) { declarationCommand( parser ); return; } break; } } } } The next methods are the real core of parsing a language. When declaring a struct, the token we have are:\nIdentifier \u0026lsquo;struct\u0026rsquo; (parsed already by generateAST method) Name of the struct Open braces Zero or more variables The method expectToken checks the presence of the expected token and saves the line if an error occurs.\ninline void declarationStruct( Parser* parser ) { // name Token token; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) { return; } // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = name; type.type = ast::Type::Types_Struct; type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { declarationVariable( parser, token.text, type ); } } } The parsing of a variable is even simpler, just a type followed by the name. When reading the type, it searches through the list of all types saved until then.\ninline void declarationVariable( Parser* parser, const StringRef\u0026amp; type_name, ast::Type\u0026amp; type ) { const ast::Type* variable_type = findType( parser, type_name ); Token token; // Name if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Semicolon ) ) { return; } type.types.emplace_back( variable_type ); type.names.emplace_back( name ); } The parsing of the enum is:\n\u0026rsquo;enum\u0026rsquo; keyword Enum name (optional) Semicolon and type, taken from Flatbuffers syntax Open brace List of identifiers that corresponds to the enum values inline void declarationEnum( Parser* parser ) { Token token; // Name if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; // Optional \u0026#39;: type\u0026#39; for the enum nextToken( parser-\u0026gt;lexer, token ); if ( token.type == Token::Token_Colon ) { // Skip to open brace nextToken( parser-\u0026gt;lexer, token ); // Token now contains type_name nextToken( parser-\u0026gt;lexer, token ); // Token now contains open brace. } if ( token.type != Token::Token_OpenBrace ) { return; } // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = name; type.type = ast::Type::Types_Enum; type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { type.names.emplace_back( token.text ); } } } A command is a special construct that I use in my code, normally with a CommandBuffer, and with the current syntax from HDF:\ncommand WindowEvents { Click { int16 x; int16 y; int16 button; } Move { int16 x; int16 y; } Wheel { int16 z; } }; And this is the parsing of the command. I think this can be the best example of mapping between the language and the parsing. Parsing is:\nName Open brace Scan of identifiers until close brace For each identifier, add a type and scan for internal variables. inline void declarationCommand( Parser* parser ) { // name Token token; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) { return; } // Add new type ast::Type\u0026amp; command_type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; command_type.name = name; command_type.type = ast::Type::Types_Command; command_type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { // Create a new type for each command // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = token.text; type.type = ast::Type::Types_Struct; type.exportable = false; while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { declarationVariable( parser, token.text, type ); } } command_type.names.emplace_back( type.name ); command_type.types.emplace_back( \u0026amp;type ); } } } Abstract Syntax Tree We choose to simply have data definitions, and I’ve decided that the nodes of the tree will be types. A type can be a primitive type, a container of variables (like a Struct in C, but without methods) enums and commands. Commands are just a way of showing the creation of a construct that I use and requires some boilerplate code, but I don’t want to write that code. If we remember the definition of the class Type from the code before, it all boils down to a name,a list of names and optionally types. With this simple definition I can express primitive types, structs and enums all in one! For enums, I save the anme of the enum and in the name list all the different values. That is enough to later generate the code. For structs, again the name is saved, and then the variables. A variable is a tuple of identifiers ‘type, name’. When parsing them, the type is searched in the registered ones. A trick here is to initialize the parser with primitive types, and then add each type (both struct and enums) when parsing them.\nCode Generation The last stage will generate the files in the language that we want, using the informations from the AST. This part will literally write the code for us, the all purpose of this code. The most fundamental question is: “what code do I want to generate?”. A simple but deep question. We are trying to remove the writing of boilerplate code from or lives, so anything that you consider boilerplate and easy to automate goes here. Even if until here we wrote in C++, the final output can be any language. This means that you can define data and translate it to multiple languages!\nFor our example, we will output C++ code and add UI using ImGui, similar to the Flatbuffers example I wrote before. Let’s see the three different construct we can output with our language.\nEnum We defined an enum as a name and a list of named values. For the simplicity of this example, we are not assigning manual values to the enum, but it is something easily changeable, and I will do it in the future. Given the enum in HDF:\nenum BlendOperation : byte { Add, Subtract, RevSubtract, Min, Max } Which code do we want to generate ?\nWhen I write enums, I almost always need the stringed version of the values. Also I want to add a last value, Count, so that I can use it if I need to allocate anything based on the enum. As a bonus, I can create a second enum with the bit shifts — called mask — for some use cases. All of this will be automatically done by the code generator, starting with a simple enum! In this piece of code, I will use three different streams for the different parts of the enum (enum itself, value names and mask) and combine them into the final generated file. Also to note that the strings here are ‘String Ref’ — basically a string that points to the input source code and stores the length of the string, so that there is no need to allocate it newly. I will use a temporary buffer to null terminate it and write into the output file.\nThis will be the generated code:\nnamespace BlendOperation { enum Enum { Add, Subtract, RevSubtract, Min, Max, Count }; enum Mask { Add_mask = 1 \u0026lt;\u0026lt; 0, Subtract_mask = 1 \u0026lt;\u0026lt; 1, RevSubtract_mask = 1 \u0026lt;\u0026lt; 2, Min_mask = 1 \u0026lt;\u0026lt; 3, Max_mask = 1 \u0026lt;\u0026lt; 4, Count_mask = 1 \u0026lt;\u0026lt; 5 }; static const char* s_value_names[] = { \u0026#34;Add\u0026#34;, \u0026#34;Subtract\u0026#34;, \u0026#34;RevSubtract\u0026#34;, \u0026#34;Min\u0026#34;, \u0026#34;Max\u0026#34;, \u0026#34;Count\u0026#34; }; static const char* ToString( Enum e ) { return s_value_names[(int)e]; } } // namespace BlendOperation The enum itself (inside a namespace), a mask and the string version for debugging purposes. All generated from that one line!\nLet\u0026rsquo;s go into a step by step review of the code. First there is the initialization of some auxiliary buffers to handle dynamic strings without allocating memory. These are the usages:\nValues will contain all the enum comma separated values Value_names will contain the string version of the values Value_masks will contain an optional bitmask for the values. void outputCPPEnum( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { // Empty enum: skip output. if ( type.names.size() == 0 ) return; code_generator-\u0026gt;string_buffer_0.clear(); code_generator-\u0026gt;string_buffer_1.clear(); code_generator-\u0026gt;string_buffer_2.clear(); StringBuffer\u0026amp; values = code_generator-\u0026gt;string_buffer_0; StringBuffer\u0026amp; value_names = code_generator-\u0026gt;string_buffer_1; StringBuffer\u0026amp; value_masks = code_generator-\u0026gt;string_buffer_2; We start by adding the character \u0026lsquo;\u0026quot;\u0026rsquo; in the names - they will be C strings! Then we have a couple of options, just as demonstration: add mask (for the bitmask) and add max, that adds a last element to the generated enum.\nvalue_names.append( \u0026#34;\\\u0026#34;\u0026#34; ); bool add_max = true; bool add_mask = true; Next step is the core: go through all the names saved in the enum ast::Type during the parsing phase, and add the literal as is in the enum, the literal in string version and optional mask. We also need to take care of the enum with 1 values, they behave in a different way.\nchar name_buffer[256]; // Enums with more than 1 values if ( type.names.size() \u0026gt; 1 ) { const uint32_t max_values = type.names.size() - 1; for ( uint32_t v = 0; v \u0026lt; max_values; ++v ) { if ( add_mask ) { value_masks.append( type.names[v] ); value_masks.append( \u0026#34;_mask = 1 \u0026lt;\u0026lt; \u0026#34; ); value_masks.append( _itoa( v, name_buffer, 10 ) ); value_masks.append( \u0026#34;, \u0026#34; ); } values.append( type.names[v] ); values.append( \u0026#34;, \u0026#34; ); value_names.append( type.names[v] ); value_names.append( \u0026#34;\\\u0026#34;, \\\u0026#34;\u0026#34; ); } if ( add_mask ) { value_masks.append( type.names[max_values] ); value_masks.append( \u0026#34;_mask = 1 \u0026lt;\u0026lt; \u0026#34; ); value_masks.append( _itoa( max_values, name_buffer, 10 ) ); } values.append( type.names[max_values] ); value_names.append( type.names[max_values] ); value_names.append( \u0026#34;\\\u0026#34;\u0026#34; ); } else { if ( add_mask ) { value_masks.append( type.names[0] ); value_masks.append( \u0026#34;_mask = 1 \u0026lt;\u0026lt; \u0026#34; ); value_masks.append( _itoa( 0, name_buffer, 10 ) ); } values.append( type.names[0] ); value_names.append( type.names[0] ); value_names.append( \u0026#34;\\\u0026#34;\u0026#34; ); } After writing all the values we can add the optional max value in the output:\nif ( add_max ) { values.append( \u0026#34;, Count\u0026#34; ); value_names.append( \u0026#34;, \\\u0026#34;Count\\\u0026#34;\u0026#34; ); if ( add_mask ) { value_masks.append( \u0026#34;, Count_mask = 1 \u0026lt;\u0026lt; \u0026#34; ); value_masks.append( _itoa( type.names.size(), name_buffer, 10 ) ); } } Until now we just saved all those values in the StringBuffers, but still not in the file. The final piece of code output to file the enum with all the additional data:\ncopy( type.name, name_buffer, 256 ); fprintf( output, \u0026#34;namespace %s {\\n\u0026#34;, name_buffer ); fprintf( output, \u0026#34;\\tenum Enum {\\n\u0026#34; ); fprintf( output, \u0026#34;\\t\\t%s\\n\u0026#34;, values.data ); fprintf( output, \u0026#34;\\t};\\n\u0026#34; ); // Write the mask if ( add_mask ) { fprintf( output, \u0026#34;\\n\\tenum Mask {\\n\u0026#34; ); fprintf( output, \u0026#34;\\t\\t%s\\n\u0026#34;, value_masks.data ); fprintf( output, \u0026#34;\\t};\\n\u0026#34; ); } // Write the string values fprintf( output, \u0026#34;\\n\\tstatic const char* s_value_names[] = {\\n\u0026#34; ); fprintf( output, \u0026#34;\\t\\t%s\\n\u0026#34;, value_names.data ); fprintf( output, \u0026#34;\\t};\\n\u0026#34; ); fprintf( output, \u0026#34;\\n\\tstatic const char* ToString( Enum e ) {\\n\u0026#34; ); fprintf( output, \u0026#34;\\t\\treturn s_value_names[(int)e];\\n\u0026#34; ); fprintf( output, \u0026#34;\\t}\\n\u0026#34; ); fprintf( output, \u0026#34;} // namespace %s\\n\\n\u0026#34;, name_buffer ); } Struct Structs are the bread-and-butter of data definition. In this simple example we do not handle pointers or references, so it is pretty straight-forward, but as a start in coding generation this could already be powerful for many cases. Let’s start with a definition for our dream Data-Driven-Rendering:\n// file.hdf struct RenderTarget { uint16 width; uint16 height; float scale_x; float scale_y; TextureFormat format; }; struct RenderPass { RenderTarget rt0; }; We want to generate both the ready to use header in C++ and UI using ImGui. The output for this struct will be obtained by simply iterating through all its members and, based on the type of the member, write some code. For primitive types there is a translation that must be done to the C++ language — thus we saved a list of c++ primitive types keyword into the code. For the UI area we will define two methods: reflectMembers, that simply adds the ImGui commands needed, and reflectUI, that embeds the members into a Window. This is done so that when starting from a root type I can create a window that let me edit its value, and recursively it can add other member’s UI if they are coming from another struct. This is shown with the RenderPass struct.\nThis will be the generated code, that includes ImGui too:\n// CodeGenerated.h struct RenderTarget { uint16_t width; uint16_t height; float scale_x; float scale_y; TextureFormat::Enum format; void reflectMembers() { ImGui::InputScalar( \u0026#34;width\u0026#34;, ImGuiDataType_U16, \u0026amp;width ); ImGui::InputScalar( \u0026#34;height\u0026#34;, ImGuiDataType_U16, \u0026amp;height ); ImGui::InputScalar( \u0026#34;scale_x\u0026#34;, ImGuiDataType_Float, \u0026amp;scale_x ); ImGui::InputScalar( \u0026#34;scale_y\u0026#34;, ImGuiDataType_Float, \u0026amp;scale_y ); ImGui::Combo( \u0026#34;format\u0026#34;, (int32_t*)\u0026amp;format, TextureFormat::s_value_names, TextureFormat::Count ); } void reflectUI() { ImGui::Begin(\u0026#34;RenderTarget\u0026#34;); reflectMembers(); ImGui::End(); } }; // struct RenderTarget Now let\u0026rsquo;s have a look at the code that will generate that. First some init steps: clear and alias the StringBuffer, allocate some char buffers on the stack, copy the StringRef into the name buffer:\nvoid outputCPPStruct( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { const char* tabs = \u0026#34;\u0026#34;; code_generator-\u0026gt;string_buffer_0.clear(); StringBuffer\u0026amp; ui_code = code_generator-\u0026gt;string_buffer_0; char name_buffer[256], member_name_buffer[256], member_type_buffer[256]; copy( type.name, name_buffer, 256 ); Next is already a powerful piece of code. Outputting the UI code and iterating through each member.\nif ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026#34;\\n\\tvoid reflectMembers() {\\n\u0026#34; ); } fprintf( output, \u0026#34;%sstruct %s {\\n\\n\u0026#34;, tabs, name_buffer ); for ( int i = 0; i \u0026lt; type.types.size(); ++i ) { const ast::Type\u0026amp; member_type = *type.types[i]; const StringRef\u0026amp; member_name = type.names[i]; copy( member_name, member_name_buffer, 256 ); We are in the middle of the loop, and we want to check if the current member type is a primitive one, then it needs some work to do. First, output the language specific primitive type keyword (using the s_primitive_type_cpp array). Second, add some ImGui code to edit the field directly.\n// Translate type name based on output language. switch ( member_type.type ) { case ast::Type::Types_Primitive: { strcpy_s( member_type_buffer, 256, s_primitive_type_cpp[member_type.primitive_type] ); fprintf( output, \u0026#34;%s\\t%s %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { switch ( member_type.primitive_type ) { case ast::Type::Primitive_Int8: case ast::Type::Primitive_Uint8: case ast::Type::Primitive_Int16: case ast::Type::Primitive_Uint16: case ast::Type::Primitive_Int32: case ast::Type::Primitive_Uint32: case ast::Type::Primitive_Int64: case ast::Type::Primitive_Uint64: case ast::Type::Primitive_Float: case ast::Type::Primitive_Double: { ui_code.append( \u0026#34;\\t\\tImGui::InputScalar( \\\u0026#34;%s\\\u0026#34;, %s, \u0026amp;%s );\\n\u0026#34;, member_name_buffer, s_primitive_type_imgui[member_type.primitive_type], member_name_buffer ); break; } case ast::Type::Primitive_Bool: { ui_code.append( \u0026#34;\\t\\tImGui::Checkbox( \\\u0026#34;%s\\\u0026#34;, \u0026amp;%s );\\n\u0026#34;, member_name_buffer, member_name_buffer ); break; } } } break; } In case of a struct as a member, use the typename as is and call the \u0026lsquo;reflectMembers\u0026rsquo; method for the UI generation:\ncase ast::Type::Types_Struct: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;%s\\t%s %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026#34;\\t\\tImGui::Text(\\\u0026#34;%s\\\u0026#34;);\\n\u0026#34;, member_name_buffer ); ui_code.append( \u0026#34;\\t\\t%s.reflectMembers();\\n\u0026#34;, member_name_buffer ); } break; } For enums use the format namespace::Enum that comes with the generated code (and can be anything else) and add a Combo for ImGui. The combo is using the string array generated previously! This is powerful!\ncase ast::Type::Types_Enum: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;%s\\t%s::Enum %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026#34;\\t\\tImGui::Combo( \\\u0026#34;%s\\\u0026#34;, (int32_t*)\u0026amp;%s, %s::s_value_names, %s::Count );\\n\u0026#34;, member_name_buffer, member_name_buffer, member_type_buffer, member_type_buffer ); } break; } To finish up simlpy add the reflectUI method, that embed the members reflection in a window and finish.\ndefault: { break; } } } ui_code.append( \u0026#34;\\t}\u0026#34; ); ui_code.append( \u0026#34;\\n\\n\\tvoid reflectUI() {\\n\\t\\tImGui::Begin(\\\u0026#34;%s\\\u0026#34;);\\n\\t\\treflectMembers();\\n\\t\\tImGui::End();\\n\\t}\\n\u0026#34;, name_buffer ); fprintf( output, \u0026#34;%s\\n\u0026#34;, ui_code.data ); fprintf( output, \u0026#34;\\n%s}; // struct %s\\n\\n\u0026#34;, tabs, name_buffer ); } Command I wanted to include an example of something that does not exist in any language, but it shows the power of removing boilerplate code.\nI define commands as little structs with a type used anytime I need to do some command parsing, normally from a ring buffer.\nThe command should have an enum with all the types already, and each struct should have its type assigned. The type is normally used to cycle through the commands and do something accordingly.\nIt will output structs because of the need to allocate them in the ring buffer, thus must be simple.\nFirst let\u0026rsquo;s see the HDF file. The example are window events commands:\ncommand WindowEvents { Click { int16 x; int16 y; int16 button; } Move { int16 x; int16 y; } Wheel { int16 z; } }; The generated code will be:\nnamespace WindowEvents { enum Type { Type_Click, Type_Move, Type_Wheel }; struct Click { int16_t x; int16_t y; int16_t button; static Type GetType() { return Type_Click; } }; // struct Wheel struct Move { int16_t x; int16_t y; static Type GetType() { return Type_Move; } }; // struct Wheel struct Wheel { int16_t z; static Type GetType() { return Type_Wheel; } }; // struct Wheel }; // namespace WindowEvents And finally the C++ code that generates the output. The output starts with an enum with all the types, that I normally use to switch commands:\nvoid outputCPPCommand( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { char name_buffer[256], member_name_buffer[256], member_type_buffer[256]; copy( type.name, name_buffer, 256 ); fprintf( output, \u0026#34;namespace %s {\\n\u0026#34;, name_buffer ); // Add enum with all types fprintf( output, \u0026#34;\\tenum Type {\\n\u0026#34; ); fprintf( output, \u0026#34;\\t\\t\u0026#34; ); for ( int i = 0; i \u0026lt; type.types.size() - 1; ++i ) { const ast::Type\u0026amp; command_type = *type.types[i]; copy( command_type.name, name_buffer, 256 ); fprintf( output, \u0026#34;Type_%s, \u0026#34;, name_buffer ); } const ast::Type* last_type = type.types[type.types.size() - 1]; copy( last_type-\u0026gt;name, name_buffer, 256 ); fprintf( output, \u0026#34;Type_%s\u0026#34;, name_buffer ); fprintf( output, \u0026#34;\\n\\t};\\n\\n\u0026#34; ); Then we output all the command structs (like Click, Move, \u0026hellip;). For each command type we output a struct with all its members. This is similar to the output of the structs:\nconst char* tabs = \u0026#34;\\t\u0026#34;; for ( int i = 0; i \u0026lt; type.types.size(); ++i ) { const ast::Type\u0026amp; command_type = *type.types[i]; copy( command_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;%sstruct %s {\\n\\n\u0026#34;, tabs, member_type_buffer ); for ( int i = 0; i \u0026lt; command_type.types.size(); ++i ) { const ast::Type\u0026amp; member_type = *command_type.types[i]; const StringRef\u0026amp; member_name = command_type.names[i]; copy( member_name, member_name_buffer, 256 ); // Translate type name based on output language. switch ( member_type.type ) { case ast::Type::Types_Primitive: { strcpy_s( member_type_buffer, 256, s_primitive_type_cpp[member_type.primitive_type] ); fprintf( output, \u0026#34;%s\\t%s %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); break; } case ast::Type::Types_Struct: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;%s\\t%s %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); break; } case ast::Type::Types_Enum: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;%s\\t%s::Enum %s;\\n\u0026#34;, tabs, member_type_buffer, member_name_buffer ); break; } default: { break; } } } copy( command_type.name, member_type_buffer, 256 ); fprintf( output, \u0026#34;\\n%s\\tstatic Type GetType() { return Type_%s; }\\n\u0026#34;, tabs, member_type_buffer ); fprintf( output, \u0026#34;\\n%s}; // struct %s\\n\\n\u0026#34;, tabs, name_buffer ); } copy( type.name, name_buffer, 256 ); fprintf( output, \u0026#34;}; // namespace %s\\n\\n\u0026#34;, name_buffer ); } Conclusions We learnt how to write a complete Code Generator, an incredible tool that can speed up the development if used correctly and remove most boilerplate code possible.\nThe usage of the command keyword was an example of something I use and I don’t want to write code, something that is custom enough and hopefully will give you more ideas on how you can break free from languages constriction when you write…your own language!\nIn the quest for data-driven rendering, the next step will be to use the knowledge from code generation to create a shader effect language, that can generate both CPU and GPU code for you.\nThis article is the longest and more code-heavy I have ever written. There are many concepts that I am beginning to be familiar with, but still not so used to.\nSo please comment, give feedback, share! Thank you for reading!\n","date":1564267563,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565131563,"objectID":"3b9e1a648d0f65843a8cdecb32355e6c","permalink":"https://jorenjoestar.github.io/post/writing_a_simple_code_generator/","publishdate":"2019-07-27T18:46:03-04:00","relpermalink":"/post/writing_a_simple_code_generator/","section":"post","summary":"UI using ImGUI, SDL and the code generated with this article. Motivation Following my previous article about Flatbuffers and data reflection the quest for Data-Driven Rendering continues! In this article I want to show how to write a very simple code-generator to help you automate writing of code in any language. The code is here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nThere is a balance that constantly needs to be found between code and data, and having a code-generator in my opinion helps tremendously in focus on the code that is necessary to be written.","tags":[],"title":"Writing a simple Code Generator","type":"post"},{"authors":[],"categories":[],"content":" Some of the UI for the Hydra NES emulator, using ImGUI. Writing an emulator is an incredibly fun learning experience.\nIt is an exquisite exercise in reverse-engineering from both documentation and code.\nIn this post I want to share some tips on how and where to start based on my experience on the NES emulator I am writing.\nInformation The gathering of information is the most important (and hard!) process that will live through all the writing process.\nLuckily for us there are many websites to help in this:\nhttps://wiki.nesdev.com/w/index.php/NES_reference_guide\nhttp://forums.nesdev.com/\nhttp://obelisk.me.uk/6502/reference.html\nhttp://www.oxyron.de/html/opcodes02.html\nIt is paramount to create a list of websites and resources (maybe through some notes, like in Evernote or such) about different topics regarding the hardware to be emulated.\nHaving a central hub is powerful and counteract the sparseness of the different informations (some in txt files, different websites, forum blogposts, …).\nI can’t stress enough how important it is.\nThe amazing NesDev Wiki is the hub you need. Almost every possible information is there.\nArchitecture Next step is to understand the architecture. Write diagrams, take notes, search for the relationships of the component. What does every hardware component do ? What can that specific hardware piece access to ?\nAs you will see, writing the emulator is an iterative process of improving each component until you have something that works very well, and then refine for an infinite amount of time. On a very basic level, there should be a CPU, some form of GPU (PPU, Picture Processing Unit), some audio chip, some input peripheral and cartridge/disc/rom.\nNES architecture The NES is a beautiful machine equipped with the following:\nCPU : Ricoh RP2A03 (NTSC) / RP2A07 (PAL) 8 bit processor that contains both CPU and APU (audio) hardware. The addresses are 16 bit, but the data is 8. It contains only specific registers: 2 indices, accumulator, stack pointer, program counter and status.\nPPU : Ricoh RP2C02 (NTSC) / RP2C07 (PAL) This is what today would be called GPU. It outputs to a 256x240 pixels buffer, it has 2kib or RAM, 32 bytes for palette RAM and 288 bytes for sprite RAM. The PPU is tile based and it takes 8 PPU cycles to load a line of a background tile. Sprites are sent through DMA and background is filled during Vertical Blank state normally. A frame lasts more scanline that the one visible, so that the game can upload data to the PPU when not rendering.\nAPU : Ricoh RP2A03 (NTSC) / RP2A07 (PAL) (Contained in the CPU itself.) The sound is analogic and it comes from 5 different channels: 2 pulse, 1 triangle, 1 noise and 1 DMC. All the channels aside from the DMC create signals that are combined to output the sounds and music. The DMC loads samples using the DMA.\nCartridge/Mappers : This is a very unique topic strict to the NES as far as I know. Cartridges had unique hardware and they were used to swap banks of memory in realtime to access different parts of the cartridge. There are hundred of mappers that have unique behaviours! The biggest gist of the mappers is how they switch banks: by WRITING to the address where the execution code is it triggers the bank-switching logic. There can be internal batteries and working RAMs too, but they are very rare.\nMemory mapped I/O The different hardware access using ‘memory mapped I/O’, that is a way of saying that when you read or write to a specific address it could be memory or it could be an hardware-component.\nExamples: reading from address 0x4016 gives you the gamepad status, while reading from 0x1000 reads from the CPU ram.\nHaving clear these accesses will help in understanding even better the machine.\nBoth CPU and PPU have different memory maps. Let\u0026rsquo;s see them, it will help in understanding the internal of the NES better.\nCPU Memory Map The CPU can access basically every hardware component in the NES. PPU, APU, gamepads, both read and write.\nIt reads the ROM part of a cartridge (called PRG) and executes its instructions. Through PPU registers it can instruct the PPU to read graphical informations from the CHR part of the cartridge. It can upload sprites on the PPU Sprite Memory through DMA, upload data to the APU, or manage its internal RAM.\nFrom the source code, this is a working example of CPU Reading method:\nuint8 Nes::MemoryController::CpuRead( uint16 address ) { if ( address \u0026lt; 0x2000 ) { return cpu-\u0026gt;ram[address \u0026amp; 0x7FF]; } else if ( address \u0026lt; 0x4000 ) { return ppu-\u0026gt;CpuRead( address ); } else if ( address \u0026lt; 0x4014 ) { return apu-\u0026gt;CpuRead( address ); } else if ( address \u0026gt;= 0x4018 ) { return mapper-\u0026gt;PrgRead( address ); } switch ( address ) { case 0x4015: { return apu-\u0026gt;ReadStatus(); break; } case 0x4016: { return controllers-\u0026gt;ReadState(); break; } case 0x4017: { return 0x40; break; } } return 0; } And CPU Write:\nvoid Nes::MemoryController::CpuWrite( uint16 address, uint8 data ) { if ( address \u0026lt; 0x2000 ) { cpu-\u0026gt;ram[address \u0026amp; 0x7FF] = data; } else if ( address \u0026lt; 0x4000 ) { ppu-\u0026gt;CpuWrite( address, data ); return; } else if ( address \u0026lt; 0x4014 ) { return apu-\u0026gt;CpuWrite( address, data ); } else if ( address \u0026gt;= 0x4018 ) { mapper-\u0026gt;PrgWrite( address, data ); return; } switch ( address ) { // Sprite DMA case 0x4014: { cpu-\u0026gt;ExecuteSpriteDMA( data ); return; break; } case 0x4015: case 0x4017: { apu-\u0026gt;CpuWrite( address, data ); return; break; } case 0x4016: { controllers-\u0026gt;WriteState( data ); return; break; } } } The pattern is always the same: check the address of the instruction and choose which hardware component to interact with.\nHopefully its clear that based on the address different components can be accessed. Let\u0026rsquo;s have a look at the PPU too.\nPPU Memory Map Similar to the CPU, reading and writing on the PPU access different components, even though they are far less. The PPU either accesses its 2 rams (palette and nametable, normally from the CPU) or reads the CHR (that is the graphical data stored in the cartridge) memory.\nReading:\nuint8 Nes::MemoryController::PpuRead( uint16 address ) { address \u0026amp;= 0X3FFF; if ( address \u0026lt;= 0x1FFF ) { return mapper-\u0026gt;ChrRead( address ); } else if ( address \u0026lt;= 0x3EFF ) { return ppu-\u0026gt;nametableRam[NameTableMirroring( address, mapper-\u0026gt;mirroring )]; } else if ( address \u0026lt;= 0x3FFF ) { // Palette mirroring is handled in the write code. return ppu-\u0026gt;paletteRam[address \u0026amp; 0x1F] \u0026amp; ((ppu-\u0026gt;mask \u0026amp; Nes::Ppu::MaskFlag_GreyScale ? 0x30 : 0xFF)); } return 0; } On the writing side, there the code shows the intricancy of emulation. When writing to the paletter ram, there is a mirroring mechanism happening in the hardware that is emulated with a lookup table. Something to look out to: writing to CHR is 99% of the time useless, unless there is an additional RAM in the cartdige.\nvoid Nes::MemoryController::PpuWrite( uint16 address, uint8 data ) { address \u0026amp;= 0X3FFF; if ( address \u0026lt;= 0x1FFF ) { mapper-\u0026gt;ChrWrite( address, data ); return; } else if ( address \u0026lt;= 0x3EFF ) { ppu-\u0026gt;nametableRam[NameTableMirroring( address, mapper-\u0026gt;mirroring )] = data; return; } else if ( address \u0026lt;= 0x3FFF ) { static uint8 const palette_write_mirror[0x20] = { 0x10, 0x01, 0x02, 0x03, 0x14, 0x05, 0x06, 0x07, 0x18, 0x09, 0x0A, 0x0B, 0x1C, 0x0D, 0x0E, 0x0F, 0x00, 0x11, 0x12, 0x13, 0x04, 0x15, 0x16, 0x17, 0x08, 0x19, 0x1A, 0x1B, 0x0C, 0x1D, 0x1E, 0x1F }; ppu-\u0026gt;paletteRam[palette_write_mirror[address \u0026amp; 0x1F]] = data; return; } } Takeaways I created the memory controller as the main dispatcher of data between hardware components, to separate the duties better. We can see the following relationships based on that:\nCPU can access PPU, APU, controllers and cartridge (PRG) PPU can access screen, its own rams and cartridge (CHR) memory controller is the hub that connects everything I am not sure this is the best emulator architecture, but that is what I figured out.\nTest roms A fundamental approach to create a robust emulator is to have some tests to rely on. Sadly it is not common for all hardware, but again the NES provide plenty of roms that tests almost every aspect of your emulator! It quickly becomes a test-driven development.\nNES test roms link\nFind roms, read the source code and try to understand what they are doing and why.\nCoding start If you are writing your first emulator, I suggest to focus mostly on the emulation part.\nWhat do I mean by that ? Avoid trying too many things at once! Focus your energies towards the emulation. Use libraries that are reliable and simple and that you know. GLFW, SDL2, etc are your friends here. You want to eliminate most unknowns unknowns before hand. Of course, if you are brave enough, you can also write an emulator in a new language.\nBut for me, I preferred to concentrate on the emulation side first, in C++, using my core library, especially knowing that I could dedicate some night-time here and there, No surprises (not really true, still some happened!).\nI will possibly port the emulator to use SDL if needed, but right now the emulation code is the most important.\nThis is the mantra that helped me concentrate only on the emulation code. Again, writing-wise I am not happy about the code quality. But what I am learning from different perspectives is invaluable!\nNES coding start The quintessential basic steps to start a NES emulator coding are:\nWrite CPU basics (fetch/decode/execute loop, registers) Basic memory bus (read/write to/from memory and registers) Load a rom and start executing instruction step by step. It is already a lot, and it will require to read multiple times the different wiki pages and forum posts.\nFor a typical console, the main loop (simplified) can be something like this:\nvoid CpuTick() { uint8_t opcode = Read(program_counter++); uint8_t operand = FetchOperand(opcode); ExecuteOpcode(opcode, operand); } void ExecuteFrame() { uint32_t cycles_per_frame = … while (cycles_per_frame — ) { CpuTick(); } } To jumpstart your NES emulator you can use the majestic rom nestest.nes and its log file: it gives you a test of all instructions of the CPU and prints the status of the CPU after each one.\nAlso it does not require any PPU rendering: compare the status of your CPU with the text file line by line and its done!\nYou can see some ugly but useful code in MainState::ExecuteCpuTest in my emulator for an idea.\nA line from the nestest.log file looks like this:\n// C000 4C F5 C5 JMP $C5F5 A:00 X:00 Y:00 P:24 SP:FD PPU: 0, 0 CYC:7 it gives you the ProgramCounter (C000), byte code (1, 2 or 3 bytes depending on the instructions), human-readable-instruction (JMP) , the CPU register contents (A, X, Y, P, SP) and the theorethical PPU scanline, pixel and clock cycle.\nThere are two interesting points:\nThe ProgramCounter before execution should be set to C000 for this rom only and only when logging. The CPU cycles STARTS at 7. In a power-up/reset method there is some work done BEFORE executing any code. This is needed only if you want to have a precise cycle-to-cycle comparison. You can create a simple test method like this:\nvoid TestEmulatorCPU() { Reset(); while(true) { CpuTick(); CompareCpuStatusWithLog(); } } and catch the problems in your CPU instructions implementation!\nConclusion This is a little help in understanding how to start with an emulator.\nIt is a beautiful journey, but it is full of trial and errors.\nI am myself far from over with my emulator, and also far from being happy on HOW I write the emulator itself.\nThere are emulators of much more complex machines out there (almost every machine you can imagine!) and it blows my mind to know there are people that can emulate such complex hardware.\nThe ideal situation would be to being able of not being lost in visual emulation of the circuitry, but for now that is out of my league.\nI am thinking of creating some a series of videos and code associated starting from scratch, if anyone is interested. Please leave a comment/feedback on the article, the source code, anything!\nI hope it will help.\n","date":1564267535,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564267535,"objectID":"33334c3b94bfe395ca48dde1b71dc142","permalink":"https://jorenjoestar.github.io/post/emulation_where_to_start/","publishdate":"2019-07-27T18:45:35-04:00","relpermalink":"/post/emulation_where_to_start/","section":"post","summary":"Some of the UI for the Hydra NES emulator, using ImGUI. Writing an emulator is an incredibly fun learning experience.\nIt is an exquisite exercise in reverse-engineering from both documentation and code.\nIn this post I want to share some tips on how and where to start based on my experience on the NES emulator I am writing.\nInformation The gathering of information is the most important (and hard!) process that will live through all the writing process.","tags":[],"title":"Emulation: where to start? A use case.","type":"post"},{"authors":[],"categories":[],"content":" Auto generated UI from Flatbuffers files. Motivation Finding a good balance between code and data in Rendering. What is the necessary code that should be written ? Why ?\nIn rendering many areas can be described in a fast and robust way using data. A pipeline (in D3D12/Vulkan lingo) for example is a collection of different states: depth stencil, alpha blend, rasterizer, shaders, etc. All those state can be hard-coded or defined in data. Moving them to data can help with the visibility of them, that instead of being buried somewhere into the code can be retrieved before even running the application.\nAs a bigger-scope example, a frame-graph can be implicitly defined inside the code, if different areas, or in data. Recent posts about it started raising attention to the problem, especially after the introduction of lower-level APIs like D3D12 and Vulkan and their resource barriers. I’ve personally used something like json (xml back in the day) since 2009, after asking myself the very silly question:\nwhat is the biggest dependency in rendering?Render Targets!\nSince then I saw only in the Codemasters postprocess system (since Dirt 2) a similar approach, and have never being able to advocate towards it. The only full use case I have is my personal indie game (a full deferred rendering pipeline with many different rendering needs) all defined in a json file (render_pipeline.json). Anyway, a couple of examples of this data-driven mentality can be found here:\nhttp://bitsquid.blogspot.com/2017/03/stingray-renderer-walkthrough-7-data.html\nI chose to see what is a good way of describing low-level rendering resources, the bricks towards data-driven rendering. I’ve already tried defining them in a json file, but wanted something more direct — something I can copy easily with minimal parsing.\nI found 4 possible approaches:\nCustom data language Already existing data language Json (already used) Hard-coding everything In this experiment I’ve chosen Flatbuffers for the easy of use, the good performances and the feature set that seems complete. As an exercise, I wanted to create some UI based on the data coming from Flatbuffers without having to write too much code.\nFlatbuffers Flatbuffers is a serialization library developer by Google used by many companies.\nhttps://google.github.io/flatbuffers/\nCompared to Protocol Buffers (still developed by Google) it tries to go towards a very simple parsing/unpacking (actually ABSENT in Flatbuffers, so much faster to read/write) and serialization speed.\nFlatbuffers is mainly a compiler that accepts .fbs (FlatBuffers Schema) files and can generate code for serialization purposes.\nThe advantage is that it automatically generates the parsing files in the language you prefer (C++, Java, C#, Go, C, Lua, Javascript, Rust) without you needing to write the always tedious serialize/deserialize methods.\nIt is largely based on either simple c-structs or tables with offsets for more complex object.\nThe objective here will be to create a schema file, define a couple of resources (like textures) and use those to automatically generate UI. I will be using the SDL + ImGUI sample from the amazing ImGUI as a base.\nThe flow will be the following:\nWrite schema files Generate reflection informations Parse schemas Generate UI Schema Files Let’s write our first schema file. A bigger version (that I am using for my low-level renderer) is included in the github repository.\nnamespace rendering; enum TextureFormat : ushort { UNKNOWN, R32G32B32A32_TYPELESS, R32G32B32A32_FLOAT, R32G32B32A32_UINT, R32G32B32A32_SINT, R32G32B32_TYPELESS, R32G32B32_FLOAT, R32G32B32_UINT, R32G32B32_SINT, R16G16B16A16_TYPELESS, R16G16B16A16_FLOAT, R16G16B16A16_UNORM, R16G16B16A16_UINT, R16G16B16A16_SNORM, R16G16B16A16_SINT, R32G32_TYPELESS, R32G32_FLOAT, R32G32_UINT, R32G32_SINT, R10G10B10A2_TYPELESS, R10G10B10A2_UNORM, R10G10B10A2_UINT, R11G11B10_FLOAT, R8G8B8A8_TYPELESS, R8G8B8A8_UNORM, R8G8B8A8_UNORM_SRGB, R8G8B8A8_UINT, R8G8B8A8_SNORM, R8G8B8A8_SINT, R16G16_TYPELESS, R16G16_FLOAT, R16G16_UNORM, R16G16_UINT, R16G16_SNORM, R16G16_SINT, R32_TYPELESS, R32_FLOAT, R32_UINT, R32_SINT, R8G8_TYPELESS, R8G8_UNORM, R8G8_UINT, R8G8_SNORM, R8G8_SINT, R16_TYPELESS, R16_FLOAT, R16_UNORM, R16_UINT, R16_SNORM, R16_SINT, R8_TYPELESS, R8_UNORM, R8_UINT, R8_SNORM, R8_SINT, R9G9B9E5_SHAREDEXP, D32_FLOAT_S8X24_UINT, D32_FLOAT, D24_UNORM_S8_UINT, D24_UNORM_X8_UINT, D16_UNORM, S8_UINT, BC1_TYPELESS, BC1_UNORM, BC1_UNORM_SRGB, BC2_TYPELESS, BC2_UNORM, BC2_UNORM_SRGB, BC3_TYPELESS, BC3_UNORM, BC3_UNORM_SRGB, BC4_TYPELESS, BC4_UNORM, BC4_SNORM, BC5_TYPELESS, BC5_UNORM, BC5_SNORM, B5G6R5_UNORM, B5G5R5A1_UNORM, B8G8R8A8_UNORM, B8G8R8X8_UNORM, R10G10B10_XR_BIAS_A2_UNORM, B8G8R8A8_TYPELESS, B8G8R8A8_UNORM_SRGB, B8G8R8X8_TYPELESS, B8G8R8X8_UNORM_SRGB, BC6H_TYPELESS, BC6H_UF16, BC6H_SF16, BC7_TYPELESS, BC7_UNORM, BC7_UNORM_SRGB, FORCE_UINT } attribute \u0026#34;ui\u0026#34;; struct RenderTarget { width : ushort (ui: \u0026#34;min:1, max:16384\u0026#34;); height : ushort; scale_x : float; scale_y : float; format : TextureFormat; } There are few things here to discuss.\nEnums. Flatbuffers can generate enums with string version of each values and conversions between enum and string. Struct. It is exactly like C/C++: a simple struct that can be memcopied. Different than a Table (that can point to other structs and Tables). Attributes. This can be used to define custom parsable attributes linked to a member of a struct/table. They can be used, for example, to drive the UI generation. Generating Reflection Informations After we generated the schema file, we can serialize it and load/save it from disk. But we need reflection data to be able to automatically generate the UI we need! There are two main reflection mechanisms in Flatbuffers: mini-reflection and full-reflection. We will use both to generate a UI using ImGUI and see the differences.\nMini-Reflection This is the simplest of the two and works by generating an additional header file for each .fbs file we use. The command line is the following:\nflatc --cpp RenderDefinitions.fbs --reflect-names This will generate the RenderDefinitions_Generated.h file that must be included in your application and has the downside of needing you to recompile every time you change the data.\nAlso, and this is the biggest downside, I could not find any way to parse custom per-member attributes.\nI hope I am wrong, but could not find any documentation on the topic: everything seems to point towards the full reflection mechanism.\nSo why bothering with the mini-reflection ?\nMini-reflection generates code, and this became useful for one of the most tedious C/C++ code to write: enums!\nI can’t count how many times I wrote an enum, I wanted the string with the same value for it (for example to read from a json file and get the proper enum value) and every time an enum is changed is painful.\nSo a lesson from the mini-reflection is to have a code-generator for enums for C/C++, and I will show an example soon in another article.\nBack to the enums, Flatbuffers generates:\nEnum Name array Value array Enum to name method A nice property of the generated code for the enum is that it is easy to copy-paste in any c++ file — no Flatbuffers involved!\nThis is my first choice now when I want to write an enum in any c++ application.\nFull-reflection This is the most used (or at least documented) form of reflection in Flatbuffers.\nIt use a very elegant solution, totally data-driven: it reads a reflection schema file that can parse…ANY other schema!\nThis very Inception-esque mechanism gives the full access to all the types, including Attributes.\nBy executing this command:\nflatc.exe -b --schema reflection.fbs RenderDefinitions.fbs the RenderDefinitions.bfbs (binary fbs) file is generated.\nThis is the file that needs to be read to fully reflect the types inside the .fbs file. The order of operations is the following:\nGenerate a binary fbs with flatc (with the command line shown) Load the bfbs file generated Load the schema from the bfbs Reflect The fbfs file contains all the informations from the schema: types, enums, attributes.\nParsing schemas and Generating UI For both reflection mechanisms the objective is the same: given a type (RenderTarget) generate an editor that can edit properties and potentially load/save them.\nMini-Reflection The UI generation is pretty straightforward with mini-reflection.\nEach type defined in the .fbs file contains a type_name-TypeTable() method that gives accent to a TypeTable.\nThis contains a list of per-member type, name and default values.\nWhat is really missing here is the attributes, that could be used to generate custom UI in a more specific way (eg. adding a min/max/step to a slider).\nThe code doing this is in the github sample.\nThere are few interesting points here.\nImGui usability In order to use ImGui to modify a struct, I had to create the class FlatBuffersReflectionTable to instantiate a struct with a similar layout than the Flatbuffers struct.\nThis is annoying but I could not find a way around different than this.\nWith this in-place, a ImGUI slider can point to a memory area that can be used to save/load the data. Let’s begin by retrieving the TypeTable:\nconst TypeTable* rt_table = rendering::RenderTargetTypeTable(); The TypeTable is what is included in the generated header and contains the reflection informations. Listing the members and their type is pretty straight-forward:\nfor ( uint32_t i = 0; i \u0026lt; type_table.num_elems; ++i ) { const flatbuffers::TypeCode\u0026amp; type_code = type_table.type_codes[i]; ImGui::Text( \u0026#34;%s: %s\u0026#34;, type_table.names[i], flatbuffers::ElementaryTypeNames()[type_code.base_type] ); sprintf_s( s_string_buffer, 128, \u0026#34;%s\u0026#34;, type_table.names[i] ); if ( type_code.sequence_ref == 0 ) { if ( type_table.type_refs[type_code.sequence_ref] ) { const flatbuffers::TypeTable* enum_type = type_table.type_refs[type_code.sequence_ref](); ImGui::Combo( s_string_buffer, (int32_t*)reflection_table.GetData( i ), enum_type-\u0026gt;names, enum_type-\u0026gt;num_elems ); } } else { switch ( type_code.base_type ) { case flatbuffers::ET_BOOL: { ImGui::Checkbox( s_string_buffer, (bool*)reflection_table.GetData( i ) ); break; } } } } The interesting parts:\nflatbuffers::TypeCode* contains the reflection information for a type.\nGiven a type_code, sequence_ref can be used to check if it is an enum, pointer, or primitive type. In this case is used for enum, showing a combo with all the selectable values.\nBase_type contains instead the primitive type. In this example a bool can be mapped to a checkbox. This uses the custom reflection_table class to have a memory area for ImGUI.\nFor mini-reflection this is basically it.\nFull-reflection Code here is longer but it follows the 4 steps highlighted before.\nAll the code is inside the ReflectUIFull method.\nHere the binary fbs file and its corresponding schema are loaded.\n// 1. Obtain the schema from the binary fbs generated std::string bfbsfile; flatbuffers::LoadFile(\u0026#34;..\\\\data\\\\RenderDefinitions.bfbs\u0026#34;, true, \u0026amp;bfbsfile ); const reflection::Schema\u0026amp; schema = *reflection::GetSchema( bfbsfile.c_str() ); The schema can be used to list the types:\n// 2. List all the types present in the fbs. auto types = schema.objects(); for ( size_t i = 0; i \u0026lt; types-\u0026gt;Length(); i++ ) { const reflection::Object* type = types-\u0026gt;Get( i ); ImGui::Text( \u0026#34; %s\u0026#34;, type-\u0026gt;name()-\u0026gt;c_str() ); } (Using the auto here because I am lazy. The type is some multiple templates of offsets…) We can also list all the enums:\nauto enums = schema.enums(); for ( size_t i = 0; i \u0026lt; enums-\u0026gt;Length(); i++ ) { const reflection::Enum* enum_ = enums-\u0026gt;Get( i ); ImGui::Text( \u0026#34; %s\u0026#34;, enum_-\u0026gt;name()-\u0026gt;c_str() ); } A problem I found (with a workaround in the code) is that enums do not have an easily to access array of string values.\nSo I generated one for the sake of example, but I am far from happy with the solution!\nGoing forward, we can get the type we want to reflect (notice the full namespace.type):\nauto render_target_type = types-\u0026gt;LookupByKey( \u0026#34;rendering.RenderTarget\u0026#34; ); and begin the work on each field: auto fields = render_target_type-\u0026gt;fields(); if ( fields ) { // 5.1. List all the fields for ( size_t i = 0; i \u0026lt; fields-\u0026gt;Length(); i++ ) { auto field = fields-\u0026gt;Get( i ); ... and the UI can be generated.\nFor each field, the primitive type can be accessed with the following:\nreflection::BaseType field_base_type = field-\u0026gt;type()-\u0026gt;base_type(); and again, I found a workaround to know if a type is primitive or an enum.\nLast piece of the puzzle: attributes!\nauto field_attributes = field-\u0026gt;attributes(); if ( field_attributes ) { auto ui = field_attributes-\u0026gt;LookupByKey( \u0026#34;ui\u0026#34; ); if ( ui ) { ImGui::Text(\u0026#34;UI attribute: %s\u0026#34;, ui-\u0026gt;value()-\u0026gt;c_str()); } } These can be parsed as strings and can be used to drive UI code (like a slider with min, max and steps).\nConclusions In the end, I’ve managed to generate UI based on a type without too much code.\nThere was some reverse-engineering to do because I could not find proper documentation (I possibly miss some links to a in-depth example of reflection!) but nothing major.\nThe full source code:\n(https://github.com/JorenJoestar/FlatbuffersReflection)\n","date":1564141046,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564141046,"objectID":"afba9f8775578a3383382dfbe5a617c4","permalink":"https://jorenjoestar.github.io/post/flatbuffers_reflection_data_driven_rendering/","publishdate":"2019-07-26T07:37:26-04:00","relpermalink":"/post/flatbuffers_reflection_data_driven_rendering/","section":"post","summary":"Auto generated UI from Flatbuffers files. Motivation Finding a good balance between code and data in Rendering. What is the necessary code that should be written ? Why ?\nIn rendering many areas can be described in a fast and robust way using data. A pipeline (in D3D12/Vulkan lingo) for example is a collection of different states: depth stencil, alpha blend, rasterizer, shaders, etc. All those state can be hard-coded or defined in data.","tags":[],"title":"Flatbuffers, Reflection and Data-Driven Rendering","type":"post"},{"authors":[],"categories":[],"content":" Legend of Zelda emulated plus debugging windows. Hello everyone!\nToday I release the source code of my bare-bone NES emulator, written in C++.\nI had the idea to write an emulator of one of my favorite console (after the SNES) years ago, and started in 2015 to write the first code (actually in 2008, but it was too daunting even to start). Then I concentrated on my other big project (still ongoing) and left all the NES code on a side. Years passed and finally last winter I decided to give it a go to arrive at a ‘usable’ emulator level and release the source code.\nHere it is! (https://github.com/JorenJoestar/HydraNes)\nMotivation Main motivation both to write and to share this code is knowledge.\nI shamelessly wrote bad code just with the purpose of seeing something on screen as fast as I could. And I am very honest about that: not happy for the form, but happy for the knowledge I gained! Also, I think that this code is compact enough to be followed and to understand the basics of NES emulation coding.\nThe code The NES code lives in the Nes.h/.cpp pair of files. The APU is implemented using Blargg’s implementation: when I’ll have other time I will attemp to finish my own implementation, but for now it is ok like that.\nThe flow is the following:\nNES is initialized After loading a rom (from the Cartridge window) the mapper will be selected and memory copied to local buffers. CPU starts its continuous emulation. CPU will execute until a frame is produced. This is checked by the PPU frame changing. PPU execution is bound to memory accesses, both read and write. Each CPU memory access corresponds to 3 PPU cycles (in NTSC, the only region emulated). After the frame is ended the APU emulation is advanced. Interesting spots There are different areas of the code that are interesting, but I would like to highlight some.\nCpu::Step() This is where all the CPU instructions are executed. I opted for a macro based approach instead of tables of function pointers.\nFor each cpu cycle:\nFetch the instruction opcode Calculate the operand address (called ‘effectiveAddress’) Execute the operation All the operations and addressing modes are in the Nes.h file. Addressing modes are the way the NES gets its operand for each operation. Operations are the instruction themselves — using those operands.\nPpu::Step() PPU by itself is the most difficult part to emulate (APU is easier on the channels, but harder on the mix and signal generation!).\nI will make a post about that soon, but in the meantime here the code is and implements the behaviours described here:\nhttps://wiki.nesdev.com/w/index.php/File:Ntsc_timing.png\nThe PPU draws in tiles of 8x8 pixels, so for each pixels created on the screen there will be a gathering of all the data necessary to calculate the final color.\nThe rendering is divided in background and sprites.\nBackground is just 8x8 pixel per tile choosen from the nametable (a screen table of which tiles are visible) and sprites are either 8x8 or 8x16 rectangles coming from a different memory area (uploaded using DMA).\nThere are many quirks and uniqueness about the PPU, like the pattern table (a 16x16 grid storing the higher 2 bits of all the underlying background pixels), or the vertical blank period, or the open bus.\nPpu::DrawPixel() The color of a pixel comes from one of the 16 entries of the palette VRAM, and to do so 4 bits must be calculated for background and for sprites.\nFor background tiles, 2 pixels comes from the ‘texture’ (CHR-ROM) and 2 from the attribute table. Sprites contains all those informations together.\nThe output is a silly SSBO that contains RGBA colors to be used in a compute shader that outputs to the screen.\nCpuRead/Write, PpuRead/Write All those methods are essential because the NES uses memory mapping i/o to access the different hardware.\nFor example the PPU access the cartridge through the mapper in the memory controller to read drawing informations, the CPU writes to the PPU using address $2007, etc.\nEnding notes I will prepare more detailed posts about the NES architecture and emulation, even though there are still some concepts that are not clear to me and require a deeper investigation.\nSo far this is the most satisfactory personal project I’ve done, and one of the few that arrived at a usable level.\nIn the future I want to improve this emulator and use the knowledge to explore the writing of a SNES emulator!\nAny question or comment please let me know!\nGabriel\n","date":1563861890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563861890,"objectID":"96109d55d54d12572b76d4542ac35062","permalink":"https://jorenjoestar.github.io/post/releasing_nes_emulator_source/","publishdate":"2019-07-23T02:04:50-04:00","relpermalink":"/post/releasing_nes_emulator_source/","section":"post","summary":"Legend of Zelda emulated plus debugging windows. Hello everyone!\nToday I release the source code of my bare-bone NES emulator, written in C++.\nI had the idea to write an emulator of one of my favorite console (after the SNES) years ago, and started in 2015 to write the first code (actually in 2008, but it was too daunting even to start). Then I concentrated on my other big project (still ongoing) and left all the NES code on a side.","tags":[],"title":"Releasing NES Emulator Source","type":"post"}]