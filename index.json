[{"authors":["admin"],"categories":null,"content":"Video-games Lover. Guitar enthusiast. Life-surfer.\nPassion, mistakes and curiosity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jorenjoestar.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Video-games Lover. Guitar enthusiast. Life-surfer.\nPassion, mistakes and curiosity.","tags":null,"title":"Gabriel Sassone","type":"authors"},{"authors":[],"categories":[],"content":"After reading the great article by Sarah Drasner on productivity I wanted to share some other improvements that I use in my daily work and personal coding life.I am developing mostly rendering and other game-related code, so my OS is Windows 10.I have a background in using Linux-only for work at the beginning of my career, so bash customization and Vim were too useful to be overlooked!\nFor Windows I started using Cmder few years ago, but I should check also the revamped powershell.What I love about Cmder is that it gives you most of Unix/Linux scripts into Windows.And with that it comes also an \u0026lsquo;alias\u0026rsquo; file, in the path %CMDER_ROOT%\\config\\user_aliases.cmd.\nHere are some group of aliases I use to speed up my productivity!\nKnowing your aliases First of all some commands to know and edit your aliases.\nI tend to write aliases with acronyms for faster typing.\nma=cat %CMDER_ROOT%\\config\\user_aliases.cmd ea=vim %CMDER_ROOT%\\config\\user_aliases.cmd M.A. stands for My Aliases, while E.A. Edit Aliases.\nThis is the base - when I forget an alias I just type ma and I have all my list!\nJumping around Navigation is the first type of enhancement I recommend.This is HUGE for me and incredibly simple:\n..=cd .. ..2=cd ../.. ..3=cd ../../.. ..4=cd ../../../.. Navigate to parent folders in a much faster way!\nThe second one is actually\u0026hellip;jumps (thinking of ASM instructions)!When I identify some folders that I access often, I add these kind of lines:\njc= cd /D C:\\Coding jp= cd /D C:\\Users\\Gabriel\\Documents\\Visual Studio 2017\\Projects Again aliases. Jump Coding and Jump Projects.\nNotice the argument /D to use the absolute path - needed when you have paths in other folders.\nFile listing Again another simple trick, and you can add more variations to your needs.ls is the dir command of Linux/Unix, and is another foundation.ls=ls --show-control-chars -F --color $* ll=ls --show-control-chars -F -l --color $* lr=ls --show-control-chars -F -lrt --color $* la=ls --show-control-chars -F -a --color $* Coloring is necessary to know what is a folder and what a file, something that should be enabled by default in my opinion. Also notice the \u0026lsquo;$*\u0026rsquo; at the end - it means to append all the argument that you want to pass after the alias!\nText editing I mainly use Sublime Text and occasionally VIM, so here are some aliases as well:\nvi=vim $* vimrc=vim %CMDER_ROOT%\\vendor\\msysgit\\share\\vim\\vimrc subl=\u0026quot;C:\\Program Files\\Sublime Text 3\\sublime_text.exe\u0026quot; $* With the alias subl you can open any file into Sublime Text. Very handy combination Cmder + Sublime Text!\nAlso quickly editing your vimrc file is a need for VIM users.\nGrepping I honestly completely forgot to add the color option\u0026hellip;learning through sharing. Thank you Sarah :)\nAdded the color option, these are the two variations of grep I use the most:\ngrep=grep --color $* gric=grep --color -Iir $* gril=grep --color -Iirl $* gric stands for Grep Ignore Case Ignore Binary Recursive - more or less.gril is like gric, but just lists file instead of content per file in the search. I use it to just check files.\nQuickly going through the options for grep:\n -I: let you ignore binary files. Speed up searching quite a bit. -i: ignore case. -r: recursively search directories. -l: only list files, not content.  I\u0026rsquo;ll show you the difference.\nLet\u0026rsquo;s search for \u0026lsquo;blipbuffer\u0026rsquo; into my HydraNes/src folder:\ngrep --color -Iir blipbuffer *\nWe\u0026rsquo;ll loose color in the post, but this is the result:\nsrc/main.cpp: if (nes-\u0026gt;apu.blipBuffer-\u0026gt;samples_avail()) { src/main.cpp: int32 readSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;read_samples(sampleBuffer, kBufferSize, false); src/main.cpp: const int32 availableSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;samples_avail(); src/main.cpp: const int32 readSamples = nes-\u0026gt;apu.blipBuffer-\u0026gt;read_samples( bufferAddress, kBufferSize, false ); src/Nes.cpp: if ( !blipBuffer ) { src/Nes.cpp: blipBuffer = new Blip_Buffer(); src/Nes.cpp: blipBuffer-\u0026gt;clock_rate( CpuClockRate ); src/Nes.cpp: blipBuffer-\u0026gt;sample_rate( SampleRate ); src/Nes.cpp: externalApu-\u0026gt;output( blipBuffer ); src/Nes.cpp: blipBuffer-\u0026gt;clear(); src/Nes.cpp: blipBuffer-\u0026gt;clear(); src/Nes.cpp: blipBuffer-\u0026gt;end_frame( count ); src/Nes.cpp: // //blipBuffer-\u0026gt;end_frame( remainingCycles ); src/Nes.h:#include \u0026quot;BlipBuffer/blip_buf.h\u0026quot; src/Nes.h: Blip_Buffer* blipBuffer = nullptr; Instead using the list only option:\ngrep --color -Iirl blipbuffer *\nGives you this result:\nsrc/main.cpp src/Nes.cpp src/Nes.h Git This is another big one. Git can have very verbose commands, so aliases save a lot of typing!Again I add generic and very specific version of commands:\ngs=git status gl=git log --oneline --all --graph --decorate $* ga=git add \u0026quot;$*\u0026quot; gcm=git commit -m \u0026quot;$*\u0026quot; grmdir=git rm -r \u0026quot;$*\u0026quot; grmf=git rm \u0026quot;$*\u0026quot; gpso=git push -u origin \u0026quot;$*\u0026quot; gpsom=git push -u origin master gplo=git pull origin \u0026quot;$*\u0026quot; gplom=git pull origin master gru=git remote update gsr=git status -uno -u gt=git stash gts=git stash show -p gtl=git stash list gta=git stash apply gsps=git subtree push --previs= $* gspl=git subtree pull --previs= $* gspsh=git subtree push --prefix=source/hydra hydra master gspsl=git subtree pull --prefix=source/hydra hydra master See the difference between gpso and gpsom - the second one just using the master branch.gpsom and gplom are the ones I use the most and this again saves a lot of time.\nSame for the subtree commands, showing how I update my code using the common libraries names as hydra.In this case - and this is more a git concept - when working with subtree I use a remote alias, added with git remote add -f 'name' https://....git .\ngs is great to see what is the status of the current repository you are in. gl logs all the commits. ga adds the files and folders you write after the command.\nVisual Studio Compiler Some different aliases I use for Visual Studio:\nvs=\u0026quot;%VS140COMNTOOLS%..\\IDE\\devenv.exe\u0026quot; /edit \u0026quot;$*\u0026quot; ;= Needed to find MSBuild executable. vcvars=\u0026quot;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\u0026quot; msb=MSBuild $1 /property:Configuration=$2 /property:Platform=$3 msbd=MSBuild $1 /property:Configuration=Debug /property:Platform=x64 msbr=MSBuild $1 /property:Configuration=Release /property:Platform=x64 Again something as msb can be used to build code from a Visual Studio Solution. msbd and msbr are useful shortcuts for again commonly used configurations and platforms.\nConclusions There are many ways to improve productivity - and reducing the amount of stuff you have to write, for repetitive tasks, is very powerful. Hope this helps and again thanks to Sarah Drasner for the article that sparked the idea of writing this one!\nIf anybody wants to add more, comment, feedback please write to me! Gabriel\n","date":1586788980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586788980,"objectID":"c878b34d79c4a846d8504c72f6698bda","permalink":"https://jorenjoestar.github.io/post/productivity_terminal/productivity_terminal/","publishdate":"2020-04-13T10:43:00-04:00","relpermalink":"/post/productivity_terminal/productivity_terminal/","section":"post","summary":"After reading the great article by Sarah Drasner on productivity I wanted to share some other improvements that I use in my daily work and personal coding life.I am developing mostly rendering and other game-related code, so my OS is Windows 10.I have a background in using Linux-only for work at the beginning of my career, so bash customization and Vim were too useful to be overlooked!\nFor Windows I started using Cmder few years ago, but I should check also the revamped powershell.","tags":[],"title":"Improving Productivity in Terminals with Aliases","type":"post"},{"authors":[],"categories":[],"content":"Overview In the last articles we looked at progressively building tools to move rendering code towards data.We looked on how to create a simple lexer, a simple parser and a code generator.With those we were able to create a very simple language to augment shaders.\nWhy that ?\nThere are few reasons:\n Shader languages misses any way of linking multiple programs Shader languages misses any way to define render states CGFX and Microsoft FX are mostly dead Ability to use ANY shader language - and just add the infrastructure Ability to generate permutations without manually creating them  Hydra FX aims to add all the missing features and becoming just an augmentation to ANY shader language.\nThere is also a fundamental reason for me:\n Define all the rendering informations needed as soon as possible!\n With this reason then defining everything inside an HFX file, with the possibility of overriding some properties, it is paramount.\nThis is by far a new concept, and with the newer Graphics APIs (Vulkan and D3D12) it is becoming more and more of a need.In this article we will see how we have all those features and add the missing ones. I will try to define everything in a more API-Independent way, so it can be adapted to any engine needs.\nWhy we need shader augmentation The augmentation that I have in mind is needed for different reasons, not only if you are targeting the newer APIs.Some of the reasons are the following:\n Write the NECESSARY code. Nothing more. Logically group shaders in the same file. Describe a priori all the STATIC parts of rendering. Being more data driven, improve iteration time. Being more data driven, encourage rendering experimentation. Easiness of debugging. Encourages less hardcoded data.  Write the NECESSARY code. Nothing more. As I wrote in previous articles writing code is our biggest power and liability.Wasting time writing useless code is like slowly typing with one finger, without any clue of a design nor knowledge of the subject.Of course this is very personal, but any time I have to reiterate some steps (in anything in my life) for no purpose but bad-design/bad-architecture/technical-debt it really makes me feel bad.Again, it is like playing Diablo and clicking all the time to attack, instead of knowing that you can hold the mouse button!\nFinally to the topic: shader augmentation means moving to data what many times is expressed in code.We can both have a data-driven rendering, or even generate code for us, or a combination of both.There is not right or wrong, and this will change in the future!The best solution is the one that solves your (incredibly well described) problem.\nAdding render states, vertex inputs, render graph informations let a simple text file to find its space into your awesome rendering quite easily.\nLogically group shaders in the same file. Having to write separated files it can be ok, but many times having everything in one file (well divided) will be easier to logically connects the shader themselves.Sometimes you can get lost into the combination of shaders quite easily.And anyway you NEED to define which shaders are used together since the dawn of time.\nSo put them into the same file!\nDescribe a priori all the STATIC parts of rendering. Knowing all the static parts of rendering can lead to offline analysis and build, statistics, and all kind of things you can think of.It also serves to really have knowledge of the combinational explosion of rendering before it arrives in your beloved renderer!Sometimes you can group shaders together and improve speed and usability by just analysing how similar some shaders are.\nBeing more data driven, improve iteration time. If you think of reloading assets, then a shader reload will also load all the render stage associated.If you want to bring it a step further, adding/removing passes, changing were in the render graph the shaders are used can be an incredible tool to quickly prototype, optimize, develop ideas.\nBeing more data driven, encourage rendering experimentation. You can also add some non-coding tools to augment a shader with all those data.And again, defining this in data let\u0026rsquo;s you check relationship with the rest of the renderer more easily.\nEasiness of debugging. Data-drivenness means that data is always available.In the example I am adding here, you can see how useful can be to even have a simple ImGui debug of a HFX file.Bring that to a realtime renderer, and you can quickly debug rendering problems without having to use external tools like Pix, RenderDoc and such. These are wonderful tools, but I always love to have a defense before arriving there.\nAn example is to debug on someone\u0026rsquo;s machine that does not have installed those tools.\nSame can be applied to performances, to quickly check performances in realtime.\nTooling is essential to any developer, and should be developed with the technology itself.\nEncourages less hardcoded data. Nothing wrong to hardcoding data, and many times is necessary and useful.But the question is: when is necessary ?\nHaving a common data format gives you the tools (see previous point) to analyze, compared to hardcoded.\nShader Augmentations We will take a HFX shader and will see all the augmentations.This is used in the Render Pipeline Article and renders the GLTF assets:\nshader PBR { properties { albedo_texture(\u0026quot;Albedo\u0026quot;, 2D) = \u0026quot;\u0026quot; normals_texture(\u0026quot;Normals\u0026quot;, 2D) = \u0026quot;\u0026quot; metal_roughness_texture(\u0026quot;MetalRoughness\u0026quot;, 2D) = \u0026quot;\u0026quot; emissive_texture(\u0026quot;Emissive\u0026quot;, 2D) = \u0026quot;\u0026quot; occlusion_texture(\u0026quot;Occlusion\u0026quot;, 2D) = \u0026quot;\u0026quot; scale(\u0026quot;Scale\u0026quot;, Float) = 16.0 } layout { vertex main3D { binding 0 16 vertex attribute float3 Position 0 0 0 attribute ubyte4n Color 0 1 12 } vertex main3DPosition { binding 0 12 vertex attribute float3 Position 0 0 0 } vertex main3DPositionNormal { binding 0 12 vertex binding 1 12 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } vertex gbuffer { binding 0 12 vertex binding 1 12 vertex binding 2 8 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float2 UV 2 2 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } list gbuffer { cbuffer ViewConstants ViewConstants; texture2D albedo; texture2D normals; texture2D metalRoughness; texture2D emissive; texture2D occlusion; sampler2D linear_sampler } } sampler_states { state linear_sampler { Filter MinMagMipLinear AddressU Clamp AddressV Clamp } } render_states { state main { Cull Back ZTest LEqual ZWrite On } state linesZTest { Cull None ZTest LEqual ZWrite Off BlendMode Alpha } } glsl GBuffer_V { #pragma include \u0026quot;Platform.h\u0026quot; layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 2) in vec2 UV; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; out vec2 uv; out vec3 worldPosition; void main() { vertexNormal = (inverse(transpose((instanceTransform))) * vec4(Normal,0)).rgb; uv = UV; vec4 world_pos = instanceTransform * vec4(Position.xyz, 1.0f); worldPosition = world_pos.xyz; gl_Position = view_projection_matrix * world_pos; } } glsl GBuffer_F { #pragma include \u0026quot;Platform.h\u0026quot; layout (location = 0) out vec4 Out_Color; layout (location = 1) out vec4 Out_Normals; layout (location = 2) out vec4 Out_Properties0; //layout (location = 3) out vec4 Out_WorldPosition; layout(binding=0) uniform sampler2D albedo; layout(binding=1) uniform sampler2D normals; layout(binding=2) uniform sampler2D metalRoughness; layout(binding=3) uniform sampler2D emissive; layout(binding=4) uniform sampler2D occlusion; in vec3 vertexNormal; in vec2 uv; in vec3 worldPosition; void generate_TB_basis( out vec3 vT, out vec3 vB, vec2 texST, vec3 base_normal, vec3 sigma_x, vec3 sigma_y, float flip_sign ) { vec2 dSTdx = dFdxFine ( texST ) , dSTdy = dFdyFine ( texST ) ; float det = dot ( dSTdx , vec2 ( dSTdy .y ,- dSTdy .x )); float sign_det = det \u0026lt;0 ? -1 : 1; // invC0 represents ( dXds , dYds ) ; but we don â€™t divide by // determinant ( scale by sign instead ) vec2 invC0 = sign_det * vec2 ( dSTdy .y , - dSTdx .y ); vT = sigma_x * invC0 .x + sigma_y * invC0 .y; if( abs ( det ) \u0026gt; 0.0) vT = normalize ( vT ); vB = ( sign_det * flip_sign ) * cross ( base_normal , vT ); } void main() { // Calculate gradient base: vec3 base_normal = normalize(vertexNormal); vec3 position_derivate_x = dFdxFine( worldPosition ); vec3 position_derivate_y = dFdyFine( worldPosition ); vec3 sigma_x = position_derivate_x - dot( position_derivate_x, base_normal ) * base_normal; vec3 sigma_y = position_derivate_y - dot( position_derivate_y, base_normal ) * base_normal; float flip_sign = dot ( position_derivate_y, cross ( base_normal, position_derivate_x )) \u0026lt; 0 ? -1 : 1; vec3 tangent, bitangent; generate_TB_basis( tangent, bitangent, uv.xy, base_normal, sigma_x, sigma_y, flip_sign ); vec3 tangent_normal = texture(normals, uv.xy).xyz * 2 - 1; vec3 normal = tangent * tangent_normal.x + bitangent * tangent_normal.y + base_normal * tangent_normal.z; normal = normalize(normal); vec3 emissive_color = texture(emissive, uv.xy).rgb; //Out_Normals = vec4(vertexNormal, 1); //Out_Normals = vec4(tangent_normal * 0.5 + 0.5, 1); Out_Normals = vec4(normal, emissive_color.r); vec3 color = texture(albedo, uv.xy).xyz; float occlusion = texture(occlusion, uv.xy).r; Out_Color = vec4(color, occlusion); // G = Rougthness, B = Metalness vec2 roughness_metal = texture(metalRoughness, uv.xy).yz; Out_Properties0 = vec4(roughness_metal.xy, emissive_color.gb); // TODO: remove! This is to test world space reconstruction! //Out_WorldPosition = vec4(worldPosition, 1); } } glsl PositionOnly { #pragma include \u0026quot;Platform.h\u0026quot; #if defined VERTEX layout (location = 0) in vec3 Position; uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; void main() { gl_Position = view_projection_matrix * vec4(Position.xyz, 1.0f); } out vec4 vTexCoord; #endif // VERTEX #if defined FRAGMENT layout (location = 0) out vec4 Out_Color; void main() { Out_Color = vec4(1,1,1,1); } #endif // FRAGMENT } glsl PositionNormals { #pragma include \u0026quot;Platform.h\u0026quot; #if defined VERTEX layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; void main() { vertexNormal = Normal; gl_Position = view_projection_matrix * instanceTransform * vec4(Position.xyz, 1.0f); } #endif // VERTEX #if defined FRAGMENT layout (location = 0) out vec4 Out_Color; layout (location = 1) out vec4 Out_Normals; in vec3 vertexNormal; void main() { Out_Normals = vec4(vertexNormal * 0.5 + 0.5, 1); vec3 L = vec3(-0.7, 0.7, 0 ); float lambert_diffuse = max(0, dot(vertexNormal, L)); Out_Color = vec4(lambert_diffuse.xxx, 1); } #endif // FRAGMENT } pass GBuffer { resources = gbuffer render_states = main vertex_layout = gbuffer vertex = GBuffer_V fragment = GBuffer_F } pass PositionN { render_states = main vertex_layout = main3DPositionNormal vertex = PositionNormals fragment = PositionNormals } pass PositionOnly { render_states = main vertex_layout = main3DPosition vertex = PositionOnly fragment = PositionOnly } } 1: Linking Multiple Programs This is a pretty simple task, and the first one to be tackled.In Vulkan all the Pipelines need all the shader used at creation, using an array of VkPipelineStageCreationInfo for graphics, compute and ray-tracing.\nIn D3D12, you have the ShaderBytecode used in the pipelines, but not as arrays (just member of the various creation structs).\nFrom a functionality perspective, they are EQUAL. It makes sense - a Pipeline is the description of all the static part of a GPU pipeline, and shaders are amongst the most important part of it.\nYou can see it in the \u0026lsquo;pass\u0026rsquo; section of the HFX file:\npass PositionOnly { vertex = PositionOnly fragment = PositionOnly ... } For a compute pipeline is even simpler, and dispatch size can be added as well:\npass DeferredCompute { compute = DeferredCompute dispatch = 32, 32, 1 ... } Even just with something like this it is easy to organize different shaders.\n2: Define Render States Following the previous point, Pipelines need also (almost) all the render states (depth/stencil, alpha, raster, \u0026hellip;) to be defined.This was one of the main features of CGFX and Microsoft\u0026rsquo;s FX - and still now is incredibly useful.Unity\u0026rsquo;s ShaderLab also incorporates render states.\nI decided to separate render states on their own group:\nrender_states { state main { Cull Back ZTest LEqual ZWrite On } state linesZTest { Cull None ZTest LEqual ZWrite Off BlendMode Alpha } } Here two different render states are defined.In this case a render states defines depth/stencil, blend and rasterization.\nA great addition to that is to add the possibility of inherit/override render states.For example in a Transparent pass, the blend state could be defined in the Render Pass data, and be inherited explicitly here.\nAlso very important is the definition of input assembly - how the vertices are fed into the vertex program:\nlayout { vertex main3D { binding 0 16 vertex attribute float3 Position 0 0 0 attribute ubyte4n Color 0 1 12 } vertex main3DPosition { binding 0 12 vertex attribute float3 Position 0 0 0 } vertex main3DPositionNormal { binding 0 12 vertex binding 1 12 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } vertex gbuffer { binding 0 12 vertex binding 1 12 vertex binding 2 8 vertex binding 3 64 instance attribute float3 Position 0 0 0 attribute float3 Normal 1 1 0 attribute float2 UV 2 2 0 attribute float4 InstanceTransform 3 3 0 attribute float4 InstanceTransform 3 4 16 attribute float4 InstanceTransform 3 5 32 attribute float4 InstanceTransform 3 6 48 } } Here we can see some instancing use case, just to show the flexibility of writing this code.The bytes offset could be removed as well.\n3: Use ANY Shader Language The best way to diffuse these augmentation is to change the less possible the shader languate itself.This is because you want to be portable, and when having different platform it can be paramount even to define shaders with different languages into the same file, and switch based on platforms.This is becoming less and less of a need (see HLSL working on Vulkan) but there could be some special cases.Would it be great to fix those special cases by writing platform specific shader fragments without any of your internal rendering code changing ?\nThe choise here is to use a keyword to identify the type of language and then simply write the code in that language.This is ideal to also incorporate code from previous codebases with a small amount of work.\nLet\u0026rsquo;s look at the GBuffer Vertex GLSL code:\nglsl GBuffer_V { #pragma include \u0026quot;Platform.h\u0026quot; layout (location = 0) in vec3 Position; layout (location = 1) in vec3 Normal; layout (location = 2) in vec2 UV; layout (location = 3) in mat4 instanceTransform; layout (std140, binding=0) uniform ViewConstants { mat4 view_projection_matrix; mat4 projection_matrix; vec4 resolution; }; out vec3 vertexNormal; out vec2 uv; out vec3 worldPosition; void main() { vertexNormal = (inverse(transpose((instanceTransform))) * vec4(Normal,0)).rgb; uv = UV; vec4 world_pos = instanceTransform * vec4(Position.xyz, 1.0f); worldPosition = world_pos.xyz; gl_Position = view_projection_matrix * world_pos; } } The only modification I did, and it is sadly necessary in GLSL, is to add the \u0026lsquo;#pragma include\u0026rsquo; custom parsing to add the include in the HFX compiler.\n4: Resource Layouts as First Citizens A new addition of the new APIs, resource layouts are another great factor to take care of.Architecturally they can be implemented in different ways, but I like the idea of having them \u0026lsquo;in your face\u0026rsquo; since the beginning!\nIn the layout section, you can define resources like this:\nlist gbuffer { cbuffer ViewConstants ViewConstants; texture2D albedo; texture2D normals; texture2D metalRoughness; texture2D emissive; texture2D occlusion; sampler2D linear_sampler } The name will be used in the pass section to define which resource list is used.There can be multiple resource lists, normally they should be grouped per frequency (most frequent changes to least frequent ones) and can be separated by a comma for example.A small addition is to use externally specified resource list and code, like for starnest.hfx:\npass main { ... resources = \u0026quot;ShaderToy.Main\u0026quot;\t} This means that the pass named \u0026lsquo;main\u0026rsquo; will simply use the resources defined in \u0026lsquo;shadertoy.hfx\u0026rsquo; - resource list called main.\n5: Permutations This is the most tedious of the tasks, and also one of the most dangerous.Permutations explosion is a well known problem, and there are different ways of tackling this. If you don\u0026rsquo;t have a shader augmentation a good option is to write some scripts to help with generating the code.Otherwise if you have a shader augmentation and you define a \u0026lsquo;shader state\u0026rsquo;, you can define some \u0026lsquo;permutation flags\u0026rsquo;, and just add the defines when you compile shaders. Even in GLSL, you can do some easy string concatenation to add those defines, or use tools like GLSLang + SpirV to help.\nThis becomes a cartesian product of all the permutations/options groups and again can lead to a lot of created shader.I am still investigating the best approach and I will update this article with the results, because I want to include them into HFX but avoid that to become a huge file - and worst to include unused permutations.\nSo stay tuned as I will update this article with the solution I find!\n6: C++ Generated Helpers As finishing touch, there are some informations that can be exposed in a c++ file. \u0026hellip;\nIncluded code: \u0026lsquo;Shader Augmentation\u0026rsquo; The included code has a small application to compile and inspect HFX files. \u0026hellip;\nConclusions I tried to explain the reasons of the different shader augmentations and trying to focus more on the importance of not trying to create a new shading language, but instead empowering it with new informations.\nI can\u0026rsquo;t stress enough how important is to me to have an abstraction that is slightly on top of current shaders API - and create other systems to hide the complexities if needed.\nWith HFX, I would like to expand any language by adding all those features.I wish this could become a used tool by many in their project, and really wish it will be the initial spark.\nNext in line is a revisiting of higher level of rendering, to arrive to explore different rendering techniques with the easiness that the data-driven approach should give.\nAs always please comment, give me feedback, share and enjoy!\nThanks for reading! Gabriel\n","date":1584375097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584375097,"objectID":"d878607d4270e839a6401b74363024f4","permalink":"https://jorenjoestar.github.io/post/shader_augment_for_pipelines/shader_augment_for_pipelines/","publishdate":"2020-03-16T12:11:37-04:00","relpermalink":"/post/shader_augment_for_pipelines/shader_augment_for_pipelines/","section":"post","summary":"Overview In the last articles we looked at progressively building tools to move rendering code towards data.We looked on how to create a simple lexer, a simple parser and a code generator.With those we were able to create a very simple language to augment shaders.\nWhy that ?\nThere are few reasons:\n Shader languages misses any way of linking multiple programs Shader languages misses any way to define render states CGFX and Microsoft FX are mostly dead Ability to use ANY shader language - and just add the infrastructure Ability to generate permutations without manually creating them  Hydra FX aims to add all the missing features and becoming just an augmentation to ANY shader language.","tags":[],"title":"Augmenting shader languages for modern rendering APIs","type":"post"},{"authors":[],"categories":[],"content":"Overview   Model used in the demo.   Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  We finally arrived in the Rendering Pipeline realm.Some can write that it is useless, some can hate it.Many have some sort of abstraction for it since ages, and others have to now that new APIs like Vulkan and DX12 have it as an explicit part of their design (finally!).\nAfter we built a basic Material System in the previous article (https://jorenjoestar.github.io/post/writing_shader_effect_language_3/) we can add another layer on top of it and built a complete Rendering Frame.\nIn this article I will talk about a simplified version of Render Graph that I call Render Pipeline and came into my mind in the canteen of Codemasters after thinking:\n What is the biggest dependency in Rendering ?\n The answer is simple:\n Render Targets!\n Render Targets or Frame Buffers is just an intermediate buffer in which we can draw something and use it later.Basically a Read/Write texture!It is not easy to shuffle around a Render Target, and having knowledge of which one are you using can make a huge difference for your rendering tech.Textures and Render Targets are the biggest memory lord in any rendering application, thus knowing where you are spending your memory can be really powerful.\nFrom a pure understanding of rendering techniques, having a clear visualization of this aspect makes a HUGE difference!\nOnce I started using to describe a frame of rendering with the Render Target Dependencies I never looked back.As always, knowledge is power.Render Pipeline Thinking First of all, let\u0026rsquo;s start defining some general concepts to describe the problem we are trying to solve.The problem we are trying to solve is:\n How to describe the inter-frame dependencies of Render Targets in a frame ?\n The dependencies are who writes and/or read from/to a Render Target.That is exactly what is described in a Render Pipeline. Enter the Render Pipeline.\n A Render Pipeline is a list of Passes that read and writes Render Targets. That\u0026rsquo;s it.Done! See you next article!Of course I am kidding - but this is the gist of it.The implications, however, are profound.Next logical question is:\n How can we read and write from/to a Render Target ?\n Let\u0026rsquo;s list how we can write to a Render Target\n Graphics - binding some geometry, render states and Render Targets Compute - write anything to the Render Target  Even a so called \u0026lsquo;post-process\u0026rsquo; is just a fullscreen triangle with a shader.\nAnd to read\u0026hellip;well any shader that takes reads a texture!\nIt is incredible to think that with this simple building blocks you can describe almost everything to render!\nFor example, let\u0026rsquo;s try to express some common rendering techniques using only those concepts.\nDeferred Rendering We can define the following simple steps:\n Meshes uses their materials (shaders + textures + constants) as input and write into GBuffer Render Target + depth. A Compute/Post-process shader will read the Gbuffer Render Target and depth (to reconstruct the pixel position), a light list of some sort and outputs a texture with the result. Transparent objects are drawn into this new Render Target using their materials. And so on\u0026hellip;  Exponential Variance Shadow Mapping in a Forward Rendering Pipeline  Meshes writes into a depth-only render target using the light as \u0026lsquo;camera/point of view\u0026rsquo;. Compute or Postprocess converts the depth-only render target into a EVSM one. Meshes uses their materials and the EVSM shadow map to render into a \u0026lsquo;main\u0026rsquo; Render Target.  \u0026lt;diagram 2 goes here\u0026gt;\nOther Rendering Concepts To give a full description of the frame we need to add other concepts that will help us.These are the less strict ones - and just a personal way of seeing things.\nRender View The concept of \u0026lsquo;Render View\u0026rsquo; is just a way or representing a camera and a list of visible objects from it.We will see how we use it later, but a simple example of Render View would be the \u0026lsquo;Sun Shadow\u0026rsquo; render view - representing the sun (as a camera) and a list of visible objects from it.The \u0026lsquo;Main\u0026rsquo; render view of course represent the main camera and visible objects.This, combined with render managers becomes a powerful combination to describe what needs to be rendered.\nRender Manager If you think from an ECS mentality, this would be a \u0026lsquo;system\u0026rsquo;.Each render manager is responsible to render one or more render \u0026lsquo;aspects/entities\u0026rsquo; into a Render Pass.A render manager can subscribe to any \u0026lsquo;graphics\u0026rsquo; pass and render from there.\nFor example, a \u0026lsquo;static geometry\u0026rsquo; render manager could setup an instancing buffer for the gbuffer-generation pass and draw all objects.\nRender Pipeline Implementation After we defined the basic concepts let\u0026rsquo;s see an actual implementation of the Render Pipeline.We will see the code of each component and arrive at the actual data definition (in json).\nThe code has changed a bit since last article, with the inclusion of CGLM as math library and other high-level rendering code, included in hydra_rendering.h/.cpp.\nRender View First element is the Render View:\n//\r// Render view is a 'contextualized' camera - a way of using the camera in the render pipeline.\r//\rstruct RenderView {\rCamera camera;\rarray( RenderScene ) visible_render_scenes;\r}; // struct RenderView\rUsing STB\u0026rsquo;s array (the macro is just an aid to know it is not just a pointer) we have a list of visible render scenes from that camera.It should be pretty straighforward.\nRender Manager Next is Render Manager:\n//\rstruct RenderManager {\rstruct RenderContext {\rDevice* device;\rconst RenderView* render_view;\rCommandBuffer* commands;\rRenderScene* render_scene_array;\ruint16_t start;\ruint16_t count;\ruint16_t stage_index;\r}; // struct RenderContext\rvirtual void render( RenderContext\u0026amp; render_context ) = 0;\r}; // struct RenderManager\rThe base class is really just a \u0026lsquo;render\u0026rsquo; method.Here the RenderContext is interesting, and it gives access to all you need to render:\n Device - used to map/unmap resources. RenderView - access to camera (and more, but that\u0026rsquo;s for the next article!). CommandBuffer - the actual draw commands are written here. RenderScene - the RenderScene from start to start + count.  In this very simple demo, we have just 2 render managers: Line Renderer and Scene Renderer.The most interesting one is the second: Line Renderer has commands to draw lines that will be mapped into a GPU buffer and uses instancing to draw them.\n\rvoid LineRenderer::render( RenderContext\u0026amp; render_context ) {\rDevice\u0026amp; device = *render_context.device;\r// Update camera matrix\rconst Camera\u0026amp; camera = render_context.render_view-\u0026gt;camera;\rMapBufferParameters cb_map = { lines_cb, 0, 0 };\rfloat L = 0, T = 0;\rfloat R = device.swapchain_width, B = device.swapchain_height;\rconst float ortho_projection[4][4] =\r{\r{ 2.0f / ( R - L ), 0.0f, 0.0f, 0.0f },\r{ 0.0f, 2.0f / ( T - B ), 0.0f, 0.0f },\r{ 0.0f, 0.0f, -1.0f, 0.0f },\r{ ( R + L ) / ( L - R ), ( T + B ) / ( B - T ), 0.0f, 1.0f },\r};\rLocalConstants* cb_data = (LocalConstants*)device.map_buffer( cb_map );\rif ( cb_data ) {\rcb_data-\u0026gt;view_projection = camera.view_projection;\rmemcpy( \u0026amp;cb_data-\u0026gt;projection, \u0026amp;ortho_projection, 64 );\rcb_data-\u0026gt;resolution = { device.swapchain_width * 1.0f, device.swapchain_height * 1.0f, 1.0f / device.swapchain_width, 1.0f / device.swapchain_height };\rdevice.unmap_buffer( cb_map );\r}\rif ( current_line_index ) {\rconst uint32_t mapping_size = sizeof( LinVertex ) * current_line_index;\rMapBufferParameters map_parameters_vb = { lines_vb, 0, mapping_size };\rLinVertex* vtx_dst = (LinVertex*)device.map_buffer( map_parameters_vb );\rif ( vtx_dst ) {\rmemcpy( vtx_dst, \u0026amp;s_line_buffer[0], mapping_size );\rdevice.unmap_buffer( map_parameters_vb );\r}\rCommandBuffer* commands = render_context.commands;\rcommands-\u0026gt;begin_submit( 2 );\rShaderInstance\u0026amp; shader_instance = line_material-\u0026gt;shader_instances[3];\rcommands-\u0026gt;bind_pipeline( shader_instance.pipeline );\rcommands-\u0026gt;bind_resource_list( shader_instance.resource_lists, shader_instance.num_resource_lists, nullptr, 0 );\rcommands-\u0026gt;bind_vertex_buffer( lines_vb, 0, 0 );\r// Draw using instancing and 6 vertices.\rconst uint32_t num_vertices = 6;\rcommands-\u0026gt;draw( TopologyType::Triangle, 0, num_vertices, current_line_index / 2 );\rcommands-\u0026gt;end_submit();\rcurrent_line_index = 0;\r}\r}\rEasy to notice how, with a Vulkan/DX12 interface, there are few less commands to write. Binding a pipeline sets everything considered \u0026lsquo;static\u0026rsquo; - render states, shaders - and with just resource lists (that sets textures and constants) and vertex/index buffers we have everything needed to render.\nNOTE: HFX has gone some improvements and now supports render states and vertex declarations/formats. I\u0026rsquo;ll write about it in the next post - but this has become crucial.\nShader Resources Management This is another personal preference - but not necessary at all.Two concepts are really useful to me to be explicit and centralized: resources and bindings.\nResources are all referenced in a \u0026lsquo;Shader Resource Database\u0026rsquo;:\n//\r// Struct used to retrieve textures, buffers and samplers.\r//\rstruct ShaderResourcesDatabase {\rstruct BufferStringMap {\rchar* key;\rBufferHandle value;\r}; // struct BufferStringMap\rstruct TextureStringMap {\rchar* key;\rTextureHandle value;\r}; // struct TextureStringMap\rstruct SamplerStringMap {\rchar* key;\rSamplerHandle value;\r}; // struct SamplerStringMap\rBufferStringMap* name_to_buffer = nullptr;\rTextureStringMap* name_to_texture = nullptr;\rSamplerStringMap* name_to_sampler = nullptr;\rvoid init();\rvoid terminate();\rvoid register_buffer( char* name, BufferHandle buffer );\rvoid register_texture( char* name, TextureHandle texture );\rvoid register_sampler( char* name, SamplerHandle sampler );\rBufferHandle find_buffer( char* name );\rTextureHandle find_texture( char* name );\rSamplerHandle find_sampler( char* name );\r}; // struct ShaderResourcesDatabase\rSimply put, any resource used by rendering is here.Both Materials, Pipelines and Render Managers register and use the database to create the resource lists used in rendering.\nNext and more convoluted is the shader resources lookup class:\n//\r// Struct to link between a Shader Binding Name and a Resource. Used both in Pipelines and Materials.\r//\rstruct ShaderResourcesLookup {\renum Specialization {\rFrame, Pass, View, Shader\r}; // enum Specialization\rstruct NameMap {\rchar* key;\rchar* value;\r}; // struct NameMap\rstruct SpecializationMap {\rchar* key;\rSpecialization value;\r}; // struct SpecializationMap\rNameMap* binding_to_resource = nullptr;\rSpecializationMap* binding_to_specialization = nullptr;\rNameMap* binding_to_sampler = nullptr;\rvoid init();\rvoid terminate();\rvoid add_binding_to_resource( char* binding, char* resource );\rvoid add_binding_to_specialization( char* binding, Specialization specialization );\rvoid add_binding_to_sampler( char* binding, char* sampler );\rchar* find_resource( char* binding );\rSpecialization find_specialization( char* binding );\rchar* find_sampler( char* binding );\rvoid specialize( char* pass, char* view, ShaderResourcesLookup\u0026amp; final_lookup );\r}; // struct ShaderResourcesLookup\rThis class specify the binding between a shader resource and an actual resource.As a simple example to clarify, a shader could have an \u0026lsquo;albedo\u0026rsquo; texture defined in the code, but the actual texture is defined by the material.Or for a Render Stage, like a Post-Processing one, its input could be defined in the shader code as \u0026lsquo;input 0, input 1\u0026hellip;' and the render pipeline creates the binding.\nWith those in place, we can finalize any resource used by any shader/material/pipeline.\nThe actual usage is into the Shader Instance class. Let\u0026rsquo;s have a quick look.\n//\rstruct ShaderInstance {\rvoid load_resources( const PipelineCreation\u0026amp; pipeline, PipelineHandle pipeline_handle, ShaderResourcesDatabase\u0026amp; database, ShaderResourcesLookup\u0026amp; lookup, Device\u0026amp; device );\rPipelineHandle pipeline;\rResourceListHandle resource_lists[k_max_resource_layouts];\ruint32_t num_resource_lists;\r}; // struct ShaderInstance\rThis class is what actually contains the resource lists and pipeline used to render anything.Not very happy with the name - any suggestion welcome.A material contains a list of those - one for each pass - and is used to draw.Again with the new Vulkan/DX12 mentality, Pipeline + Resource Lists + Geometry is all you need to render almost.\nThe magic happens when creating the resource lists:\nvoid ShaderInstance::load_resources( const PipelineCreation\u0026amp; pipeline_creation, PipelineHandle pipeline_handle, ShaderResourcesDatabase\u0026amp; database, ShaderResourcesLookup\u0026amp; lookup, Device\u0026amp; device ) {\rusing namespace hydra::graphics;\rResourceListCreation::Resource resources_handles[k_max_resources_per_list];\rfor ( uint32_t l = 0; l \u0026lt; pipeline_creation.num_active_layouts; ++l ) {\r// Get resource layout description\rResourceListLayoutDescription layout;\rdevice.query_resource_list_layout( pipeline_creation.resource_list_layout[l], layout );\rWe know that a pipeline can have 1 or more resource lists, thus we just iterate through them.Next we look into each resource of the current list:\n\r// For each resource\rfor ( uint32_t r = 0; r \u0026lt; layout.num_active_bindings; r++ ) {\rconst ResourceBinding\u0026amp; binding = layout.bindings[r];\r// Find resource name\r// Copy string_buffer char* resource_name = lookup.find_resource( (char*)binding.name );\rswitch ( binding.type ) {\rcase hydra::graphics::ResourceType::Constants:\rcase hydra::graphics::ResourceType::Buffer:\r{\rBufferHandle handle = resource_name ? database.find_buffer( resource_name ) : device.get_dummy_constant_buffer();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\r... same for textures\rFor each binding coming from the shader (think \u0026lsquo;albedo\u0026rsquo; for a PBR shader) we search for the actual resource name (\u0026lsquo;WoodBeamAlbedo\u0026rsquo;) and query the database to find it.After we did that, we can create the list:\n }\r}\rResourceListCreation creation = { pipeline_creation.resource_list_layout[l], resources_handles, layout.num_active_bindings };\rresource_lists[l] = device.create_resource_list( creation );\r}\rnum_resource_lists = pipeline_creation.num_active_layouts;\rpipeline = pipeline_handle;\r}\rWith this mechanism we added another explicit connection between resources.\nIt is finally time to see the actual render pipeline!\nRender Stage/Pass This is the CORE of everything, and it must work with all both geometrical stages and post-process ones.You can either create a base virtual class or doing something like here.Important is understanding the concept!\n//\r// Encapsulate the rendering of anything that writes to one or more Render Targets.\r//\rstruct RenderStage {\renum Type {\rGeometry, Post, PostCompute, Swapchain, Count\r};\rType type = Count;\rSimply we define the types:\n Geometry - uses render manager with meshes to draw. Post - fullscreen triangle + shader. PostCompute - any compute shader execution basically! Swapchain - special case of binding the window framebuffer and render the last time.  Next is the most important part: dependencies!\n array( TextureHandle ) input_textures = nullptr;\rarray( TextureHandle ) output_textures = nullptr;\rTextureHandle depth_texture;\rWhen we create the pipeline, we save all inputs and outputs textures.Depth/Stencil is a put in its own part.\n float scale_x = 1.0f;\rfloat scale_y = 1.0f;\ruint16_t current_width = 1;\ruint16_t current_height = 1;\rHere we handle scaling. When using scale, we use the framebuffer\u0026rsquo;s window width/height to calculate the Render Target size of the output ones. When using the current width/height we instead define a specific size (like for a shadow map).\n RenderPassHandle render_pass;\rhydra::graphics low level rendering needs this handle to actually handle the drawing.\n Material* material = nullptr;\ruint8_t pass_index = 0;\rThis is for PostProcesses : material and pass index to retrieve the \u0026lsquo;shader instance\u0026rsquo; containing the pipeline and the resource lists.\n RenderView* render_view = nullptr;\rRenderView used by this stage.For example the \u0026lsquo;Sun Shadow Render Stage\u0026rsquo; will use the \u0026lsquo;Shadow Render View\u0026rsquo; to dispatch all its objects to each render manager.\n float clear_color[4];\rfloat clear_depth_value;\ruint8_t clear_stencil_value;\ruint8_t clear_rt : 1;\ruint8_t clear_depth : 1;\ruint8_t clear_stencil : 1;\ruint8_t resize_output : 1;\ruint8_t pad : 4;\rIf the stage needs to clear its output(s), these will tell what to do.\n uint64_t geometry_stage_mask; // Used to send render objects to the proper stage. Not used by compute or postprocess stages.\rThis creates a link between render managers and stages.An object is rendered only if its stage mask equals at least one stage.Why that ? Because when defining a render view, we have a list of objects visible from that camera, and we need a way of dispatching those objects to their respective managers.\nFor example a \u0026lsquo;dynamic render object\u0026rsquo; could have appear both on the gbuffer pass and an \u0026lsquo;object special effect\u0026rsquo; pass - both visible from the main camera.\nThis ideas comes from the AMAZING talk by Bungie:\nhttp://advances.realtimerendering.com/destiny/gdc_2015/Tatarchuk_GDC_2015__Destiny_Renderer_web.pdf\nA render manager is what they call a feature renderer - named differently because this version is much more basic!\n array( RenderManager* ) render_managers;\rRender Managers can register to stages even if they don\u0026rsquo;t have objects, for example a \u0026lsquo;Lighting Manager\u0026rsquo; would want to submit a list of visible light in a certain pass.\n // Interface\rvirtual void init();\rvirtual void terminate();\rvirtual void begin( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void render( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void end( Device\u0026amp; device, CommandBuffer* commands );\rvirtual void load_resources( ShaderResourcesDatabase\u0026amp; db, Device\u0026amp; device );\rvirtual void resize( uint16_t width, uint16_t height, Device\u0026amp; device );\rvoid register_render_manager( RenderManager* manager );\r}; // struct RenderStage\rThis is the final interface.Load resources is used for PostProcesses - they have a material and need to load its resources.\nRender Pipeline We arrived at the last piece of the puzzle!\n//\r// A full frame of rendering using RenderStages.\r//\rstruct RenderPipeline {\rstruct StageMap {\rchar* key;\rRenderStage* value;\r};\rstruct TextureMap {\rchar* key;\rTextureHandle value;\r};\rvoid init( ShaderResourcesDatabase* initial_db );\rvoid terminate( Device\u0026amp; device );\rvoid update();\rvoid render( Device\u0026amp; device, CommandBuffer* commands );\rvoid load_resources( Device\u0026amp; device );\rvoid resize( uint16_t width, uint16_t height, Device\u0026amp; device );\rStageMap* name_to_stage = nullptr;\rTextureMap* name_to_texture = nullptr;\rShaderResourcesDatabase resource_database;\rShaderResourcesLookup resource_lookup;\r}; // struct RenderPipeline\rThis is literally IT!This class contains all the stages and resources needed to render.Most of the time it will just iterate over the stages and execute something per stage.\nResource database contains all the resources used actually - and the lookup instead is only for the PostProcess stages.Render Pipeline Description We really have all the part to render a frame!Let\u0026rsquo;s look at the data defining the pipeline.We will define a simple-silly-non-effective PBR deferred rendering.Probably the worst shaders you saw, but it will still work.\nFirst we define the Render Targets:\n{\r\u0026quot;name\u0026quot;: \u0026quot;PBR_Deferred\u0026quot;,\r\u0026quot;RenderTargets\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferAlbedo\u0026quot;,\r\u0026quot;format\u0026quot;: \u0026quot;R8G8B8A8_UNORM\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferNormals\u0026quot;,\r\u0026quot;format\u0026quot;: \u0026quot;R16G16B16A16_SNORM\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferProperties0\u0026quot;,\r\u0026quot;format\u0026quot;: \u0026quot;R8G8B8A8_UNORM\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;MainDepth\u0026quot;,\r\u0026quot;format\u0026quot;: \u0026quot;D24_UNORM_S8_UINT\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;BackBufferColor\u0026quot;,\r\u0026quot;format\u0026quot;: \u0026quot;R16G16B16A16_FLOAT\u0026quot;\r}\r],\rby default they will have the same size as the window framebuffer, unless otherwise written (scale_x/y, width/height).\nNext are the actual render stages.The first is the GBufferOpaque one:\n \u0026quot;RenderStages\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferOpaque\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;Geometry\u0026quot;,\r\u0026quot;render_view\u0026quot;: \u0026quot;main\u0026quot;,\r\u0026quot;depth_stencil\u0026quot;: \u0026quot;Main\u0026quot;,\r\u0026quot;inputs\u0026quot;: [\r],\r\u0026quot;outputs\u0026quot;: {\r\u0026quot;rts\u0026quot;: [ \u0026quot;GBufferAlbedo\u0026quot;, \u0026quot;GBufferNormals\u0026quot;, \u0026quot;GBufferProperties0\u0026quot; ],\r\u0026quot;depth\u0026quot;: \u0026quot;MainDepth\u0026quot;,\r\u0026quot;flags\u0026quot;: \u0026quot;Common\u0026quot;,\r\u0026quot;clear_color\u0026quot;: \u0026quot;000000ff\u0026quot;,\r\u0026quot;clear_depth\u0026quot;: 1.0,\r\u0026quot;clear_stencil\u0026quot;: 0\r}\r},\rAs you see it outputs to 3 Render Targets + Depth.It also specify clear color, depth and stencil.\nNext is the silliest compute shader to calculate light:\n {\r\u0026quot;name\u0026quot;: \u0026quot;DeferredLights\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;PostCompute\u0026quot;,\r\u0026quot;material_name\u0026quot;: \u0026quot;SimpleFullscreen\u0026quot;,\r\u0026quot;material_pass_index\u0026quot;: 2,\r\u0026quot;inputs\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferAlbedo\u0026quot;,\r\u0026quot;sampler\u0026quot;: \u0026quot;Point\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;gbuffer_albedo\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferNormals\u0026quot;,\r\u0026quot;sampler\u0026quot;: \u0026quot;Point\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;gbuffer_normals\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;GBufferProperties0\u0026quot;,\r\u0026quot;sampler\u0026quot;: \u0026quot;Point\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;gbuffer_properties0\u0026quot;\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;MainDepth\u0026quot;,\r\u0026quot;sampler\u0026quot;: \u0026quot;Point\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;depth_texture\u0026quot;\r}\r],\r\u0026quot;outputs\u0026quot;: {\r\u0026quot;images\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;BackBufferColor\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;destination_texture\u0026quot;\r}\r],\r\u0026quot;flags\u0026quot;: \u0026quot;Common\u0026quot;\r}\r},\rIt will read all the previously generated textures and run a compute shader to calculate the final lighting.Worth noting \u0026lsquo;material\u0026rsquo; and \u0026lsquo;material pass index\u0026rsquo; - to retrieve the shader from the material. If you open SimpleFullscreen.hfx and go to the third defined pass, you will see the code.\nNext is an example of reusing a Render Target to add informations (like transparent objects).It will add debug rendering on top of the other objects and write in the BackBufferColor render target.The absence of clear parameters dictates that we don\u0026rsquo;t want to clear.\n {\r\u0026quot;name\u0026quot;: \u0026quot;DebugRendering\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;Geometry\u0026quot;,\r\u0026quot;render_view\u0026quot;: \u0026quot;main\u0026quot;,\r\u0026quot;inputs\u0026quot;: [\r],\r\u0026quot;outputs\u0026quot;: {\r\u0026quot;rts\u0026quot;: [ \u0026quot;BackBufferColor\u0026quot; ],\r\u0026quot;depth\u0026quot;: \u0026quot;MainDepth\u0026quot;,\r\u0026quot;flags\u0026quot;: \u0026quot;Common\u0026quot;\r}\r},\rLast step is the swapchain.It is simply using a simple shader to write to the window framebuffer as the last step of the frame.\n {\r\u0026quot;name\u0026quot;: \u0026quot;Swapchain\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;Swapchain\u0026quot;,\r\u0026quot;mask\u0026quot;: \u0026quot;FRAMEBUFFER\u0026quot;,\r\u0026quot;material_name\u0026quot;: \u0026quot;Swapchain\u0026quot;,\r\u0026quot;render_view\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;depth_stencil\u0026quot;: \u0026quot;Post\u0026quot;,\r\u0026quot;inputs\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;BackBufferColor\u0026quot;,\r\u0026quot;sampler\u0026quot;: \u0026quot;Point\u0026quot;,\r\u0026quot;binding\u0026quot;: \u0026quot;input_texture\u0026quot;\r}\r],\r\u0026quot;outputs\u0026quot;: {\r\u0026quot;rts\u0026quot;: [\r],\r\u0026quot;depth\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;flags\u0026quot;: \u0026quot;Common\u0026quot;,\r\u0026quot;clear_color\u0026quot;: \u0026quot;000000ff\u0026quot;\r}\r}\r]\r}\rVisualization With all this defined, we can arrive to have something incredibly useful as this (included in the demo!):\n  Render Pipeline   To me this is the quintessence of rendering: visualization.Seeing things helps me understanding much better.Debugging broken features, studying features, understanding dependencies, shuffling things around becomes MUCH easier.\nDemo and code The demo loads a model, apply a silly directional light and gives you some controls, and uses the render pipeline.It was setup during the night just to show something usable, but it is far from ideal!\nIn the code provided there is everything I am talking here.And now some links to libraries/resources used.\n3 models are included from the free GLTF library: https://github.com/KhronosGroup/glTF-Sample-Models\nTinyGLTF by Syoyo Fujita:\nhttps://github.com/syoyo/tinygltf\nThe always present-always amazing ImGui by Omar: https://github.com/ocornut/imgui\nWith the NodeEditor by MichaÅ‚ CichoÅ„: https://github.com/thedmd/imgui-node-editor\nFor the PBR rendering, kudos to the GREAT INFORMATIONS from Google Filament and Romain Guy.\nLastly, this is not anywhere near production ready, but I am still happy to share it as a knowledge building block for others.I am thinking of making some videos for this - if you are interested let me know (both in English and Italian).\nConclusions We arrived at defining the Render Pipeline - a way of describing how a frame is rendered.It is a very simplified version of the RenderGraph/FrameGraph - as seen in many talks - and this is something I\u0026rsquo;ve used in my home projects (and current indie game) with great success.No mention of adding resource barriers, sharing memory, async compute and more.The whole purpose of this article was instead to focus on the more high level architecture side.\nWhat is next ?\nI would write about the improvements on the HFX shader effect and would like to cleanup and make that library more robust.Then there is the Vulkan backend to be wrote and many examples to be done. Examples could be amazing to be tutorial and develop the technology more.Then there is talking deeper about dispatching rendering draws, render managers and such - another interesting and very unique subject in Rendering Engine architectures. In all the companies I\u0026rsquo;ve worked, I always found completely different solutions!\nPlease comment, share, send feedback! I am happy to answer any question and very happy to share this article. Thanks for reading!\nGabriel\n","date":1571064229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582383529,"objectID":"7c26dedfc7a8e73a1db210a4acccdded","permalink":"https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/","publishdate":"2019-10-14T10:43:49-04:00","relpermalink":"/post/data_driven_rendering_pipeline/","section":"post","summary":"Overview   Model used in the demo.   Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  We finally arrived in the Rendering Pipeline realm.Some can write that it is useless, some can hate it.Many have some sort of abstraction for it since ages, and others have to now that new APIs like Vulkan and DX12 have it as an explicit part of their design (finally!).\nAfter we built a basic Material System in the previous article (https://jorenjoestar.","tags":[],"title":"Data Driven Rendering: Pipelines","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In Part 2 of this series we added Resource Layouts and Properties to the HFX language, trying to arrive at a point in which we can describe the rendering of a Shader Effect almost entirely.In this article I would like to explore further adds to HFX, especially a proper Material System to be used in conjunction with the HFX language.I also separated the code a little bit for clarity and added the usage of STB array and hash maps.With that I would like to develop a Material System that is robust and easy to use, even though I am (DISCLAIMER!) far from it!I will first talk about the theory and thoughts behind those changes, and then go through the code changes and addition.\nMaterial System thoughts Following a nomenclature from the amazing guys at our_machinery, we are adding a Tier 1 Shader System - something that builds on top of the graphics API created in the previous article.\nFirst of all, a great series of article is again on their website:\n https://ourmachinery.com/post/the-machinery-shader-system-part-1/ https://ourmachinery.com/post/the-machinery-shader-system-part-2/ https://ourmachinery.com/post/the-machinery-shader-system-part-3/  We are building a Material System based on a graphics-API that exposes the following concepts:\n Buffer Texture Pipeline (that includes shaders) Render Pass Resource List Layout Resource List  We are using a Vulkan/D3D12 interface here, and these concepts map 1 to 1 with that.One of the big changes from a typical low-level graphics API is both the \u0026lsquo;missing\u0026rsquo; concept of Shader as a resource, and the addition of Render Pass as resource.A new concept is the one of Resource List Layout and Resource List.The Vulkan names are Descriptor Set Layout and Descriptor Set, but even though they reflect more the underlying driver nature of the term, I changed to Resource List just to have it clearer as a concept.\nThe King here is the Pipeline: it is a structure that contains all the immutable data of a pipeline. That includes our missing shaders, all the render states (DepthStencil, AlphaBlend, \u0026hellip;) and a Layout of the resources to be used by the shader.\nPart of the dynamic pipeline states are normally the geometry and the resource lists.Note that I am using the plural here: each pipeline can have 1 or more resource lists!!.This is a good to organize your resources based on update frequencies, something coming from the numerous talks about Approaching Zero Driver Overhead.\nRemembering the simple interface of our API, now we have the following:\nstruct Device {\r...\rBufferHandle create_buffer( const BufferCreation\u0026amp; creation );\rTextureHandle create_texture( const TextureCreation\u0026amp; creation );\rPipelineHandle create_pipeline( const PipelineCreation\u0026amp; creation );\rSamplerHandle create_sampler( const SamplerCreation\u0026amp; creation );\rResourceListLayoutHandle create_resource_list_layout( const ResourceListLayoutCreation\u0026amp; creation );\rResourceListHandle create_resource_list( const ResourceListCreation\u0026amp; creation );\rRenderPassHandle create_render_pass( const RenderPassCreation\u0026amp; creation );\r...\r};\rIf you look at the OpenGL implementation (the only I wrote for now :( ) you will find that Shaders are considered resources, but it is more for convenience of the attach/linking OpenGL need than anything else.\nLet\u0026rsquo;s finally introduce the new concept for this article!\nShader Effect  A Shader Effect is the blueprint of static data needed to draw something on the screen.\n It needs to include shaders (included in a Pipeline Description), properties (coming from the HFX file) and find its location into a render graph. A Shader Effect is 1 to 1 with a Binary HFX file.\n As a convenience we will add also informations about the local constants.When creating a Shader Effect, we can define properties, and we put all the numerical properties into one buffer.\nThis is the current code:\n\rstruct ShaderEffect {\r//\r//\rstruct PropertiesMap {\rchar* key;\rhfx::ShaderEffectFile::MaterialProperty* value;\r}; // struct PropertiesMap\rstruct Pass {\rPipelineCreation pipeline_creation;\rchar name[32];\rPipelineHandle pipeline_handle;\ruint32_t pool_id;\r}; // struct Pass\rPass* passes;\ruint16_t num_passes = 0;\ruint16_t num_properties = 0;\ruint32_t local_constants_size = 0;\rchar* local_constants_default_data = nullptr;\rchar* properties_data = nullptr;\rPropertiesMap* name_to_property = nullptr;\rchar name[32];\rchar pipeline_name[32];\ruint32_t pool_id;\r}; // struct ShaderEffect\rYou see both a pipeline name and an array of passes with a name. These are to insert the pass into a very primordial render graph, that I wrote just because I didn\u0026rsquo;t want to hardcode the frame structure, especially because next article will be EXACTLY on this topic!\nHaving defined the Shader Effect, we can now move into the next big actor.\nMaterial  A Material is an instance of a Shader Effect.\n Given a HFX file, we generate a new file (HMT, Hydra Material) that will contain all the informations.The concept of Material is really unique values for the properties of a Shader Effect.\nThat is basically it.\nFor example, if a shader contains a property like an albedo texture, the material answer the question \u0026ldquo;which albedo texture?\u0026quot;.This is done for every property.\nLet\u0026rsquo;s have a look at our new material:\n{\r\u0026quot;name\u0026quot;: \u0026quot;SimpleFullscreen\u0026quot;,\r\u0026quot;effect_path\u0026quot;: \u0026quot;SimpleFullscreen.hfx\u0026quot;,\r\u0026quot;properties\u0026quot;: [\r{\r\u0026quot;scale\u0026quot;: 16.0,\r\u0026quot;albedo\u0026quot;: \u0026quot;AngeloCensorship.png\u0026quot;,\r\u0026quot;modulo\u0026quot;: 2.0\r}\r],\r\u0026quot;bindings\u0026quot;: [\r{\r\u0026quot;LocalConstants\u0026quot;: \u0026quot;LocalConstants\u0026quot;,\r\u0026quot;destination_texture\u0026quot;: \u0026quot;compute_output_texture\u0026quot;,\r\u0026quot;input_texture\u0026quot;: \u0026quot;compute_output_texture\u0026quot;,\r\u0026quot;albedo_texture\u0026quot;: \u0026quot;albedo\u0026quot;\r}\r]\r}\rAs you can see there is a name, the effect path, the properties and the bindings. These will be explained in the next section.Properties are just a name-value list, coming from the Shader Effect itself (the .bhfx file).\nThe texture is my horrible drawing after reading the fantastic rendering guide by Angelo Pesce and how he censored the parts that were internal to Roblox!\nShader Resource Database and Lookup A concept that I saw only in the our_machinery posts, but I personally adopted since a couple of years, is a way of automating a daunting task: setting shader resources.\nI still need to finish the correct implementation of those, but the concepts are simple.A Shader Resource Database is a database of resources that can be searched using a Shader Resources Lookup.The name of the binding is the shader related name, while the value is the name into the database.Of course you can use hashes instead of names, and compile them into a binary version of this, but this is not important now.\nOne interesting bit (sadly not implemented here, sorry!) is the binding specialization. This is done so that resources can be specialized in the database.This is done per pass and it let you write only one binding list for all the Material, and then gather the proper resource based on the specialization.For example if there is a binding for a pass-dependent resource, writing a generic version can specialize the shader pass correctly. Or using special keywords in the bindings, you can retrieve input/output textures from the render pass in which the shader is rendered!\nFor now though it is more a manual written list, but it will be developed further.\nWhere is my code ? Having introduced the new concept, let\u0026rsquo;s look at the changes that happened in the last weeks of night coding.As said before, in general I separated the code in header/cpp for clarity and building performances (after a good talk on Twitter, https://twitter.com/GabrielSassone/status/1179810419617275905?s=20).\nApplications First big changes was separating the code from the previous articles in an application: namely CustomShaderLanguageApplication in CustomShaderLanguage.h/cpp and MaterialSystemApplication in MaterialSystem.h/cpp.\nThe first contains all the application code that uses HDF and HFX, with code generation and HFX compilation.The second contains both the new Material System and the application that uses it.I would love to say that is an usable app, but I really touched my limits in non designing clearly when night coding.Personal note: I hope this could be the spark to create a FX Composer successor, open source and free for all!\nSTB As part of this experiment I wanted to try something different.Instead of re-writing array and hash maps with templates, I wanted to give a try to the STB libraries: namely stb_ds.h and stb_image.h.Arrays and Hash Maps are now included in hydra_lib.h to be used across the code.\nHydra Graphics The device added render passes and the support for multiple resources layout.It also creates FBOs for color passes and supports resize, especially thanks to the Render Pipeline.\nPrimitive Render Graph (called Render Pipeline) I use the term I used since the inception in 2010, and honestly it is more true to what it does.It is not a graph but more a list of Render Stages with input/outputs defined clearly.In the next article I will develop more on this, but for now I needed some structure like this to be explicit.\nIn the application there are 3 pipelines, one for a single pass ShaderToy shader, one for a silly compute to framebuffer shader(that for now loads a texture and outputs it to the framebuffer), and one for just a render to window.\nI use this in my indie project, with a fully custom and data driven (written in json) pipeline that includes compute deferred lighting and shadows, shadow passes, various post-process passes and such, everything very easy to debug and very easy to modify/add/delete.There is a mechanism to send the correct draw calls to the correct pass through the usage of render systems, but again this will be a topic for the next article!\nIn the included code, there is also a small but powerful tool: a pipeline explorer.For now it will just show the render targets for each stage, and in these simple examples does not matter much.In the next article we will dive deep into the Render Pipeline/Graph subject and then all of this will make sense!\nBreak: a simple Resource Manager While being a very important topic, this is not the focus of this article.Anyway I wanted a Resource Manager that would be helpful to handle resource creation and loading.This includes also resource compilation, something that normally happens at build time, but in our exercise can be triggered at run-time.\nThe resource manager is a class that simply manages resources using factories and manages dependencies between resources.We have only 3 resources for now:\n Textures Shader Effects Materials  Resources A resource is a class that both has data and let the dependency with other data be clear.The resource\u0026rsquo;s data is actually a pointer to actual raw data used by other systems, in this case rendering.Let\u0026rsquo;s see its definition:\nstruct Resource {\rstruct ResourceReference {\ruint8_t type;\rchar path[255];\r}; // struct ResourceReference\rstruct Header {\rchar header[7];\ruint8_t type; // ResourceType::enum\rsize_t data_size;\ruint16_t num_external_references;\ruint16_t num_internal_references;\r}; // struct Header\rstruct ResourceMap {\rchar* key;\rResource* value;\r};\rHeader* header;\rchar* data;\rvoid* asset;\rResource::ResourceReference* external_references;\r// External\rResourceMap* name_to_external_resources;\r// Interal\r}; // struct Resource\rA resource is loaded from a binary file and contains a header and some data coming from the file, and an asset containing a system specific pointer.\nWe added 3 system specific resources (Texture, Shader Effect and Material) but the class handled is always resource.To access the system specific data, asset member is used.\nA resource contains also a map to the external resources loaded within it - to handle external references.\nCompilation Starting from a source file (.hfx, .png, .hmt) using the specific factory, the resource manager compiles the code to a binary resource.This means both converting the source format to a binary representation but also adding external dependencies to the file.These dependencies will be loaded when loading the resource, and before it.\nLoading Loading happens by first loading all the dependent resources and then using the specific factory to load the system specific asset.This is a very semplicistic resource management - synchronous only, single threaded, not optimized - so really was an exercise in having something running for both compiling a resource and managing dependencies.The whole point is the separation between a source and human-readable format to a binary one and encapsulate this.\nAfter this (very!) small break on resource management, let\u0026rsquo;s continue to the actual code for the materials!\nMaterial System implementation After all this thory let\u0026rsquo;s look at the code!Shader Effect The main parts of a Shader Effect are Passes and Properties.Passes are the most important one, as they contain all the informations to create an actual Pipeline, called Pipeline Creation.Remembering the Vulkan/DX12 interface, we cannot create singularly a shader, but we need all the pipeline data (depth stencil, alpha blend, \u0026hellip;) to actually create the shaders too.\nThe gist here is to access all those informations in a hiearchical way, basically reading them from the RenderPipeline and then overwriting with what is defined in the HFX file.\nRight now there is almost nothing if not the shaders, so the creation is quite simple:\nfor ( uint16_t p = 0; p \u0026lt; shader_effect_file.header-\u0026gt;num_passes; p++ ) {\rhfx::ShaderEffectFile::PassHeader* pass_header = hfx::get_pass( shader_effect_file, p );\ruint32_t shader_count = pass_header-\u0026gt;num_shader_chunks;\rmemcpy( effect-\u0026gt;passes[p].name, pass_header-\u0026gt;stage_name, 32 );\rPipelineCreation\u0026amp; pipeline_creation = effect-\u0026gt;passes[p].pipeline_creation;\rShaderCreation\u0026amp; creation = pipeline_creation.shaders;\rbool compute = false;\r// Create Shaders\rfor ( uint16_t i = 0; i \u0026lt; shader_count; i++ ) {\rhfx::get_shader_creation( pass_header, i, \u0026amp;creation.stages[i] );\rif ( creation.stages[i].type == ShaderStage::Compute )\rcompute = true;\r}\rcreation.name = pass_header-\u0026gt;name;\rcreation.stages_count = shader_count;\reffect-\u0026gt;passes[p].pipeline_creation.compute = compute;\r// Create Resource Set Layouts\rfor ( uint16_t l = 0; l \u0026lt; pass_header-\u0026gt;num_resource_layouts; l++ ) {\ruint8_t num_bindings = 0;\rconst ResourceListLayoutCreation::Binding* bindings = get_pass_layout_bindings( pass_header, l, num_bindings );\rResourceListLayoutCreation resource_layout_creation = { bindings, num_bindings };\rpipeline_creation.resource_list_layout[l] = context.device.create_resource_list_layout( resource_layout_creation );\r}\rpipeline_creation.num_active_layouts = pass_header-\u0026gt;num_resource_layouts;\r// Create Pipeline\reffect-\u0026gt;passes[p].pipeline_handle = context.device.create_pipeline( pipeline_creation );\rif ( effect-\u0026gt;passes[p].pipeline_handle.handle == k_invalid_handle ) {\rinvalid_effect = true;\rbreak;\r}\r}\rWhen we will have a proper RenderPipeline, we will get the basic pipeline creation from there, and overwrite the shaders and states that will be defined in the HFX.\nThere are 3 main steps here:\n Create Shaders Create Resource Set Layouts Create Pipelines  These are simple operations that rely heavily on the device.The objective of the HFX is to embed most information possible to create a complete pipeline.\nAnother important step is to populate the properties map:\nstring_hash_init_arena( effect-\u0026gt;name_to_property );\rfor ( uint32_t p = 0; p \u0026lt; effect-\u0026gt;num_properties; ++p ) {\rhfx::ShaderEffectFile::MaterialProperty* property = hfx::get_property( effect-\u0026gt;properties_data, p );\rstring_hash_put( effect-\u0026gt;name_to_property, property-\u0026gt;name, property );\r}\rWe are using the STB String Hashmap here with the property that are inside the shader effect file. Those will contain the type, name for UI and the pointer to a default value. The default value will be used based on the type of course.\nWe are also saving the local constant buffer size, so that we can allocate some memory in the Material and alter its property using the UI.\nWe will see the importance of this next.\nMaterial struct ShaderInstance {\rPipelineHandle pipeline;\rResourceListHandle resource_lists[hydra::graphics::k_max_resource_layouts];\ruint32_t num_resource_lists;\r}; // struct ShaderInstance\rstruct Material {\rShaderInstance* shader_instances = nullptr;\ruint32_t num_instances = 0;\rShaderResourcesLookup lookups; // Per-pass resource lookup. Same count as shader instances.\rShaderEffect* effect = nullptr;\rBufferHandle local_constants_buffer;\rchar* local_constants_data = nullptr;\rconst char* name = nullptr;\rStringBuffer loaded_string_buffer;\ruint32_t num_textures = 0;\ruint32_t pool_id = 0;\rTexture** textures = nullptr;\r}; // struct Material\rThis is the glue to actually render something on the screen.As a recap, we need 3 informations to render something:\n Pipeline (shaders + render states) Resources (handles to buffers and textures) Geometry (in this case a fullscreen quad)  Material gives all those informations.\nA Shader Instance is defined for each pass, and actually contains the Pipeline Handle and the List of Resource Lists to be used.This is one of the new concepts for Vulkan/DX12: you can use one of more lists of resources to render, and normally it is better to group them by frequency.\nFinally, a list of textures is saved to be modified by the editor.\nTo understand more the process, let\u0026rsquo;s look at the loading code of a Material.\nvoid* MaterialFactory::load( LoadContext\u0026amp; context ) {\rusing namespace hydra::graphics;\r// 1. Read header from file\rMaterialFile material_file;\rmaterial_file.header = (MaterialFile::Header*)context.data;\rmaterial_file.property_array = (MaterialFile::Property*)(context.data + sizeof( MaterialFile::Header ));\rmaterial_file.binding_array = (MaterialFile::Binding*)(context.data + sizeof( MaterialFile::Header ) + sizeof( MaterialFile::Property ) * material_file.header-\u0026gt;num_properties);\rWe are using the data from the material file to access properties and bindings.Properties are both numerical and path to textures, bindings are name to retrieve resources from the database. We will look into that later.\n // 2. Read shader effect\rResource* shader_effect_resource = string_hash_get( context.resource-\u0026gt;name_to_external_resources, material_file.header-\u0026gt;hfx_filename );\rShaderEffect* shader_effect = shader_effect_resource ? (ShaderEffect*)shader_effect_resource-\u0026gt;asset : nullptr;\rif ( !shader_effect ) {\rreturn nullptr;\r}\r// 3. Search pipeline\rRenderPipeline* render_pipeline = string_hash_get( context.name_to_render_pipeline, shader_effect-\u0026gt;pipeline_name );\rif ( !render_pipeline ) {\rreturn nullptr;\r}\rAccess the Shader Effect through the resource dependencies, and the Render Pipeline from the map.\n // 4. Load material\rchar* material_name = material_file.header-\u0026gt;name;\ruint32_t pool_id = materials_pool.obtain_resource();\rMaterial* material = new (materials_pool.access_resource(pool_id))Material();\rmaterial-\u0026gt;loaded_string_buffer.init( 1024 );\rmaterial-\u0026gt;pool_id = pool_id;\r// TODO: for now just have one lookup shared.\rmaterial-\u0026gt;lookups.init();\r// TODO: properly specialize.\r// For each pass\r//for ( uint32_t i = 0; i \u0026lt; effect-\u0026gt;num_pipelines; i++ ) {\r// PipelineCreation\u0026amp; pipeline = effect-\u0026gt;pipelines[i];\r// //final ShaderBindings specializedBindings = bindings.specialize( shaderTechnique.passName, shaderTechnique.viewName );\r// //shaderBindings.add( specializedBindings );\r//}\rmaterial-\u0026gt;effect = shader_effect;\rmaterial-\u0026gt;num_instances = shader_effect-\u0026gt;num_passes;\rmaterial-\u0026gt;shader_instances = new ShaderInstance[shader_effect-\u0026gt;num_passes];\rmaterial-\u0026gt;name = material-\u0026gt;loaded_string_buffer.append_use( material_name );\rmaterial-\u0026gt;num_textures = material_file.header-\u0026gt;num_textures;\rmaterial-\u0026gt;textures = (Texture**)hydra::hy_malloc( sizeof( Texture* ) * material-\u0026gt;num_textures );\rHere is the meaty part.We create the Material, initialize a StringBuffer used to store all the names found in the file, init the db-\u0026gt;resource lookup and create the ShaderInstance array.\n // Init memory for local constants\rmaterial-\u0026gt;local_constants_data = (char*)hydra::hy_malloc( shader_effect-\u0026gt;local_constants_size );\r// Copy default values to init to sane valuess\rmemcpy( material-\u0026gt;local_constants_data, material-\u0026gt;effect-\u0026gt;local_constants_default_data, material-\u0026gt;effect-\u0026gt;local_constants_size );\rWe cached the constant data size to allocate its memory, and we copy the default values in it. This memory will be overwritten by the other numerical properties and used to initialize the local constant buffer.\n // Add properties\ruint32_t current_texture = 0;\rfor ( size_t p = 0; p \u0026lt; material_file.header-\u0026gt;num_properties; ++p ) {\rMaterialFile::Property\u0026amp; property = material_file.property_array[p];\rhfx::ShaderEffectFile::MaterialProperty* material_property = string_hash_get( material-\u0026gt;effect-\u0026gt;name_to_property, property.name );\rswitch ( material_property-\u0026gt;type ) {\rcase hfx::Property::Texture2D:\r{\rconst char* texture_path = material-\u0026gt;loaded_string_buffer.append_use( property.data );\rResource* texture_resource = string_hash_get( context.resource-\u0026gt;name_to_external_resources, texture_path );\rTexture* texture = (Texture*)texture_resource-\u0026gt;asset;\rtexture-\u0026gt;filename = texture_path;\rrender_pipeline-\u0026gt;resource_database.register_texture( property.name, texture-\u0026gt;handle );\rmaterial-\u0026gt;textures[current_texture] = texture;\r++current_texture;\rbreak;\r}\rcase hfx::Property::Float:\r{\rmemcpy( material-\u0026gt;local_constants_data + material_property-\u0026gt;offset, property.data, sizeof( float ) );\rbreak;\r}\r}\r}\rWhen cycling through the properties, we are copying the numerical properties into the newly allocated memory (local_constant_data) and we load the textures from the dependencies.\n // Add bindings\rfor ( size_t b = 0; b \u0026lt; material_file.header-\u0026gt;num_bindings; ++b ) {\rMaterialFile::Binding\u0026amp; binding = material_file.binding_array[b];\rchar* name = material-\u0026gt;loaded_string_buffer.append_use( binding.name );\rchar* value = material-\u0026gt;loaded_string_buffer.append_use( binding.value );\rmaterial-\u0026gt;lookups.add_binding_to_resource( name, value );\r}\rWe populate the resource lookups.\n BufferCreation checker_constants_creation = {};\rchecker_constants_creation.type = BufferType::Constant;\rchecker_constants_creation.name = s_local_constants_name;\rchecker_constants_creation.usage = ResourceUsageType::Dynamic;\rchecker_constants_creation.size = shader_effect-\u0026gt;local_constants_size;\rchecker_constants_creation.initial_data = material-\u0026gt;local_constants_data;\rmaterial-\u0026gt;local_constants_buffer = context.device.create_buffer( checker_constants_creation );\rrender_pipeline-\u0026gt;resource_database.register_buffer( (char*)s_local_constants_name, material-\u0026gt;local_constants_buffer );\rGenerate the actual constant buffer.\n // Bind material resources\rupdate_material_resources( material, render_pipeline-\u0026gt;resource_database, context.device );\rAnd finally search the bindings for the resources.static void update_material_resources( hydra::graphics::Material* material, hydra::graphics::ShaderResourcesDatabase\u0026amp; database, hydra::graphics::Device\u0026amp; device ) {\rusing namespace hydra::graphics;\r// Create resource list\r// Get all resource handles from the database.\rResourceListCreation::Resource resources_handles[k_max_resources_per_list];\r// For each pass\rfor ( uint32_t i = 0; i \u0026lt; material-\u0026gt;effect-\u0026gt;num_passes; i++ ) {\rPipelineCreation\u0026amp; pipeline = material-\u0026gt;effect-\u0026gt;passes[i].pipeline_creation;\rfor ( uint32_t l = 0; l \u0026lt; pipeline.num_active_layouts; ++l ) {\r// Get resource layout description\rResourceListLayoutDescription layout;\rdevice.query_resource_list_layout( pipeline.resource_list_layout[l], layout );\r// For each resource\rfor ( uint32_t r = 0; r \u0026lt; layout.num_active_bindings; r++ ) {\rconst ResourceBinding\u0026amp; binding = layout.bindings[r];\r// Find resource name\r// Copy string_buffer char* resource_name = material-\u0026gt;lookups.find_resource( (char*)binding.name );\rswitch ( binding.type ) {\rcase hydra::graphics::ResourceType::Constants:\rcase hydra::graphics::ResourceType::Buffer:\r{\rBufferHandle handle = resource_name ? database.find_buffer( resource_name ) : device.get_dummy_constant_buffer();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\rcase hydra::graphics::ResourceType::Texture:\rcase hydra::graphics::ResourceType::TextureRW:\r{\rTextureHandle handle = resource_name ? database.find_texture( resource_name ) : device.get_dummy_texture();\rresources_handles[r].handle = handle.handle;\rbreak;\r}\rdefault:\r{\rbreak;\r}\r}\r}\rResourceListCreation creation = { pipeline.resource_list_layout[l], resources_handles, layout.num_active_bindings };\rmaterial-\u0026gt;shader_instances[i].resource_lists[l] = device.create_resource_list( creation );\r}\rmaterial-\u0026gt;shader_instances[i].num_resource_lists = pipeline.num_active_layouts;\rmaterial-\u0026gt;shader_instances[i].pipeline = material-\u0026gt;effect-\u0026gt;passes[i].pipeline_handle;\r}\r}\rFor each Pass, Resource Layout and Binding, we search the Database to retrieve the actual resource and create the Resource List.\nThis can be improved - having a global database of resources and a \u0026lsquo;local\u0026rsquo; one based on material resources. // 5. Bind material to pipeline\rfor ( uint8_t p = 0; p \u0026lt; shader_effect-\u0026gt;num_passes; ++p ) {\rchar* stage_name = shader_effect-\u0026gt;passes[p].name;\rhydra::graphics::RenderStage* stage = string_hash_get( render_pipeline-\u0026gt;name_to_stage, stage_name );\rif ( stage ) {\rstage-\u0026gt;material = material;\rstage-\u0026gt;pass_index = (uint8_t)p;\r}\r}\rreturn material;\r}\rFinally, and this is hacky, we assing the current material and pass index to the found stage.Once we have the real Render Pipeline/Graph working, we will use another dispatching mechanism.\nRendering of a Material After all of this we finally have created a Material.But how can we render it ?The magic here happens in a Render Pipeline!A Render Pipeline is a list of Render Stages and some resources with it. In this case resources are the render targets and the buffers that are shared amongst Stages (and Render Systems in the future).Resources are inside a Shader Resources Database and they can be retrieved using a Shader Resource Lookup.\nEach Render Stage has defined a list of input and output textures plus some resize data. This data is needed to recreate textures when a resize event arrives if needed, or change size if an option is changed (like a Shadow Map resolution option).As everthing in this articles, this is primordial and simple, but I think is a very good start, especially from a mindset perspective.\nIn this simple scenario we render 1 material only, and normally it simply 1 Material Pass for each Render Stage Pass, rendering either using a fullscreen quad or through compute.\nThere are 2 pipelines, both simple and used as a test, one is for a ShaderToy shader that I use as test, the other as a compute only pipeline. They are both hardcoded and created at the beginning of the Material Application, but as said before, it should be data-driven and reloadable to have great rendering power.\nRendering of a Pipeline The code is simple:\nvoid RenderPipeline::render( CommandBuffer* commands ) {\rfor ( size_t i = 0; i \u0026lt; string_hash_length( name_to_stage ); i++ ) {\rRenderStage* stage = name_to_stage[i].value;\rstage-\u0026gt;begin( commands );\rstage-\u0026gt;render( commands );\rstage-\u0026gt;end( commands );\r}\r}\rWe cycle through each stage and render.void RenderStage::begin( CommandBuffer* commands ) {\rcommands-\u0026gt;begin_submit( 0 );\rcommands-\u0026gt;begin_pass( render_pass );\rcommands-\u0026gt;set_viewport( { 0, 0, (float)current_width, (float)current_height, 0.0f, 1.0f } );\rif ( clear_rt ) {\rcommands-\u0026gt;clear( clear_color[0], clear_color[1], clear_color[2], clear_color[3] );\r}\rcommands-\u0026gt;end_submit();\r// Set render stage states (depth, alpha, ...)\r}\rBefore rendering anything this code will bind the correct FBO/Render Targets, clear and set viewport and set render states.After this we are ready to render the actual stage. In this simple implementation we have only 3 type of stages: Compute, Post and Swapchain.They are very simple and similar, like this:\ncommands-\u0026gt;begin_submit( pass_index );\rcommands-\u0026gt;bind_pipeline( shader_instance.pipeline );\rcommands-\u0026gt;bind_resource_list( \u0026amp;shader_instance.resource_lists[0], shader_instance.num_resource_lists );\rcommands-\u0026gt;draw( graphics::TopologyType::Triangle, 0, 3 );\rcommands-\u0026gt;end_submit();\rSet the pipeline, bind all the different resource lists and issue the draw (in this case a full screen triangle).\nIncluded in the code Material application I just added a simple Material Application to render the content of one of those simple shaders.\nHonestly not very happy about the code quality - and you can see why trying to add big features like memory management or multi-threading is a no-go.\nThe application let you switch between materials by right clicking on the .hmt file.The whole purpose is to explore with the given code a couple of materials and their dependencies.Starnest is a shader by the amazing Pablo Roman Andrioli, so all credits are to him! I wanted something beautiful to show in this simple example from ShaderToy.\nConclusions and some thoughts We added a simple material system based on our HFX language.Interestingly enough code generation is used much less - if almost nothing - instead of serializing data into files and using them.As stated in the other articles, the goal is to have a parsing and code generation knowledge under your belt, and understand when it is time to use it!We also introduced a lot of connections to other topics that are lengthy enough - like resource management - that need more time and dedication to properly be explored.I am continuing working on this until it will become my rendering explorer - a tool I can use to easily explore ideas, much like ShaderToy but in an even more powerful way.How ?In the next article we will explore the final piece of the puzzle, and then we will probably start iterating and improving on what we have!We will see how we can describe a frame and the rendering dependencies in an easy way, especially if done since the beginning, and how much having that knowledge upfront is GREAT to work on rendering.I am honestly not happy about the overall architecture though - here you have an example of exploring code - code written to explore a specific subject, and after venturing more into it you want to rewrite it.To properly rewrite it you need to create solid foundations - namely Memory Management, Multi-Threading, Basic Data Structures, \u0026hellip; and choose to pick your battles!\nThis is a huge lesson: pick your battles, choose what to concentrate on.These articles are more towards code generation and rendering, but defining the constraints of the articles helps in narrowing down what to do.If, as I would like, you want to use this code to evolve into something like a \u0026lsquo;desktop\u0026rsquo; Shadertoy, then you can\u0026rsquo;t ignore all the foundational topics.On the other end if you just quickly want to experiment with those topics, this should suffice.I have two paths here: rewriting most of this code with a solid foundations, and delaying a RenderPipeline/Graph article, or finishing with this architecture and then re-write everything with the \u0026lsquo;desktop Shadertoy\u0026rsquo;.Again, pick your battles :)\nAs always, please comment, feedback, share!I really hope soon there will be some rendering joy!\nGabriel\n","date":1571064229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576767529,"objectID":"f35b15163ee0411fa4b054c437381030","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_3/","publishdate":"2019-10-14T10:43:49-04:00","relpermalink":"/post/writing_shader_effect_language_3/","section":"post","summary":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In Part 2 of this series we added Resource Layouts and Properties to the HFX language, trying to arrive at a point in which we can describe the rendering of a Shader Effect almost entirely.In this article I would like to explore further adds to HFX, especially a proper Material System to be used in conjunction with the HFX language.","tags":[],"title":"Writing a Shader Effect Language Part 3: Materials","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In Part 1 of this series we created a simple language to work as \u0026lsquo;shader effect\u0026rsquo; - a shader language superset to make our life easier, by adding missing features.The fact that there is not an industry standard for a shader effect language leads to either hand-crafted (and secret) languages, or to hardcoded permutations, or to other gray-area solutions.\n(Personal though: part of me would like to help in contributing to the creation of a standard through these articles and code.)\nWhat is the goal of this article ?\nThe goal is to enrich the HFX language to generate more code possible and/or bake data for us, namely:\n Shader constants generation Shader resource bindings Render states (depth stencil, blend, rasterization) Render pass hints for a future framegraph  We will see Render States and Render Pass hints in a following article, because this is an already lengthy article!\nI hope that by now the way of adding an identifier, parsing it and generating code is clearer.In this article we will focus more on the features than anything else, even though I will put a lot of code still.But before that, we need to have a big addition to our example: a rendering API!We will use this as target of our code generation, and it will be an amazing example to see something working.\nMaybe this will spark a new FX Composer ?\nThis article will be divided in 2 parts.Part 1 of this article will talk about the rendering API.Part 2 will be about the extended HFX language.If you are not interested in that, jump to part 2 of this article.\nPart 1: adding a low-level rendering API Writing articles on rendering without some sort of API to use is tricky.Creating a language to speed up data driven rendering, either for generating code and/or for baking data needs a target API.The main idea is to have an abstract API to map more easily rendering concepts instead of losing ourselves in specific API needs.\nThe search for an abstract API The first thing to do is to search for an existing abstract API.I have few criteria in mind:\n Simple and clear interface Compact and clear code Vulkan and D3D12 interface  With those in mind, I found 2 alternatives: BGFX and Sokol.\nI am an honest fan of both, they are brilliant, robust and well written.But for the purpose of these articles, sadly they miss my search criteria.There is also a huge disclaimer here: I used them too little, so it is possible I overlooked the usage of them.I will be more than glad to use either instead of my toy API!I respect the developers and the library a lot, they are doing an amazing job!But we are handcrafting something, and to properly do that I personally need to know deeply the code. And I am not.\nBGFX is very complete, but the interface is a little confusing for me, possibly because I never used it but just read the code few times.The main reason I chose not to use it is because the interface is missing the resource interface like Vulkan and D3D12 (DescriptorSets, \u0026hellip;), otherwise it would have been an amazing choice.\nSokol is also very good, I love the code and the simple interface.Two main problems here: again no Vulkan/D3D12 resource interface, and in this case a different target: it does not support compute shaders.\nAgain, I want to make it clear: I am not saying these are not good libraries. They are amazing. They just don\u0026rsquo;t fit my search criteria, plus I LOVE to work on rendering architecture. Well actually, it is my favourite job!\nSo kudos to them (I also wrote to Andre Weissflog to ask for compute shader support, but it is not in his plans for now) but we are making a different choice.If you ever find anything that I write useful guys, please let me know!\nHydra Graphics: design principles Small trivia: the name comes from my first ever graphics engine written in 2006 (I think), after devouring 3D Game Engine Design by Dave Eberly. I already knew I would write many engines and I would learn and grow stronger from every of them, so I chose the name Hydra from the Greek mythology monster.The other name would have been Phoenx engine, but I remember finding already some tech with that name.\nAnyway, design principles!I really loved the interface of Sokol, and often I used something similar by myself.I opted for a pair of header/implementation files as the only needed files.\nThe backend is OpenGL for now, just because I have a working implementation in my indie project that works with pretty complex rendering, and I can use that as reference.\nInterface Rendering in general is a matter of creating, modifying and combining resources.There are mainly 2 classes that do all the rendering work:\n Device Command Buffer  The Device is responsible for creation, destruction, modification and query of the resources.The Command Buffer is responsible for the usage of resources for rendering.\nThe obvious fundamental concept is resource.A resource is handled externally through handles, can be created using creation structs and has both a common and an API-specific representation.\nBuffers are specialized in vertex/index/constant/\u0026hellip; depending on their creation parameters.\nThis is a small example on creation/usage/destruction of a resource.First, we can create a texture:\ngraphics::TextureCreation first_rt = {};\rfirst_rt.width = 512;\rfirst_rt.height = 512;\rfirst_rt.render_target = 1;\rfirst_rt.format = graphics::TextureFormat::R8G8B8A8_UNORM;\rTextureHandle render_target = gfx_device.create_texture( first_rt );\rNext we can create a command buffer:\nCommandBuffer* commands = gfx_device.get_command_buffer( graphics::QueueType::Graphics, 1024 );\rSkipping other creations, we bind resources and add the commands:\ncommands-\u0026gt;bind_pipeline( first_graphics_pipeline );\rcommands-\u0026gt;bind_resource_set( gfx_resources );\rcommands-\u0026gt;bind_vertex_buffer( gfx_device.get_fullscreen_vertex_buffer() );\rcommands-\u0026gt;draw( graphics::TopologyType::Triangle, 0, 3 );\rAt this point we can execute the command buffer to draw.\ngfx_device.execute_command_buffer( commands );\rUpdating a resource can be done like that:\nhydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\rLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters );\rEverything uses structs to perform creation/updates.Nothing new, but I always loved this design.\nResource layout and resource lists I wanted to bring the Vulkan/D3D12 resource interface as first class citizens, and remove completely old concepts (like single constants, render states as single objects, single bind of a resource) and add new ones: resource layout, resource lists and command buffers. Well command buffers are not new, but finally you can draw only with those!\nIn Vulkan/D3D12 you can bind resources through the usage of sets: basically tables that contains the resources used.This is a welcomed difference from previous APIs, and I think it is a concept not too hard to grasp but very useful to have it explicit.\nThe first thing to define is the resource layout describes the layout of a set of resources.For example, if we have a material that uses Albedo and Normals textures and a constant buffer, the layout will contain all the informations about that (like the type, the GPU registers and so on).This though still does not contain the resources themselves!Enter resource list.A resource list is a list of actual resources relative to a layout.It sets resources using a layout.\nFrom now on, when we draw we can bind one or more resource lists.\nIn Vulkan lingo, the resource layout is called descriptor set layout, and a resource list is a descriptor set.Here are a couple of articles for the Vulkan side:\nOfficial Vulkan Documentation on Descriptor Layouts and Sets\nIntel API Without Secrets Part 6\nSimilarly in D3D12 there are Root Tables and Descriptor Tables. The concepts do no map 1 to 1 but they are pretty similar:\nD3D12 Descriptor Tables\nI tried to map these concepts using different words that would make more sense to me, so from Descriptor Set or Root Table it became Resource List and Resource Layout.\nPipelines Finally a pipeline is the complete description of what is needed by the GPU to draw something on the screen (or to use a Compute Shader for any other purpose).Basically a pipeline must fill all the informations for all the GPU stages like this (thanks to RenderDoc):\n  RenderDoc Pipeline   What once was setup individually now is all in one place (reflecting what happened behind the scene, into the driver).DepthStencil, AlphaBlend, Rasterization, Shaders, all must be defined here.\nIn the currrent implementation of the graphics-API a lot of states are still missing!.\nNow that we say the basic principles of the target rendering API, we can finally concentrate on the new freatures of HFX.\nPart 2: forging the HFX language features Our HFX language needs some properties to be added but first there is a change: HFX will generate a binary version to embed all the informations needed to create a shader.\nHFX evolution: what files are generated ? In the previous article, we used a single HFX file to generate multiple glsl files, ready to be used by any OpenGL renderer:\n  Shader Generation   Remembering the article on Hydra Data Format, we instead were generating an header file.For our needs, we will generate an embedded HFX (binary HFX) AND a C++ header:\n  Binary and Header Generation   What is the next step for HFX ?For shader generation, we want ideally to load a HFX file without having to manually stick together the single shader files, and that is why the first step is to create embedded HFX files.This will contain all the information to create a shader, and this includes also the resource layouts.\nFor constant handling, we want to have UI generated and easy update on the gpu. We want to automate these things.This can be done in a more code-generated way or by generating data.\nIf we abstract the problem, all these articles are about understanding how you want to generate code or data to maximise iteration time, performances and control.By moving the HFX to being binary, we are effectively generating data used by the renderer.For the shader UI, we can do both: generate code or create data. We will see the generated code part here.\nLet\u0026rsquo;s see briefly the internals of the Embedded HFX file format:\nEmbedded HFX As a Recap, when parsing HFX we store some informations.\nFirst is the CodeFragment, including also (spoiler!) the addition of resources for the sake of this article:\n// C++\r//\rstruct CodeFragment {\rstruct Resource {\rhydra::graphics::ResourceType::Enum type;\rStringRef name;\r}; // struct Resource\rstd::vector\u0026lt;StringRef\u0026gt; includes;\rstd::vector\u0026lt;Stage\u0026gt; includes_stage; // Used to separate which include is in which shader stage.\rstd::vector\u0026lt;Resource\u0026gt; resources; // Used to generate the layout table.\rStringRef name;\rStringRef code;\rStage current_stage = Stage::Count;\ruint32_t ifdef_depth = 0;\ruint32_t stage_ifdef_depth[Stage::Count];\r}; // struct CodeFragment\rThe rest is unchanged from the previous article.We have basically code and includes to bake the final shader.Remember, we are handling GLSL in these examples!\nNext is the Pass:\n// C++\r//\rstruct Pass {\rStringRef name;\rstruct ShaderStage {\rconst CodeFragment* code = nullptr;\rStage stage = Stage::Count;\r}; // struct ShaderStage\rStringRef name;\rstd::vector\u0026lt;ShaderStage\u0026gt; shader_stages;\r}; // struct Pass\rNothing changed here.A pass is a container of one of more shaders.In general we will use the term shader state to describe the shaders that needs to be bound to the pipeline.Most common are the couple Vertex and Fragment shaders, or the Compute by itself.\nLast is the Shader itself:\n// C++\r//\rstruct Shader {\rStringRef name;\rstd::vector\u0026lt;Pass*\u0026gt; passes;\rstd::vector\u0026lt;Property*\u0026gt; properties;\r}; // struct Shader\rBeing just a collection of passes.Again we are seeing the properties here, that I will talk later on in the article.\nThese will be used to \u0026lsquo;bake\u0026rsquo; data into a \u0026lsquo;bhfx\u0026rsquo; (binary HFX) file.\nBHFX layout In order to maximise efficiency, we are packing the data in the way we will use it.The file is divided in two main sections: common and passes.The overall layout is as follows:\n   The trick is to have the offset for each section easy to access.\nThe pass section contains several informations as following:\n   As we will see later we include shaders, resources layout and other data based on our target API (Hydra Graphics).\nWriting the BHFX file To write our file, we need to parse the HFX file.A quick code could be something like this:\n// C++\r//\r...\rchar* text = ReadEntireFileIntoMemory( \u0026quot;..\\\\data\\\\SimpleFullscreen.hfx\u0026quot;, nullptr );\rinitLexer( \u0026amp;lexer, (char*)text, data_buffer );\rhfx::initParser( \u0026amp;effect_parser, \u0026amp;lexer );\rhfx::generateAST( \u0026amp;effect_parser );\rhfx::initCodeGenerator( \u0026amp;hfx_code_generator, \u0026amp;effect_parser, 4096, 5 );\rhfx::compileShaderEffectFile( \u0026amp;hfx_code_generator, \u0026quot;..\\\\data\\\\\u0026quot;, \u0026quot;SimpleFullscreen.bhfx\u0026quot; );\rHere we are parsing the file (generateAST) and then using that to compile our shader effect file. This is where the magic happens.\n// C++\r//\rvoid compileShaderEffectFile( CodeGenerator* code_generator, const char* path, const char* filename ) {\r// Create the output file\rFILE* output_file;\r// Alias the StringBuffer for better readability.\rStringBuffer\u0026amp; filename_buffer = code_generator-\u0026gt;string_buffers[0];\r// Concatenate name\rfilename_buffer.clear();\rfilename_buffer.append( path );\rfilename_buffer.append( filename );\rfopen_s( \u0026amp;output_file, filename_buffer.data, \u0026quot;wb\u0026quot; );\rif ( !output_file ) {\rprintf( \u0026quot;Error opening file. Aborting. \\n\u0026quot; );\rreturn;\r}\rTypical file creation preamble.Concatenate the file using the StringBuffer, and try to create it.\nRemember that overall the file structure is:\n File header Pass offset list Pass sections  Let\u0026rsquo;s start with the file header:\n const uint32_t pass_count = (uint32_t)code_generator-\u0026gt;parser-\u0026gt;passes.size();\rShaderEffectFile shader_effect_file;\rshader_effect_file.num_passes = pass_count; fwrite( \u0026amp;shader_effect_file, sizeof(ShaderEffectFile), 1, output_file );\rIn this case we are writing straight to the file, because it is an in-order operation with the file layout.For the rest of the file writing we will need to use String Buffers to accumulate data out-of-order and then write the file in the correct order.Think of the Pass Offset List: to calculate the offsets we need to know the size of the passes. To know the size we need to finalize the pass data. To finalize the pass data we need to finalize shaders, and that means adding the includes.\nAgain for code clarity I use aliases like this:\n StringBuffer\u0026amp; code_buffer = code_generator-\u0026gt;string_buffers[1];\rStringBuffer\u0026amp; pass_offset_buffer = code_generator-\u0026gt;string_buffers[2];\rStringBuffer\u0026amp; shader_offset_buffer = code_generator-\u0026gt;string_buffers[3];\rStringBuffer\u0026amp; pass_buffer = code_generator-\u0026gt;string_buffers[4];\rLet\u0026rsquo;s continue.We start tracking the pass section memory offset knowing that it will be after the header and the pass offset list:\n pass_offset_buffer.clear();\rpass_buffer.clear();\r// Pass memory offset starts after header and list of passes offsets.\ruint32_t pass_offset = sizeof( ShaderEffectFile ) + sizeof(uint32_t) * pass_count;\rNow into the most interesting part. We will avoid talking about the resource layout part, that will be added later. // Pass Section:\r// ----------------------------------------------------------------------------------------\r// Shaders count | Name | Shader Offset+Count List | Shader Code 0, Shader Code 1\r// ----------------------------------------------------------------------------------------\rShaderEffectFile::PassHeader pass_header;\rfor ( uint32_t i = 0; i \u0026lt; pass_count; i++ ) {\rpass_offset_buffer.append( \u0026amp;pass_offset, sizeof( uint32_t ) );\rconst Pass\u0026amp; pass = code_generator-\u0026gt;parser-\u0026gt;passes[i];\rconst uint32_t pass_shader_stages = (uint32_t)pass.shader_stages.size();\rconst uint32_t pass_header_size = pass_shader_stages * sizeof( ShaderEffectFile::Chunk ) + sizeof( ShaderEffectFile::PassHeader );\ruint32_t current_shader_offset = pass_header_size;\rWe start iterating the passes and calculate the shader offset.Shader Chunks (the actual shader code) are written after the Pass Header and the dynamic list of shader chunk offset and size.Next we will calculate the offsets of the single shaders AFTER we finalize the code - that means after the includes are added!\n shader_offset_buffer.clear();\rcode_buffer.clear();\rfor ( size_t s = 0; s \u0026lt; pass.shader_stages.size(); ++s ) {\rconst Pass::ShaderStage shader_stage = pass.shader_stages[s];\rappendFinalizedCode( path, shader_stage.stage, shader_stage.code, filename_buffer, code_buffer, true, constants_buffer );\rupdateOffsetTable( \u0026amp;current_shader_offset, pass_header_size, shader_offset_buffer, code_buffer );\r}\r// Update pass offset\rpass_offset += code_buffer.current_size + shader_offset;\rAt this point we have code_buffer containing all the shaders of the pass one after another (null terminated) and we can update the pass offset for the next pass.We also calculated the single shader offsets with the updateOffsetTable method in shader_offset_buffer.We need to finalize the Pass Header and then we can merge the pass memory in one block and proceed to the next pass:\n // Fill Pass Header\rcopy( pass.name, pass_header.name, 32 );\rpass_header.num_shader_chunks = pass.num_shaders;\rThis is a very IMPORTANT part.Merge in the pass_buffer all the pass section currently calculated: pass header, the single shader code offsets and the shader code itself.\n pass_buffer.append( (void*)\u0026amp;pass_header, sizeof( ShaderEffectFile::PassHeader ) );\rpass_buffer.append( shader_offset_buffer );\rpass_buffer.append( code_buffer );\r}\rAfter we finished with all the passes, we have 2 buffers: one containing the pass offset list, the other the pass sections.We can write them off in the correct order finally and close the file:\n fwrite( pass_offset_buffer.data, pass_offset_buffer.current_size, 1, output_file );\rfwrite( pass_buffer.data, pass_buffer.current_size, 1, output_file );\rfclose( output_file );\r}\rWe can see why we chose this format when looking at the code to actually create a shader state.First of all this is the struct to create a shader state:\n// hydra_graphics.h\r//\rstruct ShaderCreation {\rstruct Stage {\rShaderStage::Enum type = ShaderStage::Compute;\rconst char* code = nullptr;\r}; // struct Stage\rconst Stage* stages = nullptr;\rconst char* name = nullptr;\ruint32_t stages_count = 0;\r}; // struct ShaderCreation\rIt is very simple, each stage has a code and type.A shader state can have one or more stages.This was already the case in OpenGL - compiling shaders and linking them - so the interface is similar - but it maps well to Vulkan/D3D12 as well, in which the Pipeline State, that describe almost everything the GPU needs to draw, needs an unique set of vertex/fragment/compute shaders.Anyway, we embed this data already in the binary HFX file, and thus we can easily create a shader state like this:\nstatic void compile_shader_effect_pass( hydra::graphics::Device\u0026amp; device, char* hfx_memory, uint16_t pass_index, hydra::graphics::ShaderHandle\u0026amp; out_shader ) {\rusing namespace hydra;\r// Get pass section memory\rchar* pass = hfx::getPassMemory( hfx_memory, pass_index );\rhfx::ShaderEffectFile::PassHeader* pass_header = (hfx::ShaderEffectFile::PassHeader*)pass;\rconst uint32_t shader_count = pass_header-\u0026gt;num_shader_chunks; graphics::ShaderCreation::Stage* stages = new graphics::ShaderCreation::Stage[shader_count];\r// Get individual shader code and type\rfor ( uint16_t i = 0; i \u0026lt; shader_count; i++ ) {\rhfx::getShaderCreation( shader_count, pass, i, \u0026amp;stages[i] );\r}\rgraphics::ShaderCreation first_shader = {};\rfirst_shader.stages = stages;\rfirst_shader.stages_count = shader_count;\rfirst_shader.name = pass_header-\u0026gt;name;\rout_shader = device.create_shader( first_shader );\rdelete stages;\r}\rNothing really interesting here, but we read the file in memory and use the offsets we store to access the different sections of the file.To access the Pass Section we first need to read its memory offset and then read from there.Remember from before that the offset is in the list AFTER the ShaderEffectFile header, and it is a single uint32:\nchar* getPassMemory( char* hfx_memory, uint32_t index ) {\r// Read offset form list after the ShaderEffectFile header.\rconst uint32_t pass_offset = *(uint32_t*)(hfx_memory + sizeof( ShaderEffectFile ) + (index * sizeof( uint32_t )));\rreturn hfx_memory + pass_offset;\r}\rFrom the pass offset, the list of shader chunks (that are defined as code offset and size) is right after the pass header\nvoid getShaderCreation( uint32_t shader_count, char* pass_memory, uint32_t index,\rhydra::graphics::ShaderCreation::Stage* shader_creation ) {\rchar* shader_offset_list_start = pass_memory + sizeof( ShaderEffectFile::PassHeader );\rRead the single shader offset and access the memory there:\n const uint32_t shader_offset = *(uint32_t*)(shader_offset_list_start + (index * sizeof( ShaderEffectFile::Chunk )));\rchar* shader_chunk_start = pass_memory + shader_offset;\rThe baked informations are first the type (as a single char, but called hfx::ShaderEffectFile::ChunkHeader in case we change it) and the actual shader code is right after!\n shader_creation-\u0026gt;type = (hydra::graphics::ShaderStage::Enum)(*shader_chunk_start);\rshader_creation-\u0026gt;code = (const char*)(shader_chunk_start + sizeof( hfx::ShaderEffectFile::ChunkHeader ));\r}\rIn this case I chose to bake the file instead of generating a header file - just cause I can reuse this code for every shader effect. I could have generated an header instead of the binary BHFX file, but then including it would mean that you need to recompile at every change.We will see some areas in which we can have both approaches!\nFinally done with the new embedded format, let\u0026rsquo;s see the new features!\nBrainstorming: what features are needed ? We already talked about the features at the beginning of the articles, but let\u0026rsquo;s write them again to refresh our memory:\n Shader constants generation Shader resource bindings Render states (depth stencil, blend, rasterization) (in the next article) Render pass hints for a future framegraph (in the next article)  There are few articles around this subject, but the most complete is from the amazing guys at OurMachinery, and in particular this article.These guys does (as always honestly) an amazing job in describing the problem we are facing and the solutions, and how enriching a shader language can make a huge difference in making better rendering (faster iteration time, less error prone, more artist friendly..) so I would suggest to read those articles (and in general any article/presentation/blog post they write!).\nWe will go through each feature in depth so get ready!\nConstants: artists, programmers, both ? Constants\u0026hellip;uniforms\u0026hellip;whatever name you choose, they represent the same concept: numerical properties.\nEven if they are a simple concept, still it is hard to make both rendering citizens happy: artists and programmers!\nArtists want tweakable UI, simple variables and fast iteration.Programmers want optimal layout, more CPU calculated variables possible, and ultimate control.How to make them both happy ?\nI brainstormed and designed for few days (well evenings) to solve this problem.One thought that came to me is that artists want to create a material interface, something they can tweak and change easily, and when you want to quickly prototype something, create and such, you don\u0026rsquo;t want to deal with low-level resource management and such.Let\u0026rsquo;s solve this first: give artists a simple way of creating a material interface!\nAfter searching for a bit, I chose to use a syntax very similar to Unity ShaderLab. Let\u0026rsquo;s see the HFX (finally!):\n// .HFX\r//\r// For the artist: create a material interface.\rproperties {\r// Using Unity ShaderLab syntax:\r// AORemapMin0(\u0026quot;AORemapMin0\u0026quot;, Range(0.0, 1.0)) = 0.0\rscale(\u0026quot;Scale\u0026quot;, Float) = 32.00\rmodulo(\u0026quot;Modulo\u0026quot;, Float) = 2.0\r}\rWe added a new section in the language, named \u0026ldquo;properties\u0026quot;.Why this name ?Because properties contains both numerical properties and textures!The name makes sense in this way. Naming \u0026lsquo;constants\u0026rsquo; and having also textures, not.\nThere are 2 possible outputs from this, one that is pure code-generation and the other that is more data-driven. I will dwelve into the code-generation one and talk about the data-driven one in another post.\nThere are 3 parts for the generated code of the properties:\n Properties UI GPU-ready constant buffer API-dependant buffer  For the Properties UI, we want to generate something like this:\n// C++\rstruct LocalConstantsUI {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rvoid reflectMembers() {\rImGui::InputScalar( \u0026quot;Scale\u0026quot;, ImGuiDataType_Float, \u0026amp;scale);\rImGui::InputScalar( \u0026quot;Modulo\u0026quot;, ImGuiDataType_Float, \u0026amp;modulo);\r}\rvoid reflectUI() {\rImGui::Begin( \u0026quot;LocalConstants\u0026quot; );\rreflectMembers();\rImGui::End();\r}\r}; // struct LocalConstantsUI\rFor the GPU-ready constants, we want to have a both a GPU and a CPU representation like this:\n// C++\r//\rstruct LocalConstants {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rfloat pad_tail[2];\r}; // struct LocalConstants\r// GLSL\r//\rlayout (std140, binding=7) uniform LocalConstants {\rfloat scale;\rfloat modulo;\rfloat pad[2];\r} local_constants;\rAnd for the API-dependant buffer, we want to create code that takes care of everything for us. This is the real deal here - and something we will revisit in next articles to show some advanced features.\nvoid create( hydra::graphics::Device\u0026amp; device ) {\rusing namespace hydra;\rgraphics::BufferCreation constants_creation = {};\rconstants_creation.type = graphics::BufferType::Constant;\rconstants_creation.name = \u0026quot;LocalConstants\u0026quot;;\rconstants_creation.usage = graphics::ResourceUsageType::Dynamic;\r// NOTE: using LocalConstants struct - is the GPU ready one with padding and such!\rconstants_creation.size = sizeof( LocalConstants );\r// Struct is initialized with default values already, so it is safe to copy it to the GPU.\rconstants_creation.initial_data = \u0026amp;constants;\rbuffer = device.create_buffer( constants_creation );\r}\rvoid destroy( hydra::graphics::Device\u0026amp; device ) {\rdevice.destroy_buffer( buffer );\r}\rvoid updateUI( hydra::graphics::Device\u0026amp; device ) {\r// Draw UI\rconstantsUI.reflectUI();\r// TODO:\r// Ideally there should be a way to tell if a variable has changed and update only in that case.\r// Map buffer to GPU and upload parameters from the UI\rhydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\rLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters );\rif ( buffer_data ) {\rbuffer_data-\u0026gt;scale = constantsUI.scale;\rbuffer_data-\u0026gt;modulo = constantsUI.modulo;\rdevice.unmap_buffer( map_parameters );\r}\r}\rFor the sake of the example this could be a possible implementation - but really depends on the rendering API. Let\u0026rsquo;s quickly check parsing and code-generation.\nConstants Parsing To parse the new property section, there is the new method void declarationProperties( Parser* parser ) that iterates through all properties, and inside that the void declarationProperty( Parser* parser, const StringRef\u0026amp; name ) one.\nWe are parsing the following HFX syntax:\n// Syntax\r//\ridentifier(string, identifier[(arguments)]) [= default_value]\rWith this is an example:\n// HFX\r//\rproperties {\rscale(\u0026quot;Scale\u0026quot;, Float) = 32.0\r}\rWe will add a simple backtracking to the parsing because of the optional parameters.Let\u0026rsquo;s check the code!\ninline void declarationProperty( Parser* parser, const StringRef\u0026amp; name ) {\rProperty* property = new Property();\r// Cache name\rproperty-\u0026gt;name = name;\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenParen ) ) {\rreturn;\r}\rWe just parsed the property name and the \u0026lsquo;('. Next is the string containing the UI name:\n // Advance to the string representing the ui_name\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_String ) ) {\rreturn;\r}\rproperty-\u0026gt;ui_name = token.text;\rSaved the ui name and then we have the type.Types can be Float, Int, Range, Texture, Vector, Color and we will simply parse their text and convert it to an enum that we will use in the code generation phase.\n if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Comma ) ) {\rreturn;\r}\r// Next is the identifier representing the type name\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r// Parse property type and convert it to an enum\rproperty-\u0026gt;type = propertyTypeIdentifier( token );\rNow will come the most complicated part.We have optional \u0026lsquo;(\u0026rsquo; open parenthesis for the parameters if the type needs it.For the length of code and article, I skip this part and will add it in next article!\n // If an open parenthesis is present, then parse the ui arguments.\rnextToken( parser-\u0026gt;lexer, token );\rif ( token.type == Token::Token_OpenParen ) {\rproperty-\u0026gt;ui_arguments = token.text;\rwhile ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseParen ) ) {\r// TODO:\r// Parse parameters!\r}\r// Advance to the last close parenthesis\rnextToken( parser-\u0026gt;lexer, token );\rproperty-\u0026gt;ui_arguments.length = token.text.text - property-\u0026gt;ui_arguments.text;\r}\rif ( !checkToken( parser-\u0026gt;lexer, token, Token::Token_CloseParen ) ) {\rreturn;\r}\rAt this point we can either be at the end of the property or we could have a \u0026lsquo;=\u0026rsquo; token to add a default value. Being that the Lexer class is small, we can backtrack by saving the current Lexer status:\n // Cache lexer status and advance to next token.\r// If the token is '=' then we parse the default value.\r// Otherwise backtrack by one token.\rLexer cached_lexer = *parser-\u0026gt;lexer;\rNow we can advance to the next token and:\n If the token is \u0026lsquo;=\u0026rsquo;, parse the default value. If not, backtrack the position of the Lexer and finish the parsing.   nextToken( parser-\u0026gt;lexer, token );\r// At this point only the optional default value is missing, otherwise the parsing is over.\rif ( token.type == Token::Token_Equals ) {\rnextToken( parser-\u0026gt;lexer, token );\rif ( token.type = Token::Token_Number ) {\r// Cache the data buffer entry index into the property for later retrieval.\rproperty-\u0026gt;data_index = parser-\u0026gt;lexer-\u0026gt;data_buffer-\u0026gt;current_entries - 1;\r}\relse {\r// TODO:\r// Handle vectors, colors and non single number default values\r}\r}\relse {\r*parser-\u0026gt;lexer = cached_lexer;\r}\rparser-\u0026gt;shader.properties.push_back( property );\r}\rAn interesting point is that the numbers are parsed in a DataBuffer, and during the parsing of the token we will add the number to it.To retrieve it, we have the data_index field of the Property struct.Also here, for the sake of \u0026lsquo;brevity\u0026rsquo;, I am handling only floats and ints. Vectors, colors and texture property should be easy to add.\nFor vectors and colors we should parse a list of them and save them into the data buffer.\nFor textures we should just save the default value as text and use it in the code-generation part.\nCode Generation This should be pretty straight forward.We can iterate the properties and generate both a C++ struct and a HLSL/GLSL buffer.The only thing to be concerned is the padding: on the GPU normally the alignment is 16 bytes, so we can track that and insert padding when generating the code.In the method void generateShaderResourceHeader( CodeGenerator* code_generator, const char* path ) we can see how we generate the different code for C++:\n// C++\r//\r// Beginning\rfprintf( output_file, \u0026quot;\\n#pragma once\\n#include \u0026lt;stdint.h\u0026gt;\\n#include \\\u0026quot;hydra_graphics.h\\\u0026quot;\\n\\n// This file is autogenerated!\\nnamespace \u0026quot; );\rfwrite( shader.name.text, shader.name.length, 1, output_file );\rfprintf( output_file, \u0026quot; {\\n\\n\u0026quot; );\r// Preliminary sections\rconstants_ui.append( \u0026quot;struct LocalConstantsUI {\\n\\n\u0026quot; );\rcpu_constants.append( \u0026quot;struct LocalConstants {\\n\\n\u0026quot; );\rconstants_ui_method.append(\u0026quot;\\tvoid reflectMembers() {\\n\u0026quot;);\rbuffer_class.append( \u0026quot;struct LocalConstantsBuffer {\\n\\n\\thydra::graphics::BufferHandle\\tbuffer;\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\tLocalConstants\\t\\t\\t\\t\\tconstants;\\n\\tLocalConstantsUI\\t\\t\\t\\tconstantsUI;\\n\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\tvoid create( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\tusing namespace hydra;\\n\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\t\\tgraphics::BufferCreation constants_creation = { graphics::BufferType::Constant, graphics::ResourceUsageType::Dynamic, sizeof( LocalConstants ), \u0026amp;constants, \\\u0026quot;LocalConstants\\\u0026quot; };\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\t\\tbuffer = device.create_buffer( constants_creation );\\n\\t}\\n\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\tvoid destroy( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\tdevice.destroy_buffer( buffer );\\n\\t}\\n\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\tvoid updateUI( hydra::graphics::Device\u0026amp; device ) {\\n\\t\\t// Draw UI\\n\\t\\tconstantsUI.reflectUI();\\n\\t\\t// Update constants from UI\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\t\\thydra::graphics::MapBufferParameters map_parameters = { buffer.handle, 0, 0 };\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\t\\tLocalConstants* buffer_data = (LocalConstants*)device.map_buffer( map_parameters );\\n\\t\\tif (buffer_data) {\\n\u0026quot; );\r// For GPU the struct must be 16 bytes aligned. Track alignment\ruint32_t gpu_struct_alignment = 0;\rDataBuffer* data_buffer = code_generator-\u0026gt;parser-\u0026gt;lexer-\u0026gt;data_buffer;\r// For each property write code\rfor ( size_t i = 0; i \u0026lt; shader.properties.size(); i++ ) {\rhfx::Property* property = shader.properties[i];\rswitch ( property-\u0026gt;type ) {\rcase Property::Float:\r{\rconstants_ui.append(\u0026quot;\\tfloat\\t\\t\\t\\t\\t\u0026quot;);\rconstants_ui.append( property-\u0026gt;name );\rcpu_constants.append( \u0026quot;\\tfloat\\t\\t\\t\\t\\t\u0026quot; );\rcpu_constants.append( property-\u0026gt;name );\rif ( property-\u0026gt;data_index != 0xffffffff ) {\rfloat value = 0.0f;\rgetData( data_buffer, property-\u0026gt;data_index, value );\rconstants_ui.append( \u0026quot;\\t\\t\\t\\t= %ff\u0026quot;, value );\rcpu_constants.append( \u0026quot;\\t\\t\\t\\t= %ff\u0026quot;, value );\r}\rconstants_ui.append( \u0026quot;;\\n\u0026quot; );\rcpu_constants.append( \u0026quot;;\\n\u0026quot; );\rconstants_ui_method.append(\u0026quot;\\t\\tImGui::InputScalar( \\\u0026quot;\u0026quot;);\rconstants_ui_method.append( property-\u0026gt;ui_name );\rconstants_ui_method.append( \u0026quot;\\\u0026quot;, ImGuiDataType_Float, \u0026amp;\u0026quot; );\rconstants_ui_method.append( property-\u0026gt;name );\rconstants_ui_method.append( \u0026quot;);\\n\u0026quot; );\r// buffer_data-\u0026gt;scale = constantsUI.scale;\rbuffer_class.append(\u0026quot;\\t\\t\\tbuffer_data-\u0026gt;\u0026quot;);\rbuffer_class.append( property-\u0026gt;name );\rbuffer_class.append( \u0026quot; = constantsUI.\u0026quot; );\rbuffer_class.append( property-\u0026gt;name );\rbuffer_class.append( \u0026quot;;\\n\u0026quot; );\r++gpu_struct_alignment;\rbreak;\r}\r}\r}\r// Post-property sections\rconstants_ui.append( \u0026quot;\\n\u0026quot; );\rconstants_ui_method.append( \u0026quot;\\t}\\n\\n\u0026quot; );\rconstants_ui_method.append( \u0026quot;\\tvoid reflectUI() {\\n\\t\\tImGui::Begin( \\\u0026quot;LocalConstants\\\u0026quot; );\\n\u0026quot; );\rconstants_ui_method.append( \u0026quot;\\t\\treflectMembers();\\n\\t\\tImGui::End();\\n\\t}\\n\\n\u0026quot; );\rconstants_ui_method.append( \u0026quot;}; // struct LocalConstantsUI\\n\\n\u0026quot; );\r// Add tail padding data\ruint32_t tail_padding_size = 4 - (gpu_struct_alignment % 4);\rcpu_constants.append( \u0026quot;\\tfloat\\t\\t\\t\\t\\tpad_tail[%u];\\n\\n\u0026quot;, tail_padding_size );\rcpu_constants.append( \u0026quot;}; // struct LocalConstants\\n\\n\u0026quot; );\rbuffer_class.append( \u0026quot;\\t\\t\\tdevice.unmap_buffer( map_parameters );\\n\\t\\t}\\n\\t}\\n}; // struct LocalConstantBuffer\\n\\n\u0026quot; );\rfwrite( constants_ui.data, constants_ui.current_size, 1, output_file );\rfwrite( constants_ui_method.data, constants_ui_method.current_size, 1, output_file );\rfwrite( cpu_constants.data, cpu_constants.current_size, 1, output_file );\rfwrite( buffer_class.data, buffer_class.current_size, 1, output_file );\r// End\rfprintf( output_file, \u0026quot;} // namespace \u0026quot; );\rfwrite( shader.name.text, shader.name.length, 1, output_file );\rfprintf( output_file, \u0026quot;\\n\\n\u0026quot; );\rfclose( output_file );\rThis piece of code will generate a constant buffer from the properties:\n// GLSL\r//\rstatic void generateConstantsCode( const Shader\u0026amp; shader, StringBuffer\u0026amp; out_buffer ) {\rif ( !shader.properties.size() ) {\rreturn;\r}\r// Add the local constants into the code.\rout_buffer.append( \u0026quot;\\n\\t\\tlayout (std140, binding=7) uniform LocalConstants {\\n\\n\u0026quot; );\r// For GPU the struct must be 16 bytes aligned. Track alignment\ruint32_t gpu_struct_alignment = 0;\rconst std::vector\u0026lt;Property*\u0026gt;\u0026amp; properties = shader.properties;\rfor ( size_t i = 0; i \u0026lt; shader.properties.size(); i++ ) {\rhfx::Property* property = shader.properties[i];\rswitch ( property-\u0026gt;type ) {\rcase Property::Float:\r{\rout_buffer.append( \u0026quot;\\t\\t\\tfloat\\t\\t\\t\\t\\t\u0026quot; );\rout_buffer.append( property-\u0026gt;name );\rout_buffer.append( \u0026quot;;\\n\u0026quot; );\r++gpu_struct_alignment;\rbreak;\r}\r}\r}\ruint32_t tail_padding_size = 4 - (gpu_struct_alignment % 4);\rout_buffer.append( \u0026quot;\\t\\t\\tfloat\\t\\t\\t\\t\\tpad_tail[%u];\\n\\n\u0026quot;, tail_padding_size );\rout_buffer.append( \u0026quot;\\t\\t} local_constants;\\n\\n\u0026quot; );\r}\rExpert constants: an interesting problem A problem many times surfaces is that the material interface does not correspond to the buffer sent to the GPU, because the programmers will do the following:\n Add system constants, that don\u0026rsquo;t need a UI Change order of the constants Change constants to more GPU friendly values, calculating some stuff on the CPU Pack constants into smaller ones  This is an interesting topic and I\u0026rsquo;ll cover it in another article, but a simple solution would be to add a mapping between the GPU constants and the UI, so that we can separate the UI constants from the GPU ones.\nI\u0026rsquo;ll give a brief example but it would be too much for this article and will not be included in the source code.\nBasically we are trying to create a mapping between the material interface:\n// C++\rstruct LocalConstantsUI {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rvoid reflectMembers() {\rImGui::InputScalar( \u0026quot;Scale\u0026quot;, ImGuiDataType_Float, \u0026amp;scale);\rImGui::InputScalar( \u0026quot;Modulo\u0026quot;, ImGuiDataType_Float, \u0026amp;modulo);\r}\rvoid reflectUI() {\rImGui::Begin( \u0026quot;LocalConstants\u0026quot; );\rreflectMembers();\rImGui::End();\r}\r}; // struct LocalConstantsUI\rAnd the GPU constants:\n// C++\rstruct LocalConstants {\rfloat scale = 32.000000f;\rfloat modulo = 2.000000f;\rfloat pad_tail[2];\r}; // struct LocalConstants\rWe could enhance HFX with some syntax to mark the derivate properties and just add the system ones in an explicit buffer layout, and add a layout section in the HFX:\n// HFX\rproperties {\r// Using Unity ShaderLab syntax:\rscale(\u0026quot;Scale\u0026quot;, Range(0.0, 100.0)) = 100.0\rmodulo(\u0026quot;Modulo\u0026quot;, Float) = 2.0\r}\rlayout {\rCBuffer LocalConstants {\rfloat4x4 world_view_projection; // 'System' variable\rfloat scale01 = (scale); // Silly normalized version of scale interface property\rfloat modulo;\rfloat pad[2];\r}\r}\rwe could completely override the automatic constant buffer generation from the properties.With this we can:\n Add a system variable like world_view_projection Flag the property scale as UI only, by saying that property scale01 uses it.  I think that with this syntax both artists and programmers can be happy together!I will try to work on this on a later article.\nResource bindings: Vulkan and D3D12 mentality As stated multiple times, the shift in mentality is towards the new APIs, and that includes the concept of resource lists.The problem is that we don\u0026rsquo;t want artists to have to handle this kind of things - especially if you want to quickly prototype things!But at the same time, we want programmers to have the possibility to optimize the shaders the artists gave them.What is the solution?Simple: creating an optional resource layout section and automatically generate it if not present, so that artists (and not only) can happily create amazing tech and THEN worry about these details!\nAutomatic Resource Layout The easiest way to handle resource layout is to make them SIMPLE. Remember the K.I.S.S. principle.In this case it means that we can create a Resource List for each pass, that will contain:\n One constant/uniform buffer containing all the properties All the textures used by the shader  How can we achieve that ?\nWe already saw how we can generate the constant buffer from the properties in the previous section. For textures we have a couple of options.\nList of Textures Being in automation land, there are 2 ways to add texture dependencies:\n Use reflection mechanism from the target shader language Parse identifiers in the current finalized shader  For the sake of fun we will look into the second of course!If we go back to void declarationGlsl( Parser* parser ), we can add a new method to parse the keyword:\n// Parse hash for includes and defines\rif ( token.type == Token::Token_Hash ) {\r// Get next token and check which directive is\rnextToken( parser-\u0026gt;lexer, token );\rdirectiveIdentifier( parser, token, code_fragment );\r}\relse if ( token.type == Token::Token_Identifier ) { \u0026lt;------------ New Code!\r// Parse uniforms to add resource dependencies if not explicit in the HFX file.\rif ( expectKeyword( token.text, 7, \u0026quot;uniform\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, token );\runiformIdentifier( parser, token, code_fragment );\r}\r}\rIn this way it will search for the identifier uniform and search for the other identifiers. This is GLSL centric of course.\ninline void uniformIdentifier( Parser* parser, const Token\u0026amp; token, CodeFragment\u0026amp; code_fragment ) {\rfor ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase 'i':\r{\rif ( expectKeyword( token.text, 7, \u0026quot;image2D\u0026quot; ) ) {\r// Advance to next token to get the name\rToken name_token;\rnextToken( parser-\u0026gt;lexer, name_token );\rCodeFragment::Resource resource = { hydra::graphics::ResourceType::TextureRW, name_token.text };\rcode_fragment.resources.emplace_back( resource );\r}\rbreak;\r}\rcase 's':\r{\rif ( expectKeyword( token.text, 9, \u0026quot;sampler2D\u0026quot; ) ) {\r// Advance to next token to get the name\rToken name_token;\rnextToken( parser-\u0026gt;lexer, name_token );\rCodeFragment::Resource resource = { hydra::graphics::ResourceType::Texture, name_token.text };\rcode_fragment.resources.emplace_back( resource );\r}\rbreak;\r}\r}\r}\r}\rShould be pretty straight-forward: if you find the identifier for texture, add a resource dependency with type and name to the current code fragment!Is this the ideal solution ?Probably not.But I wanted to show what we can achieve once we have fun with parsing, including the understanding on when to say NO to it!Manual Resource Layout Now that the effect can work without too much programmer time, it is time to give back to programmers the control they want.In the previous paragraph about Expert Constants we talked about adding a new section, called layout.In this section we can specify the resource list for each pass manually, and later on in the pass we can reference this lists as used by the pass.\nGoing on a more complete solution, layouts should be included and merged when including other HFX files.This is something we want and we\u0026rsquo;ll look in another post, we can start simple by defining something local:\n// HFX\r//\r// For the developer\rlayout {\rlist LocalCompute {\rcbuffer LocalConstants;\rtexture2Drw(rgba8) destination_texture;\r}\rlist Local {\rtexture2D input_texture;\r}\r}\rThis is a rather simple layout, but let\u0026rsquo;s see it.First of all, for each \u0026lsquo;list\u0026rsquo; keyword we define a single list with a unique name.With that, we can reference in the pass which list to use.The code that does the parsing is (at this point) pretty straight-forward, both in void declarationResourceList( Parser* parser, ResourceList\u0026amp; resource_list ) and void resourceBindingIdentifier( Parser* parser, const Token\u0026amp; token, ResourceBinding\u0026amp; binding ).I will not go over it, but basically it will parse the resource lists and add them to the shader.The parsing itself will read the text and create the ResourceSetLayoutCreation::Binding and add it to the list of the resources.We then add a new identifier in the pass to choose which resource list to be used:\n// HFX\r//\rpass FillTexture {\rresources = LocalCompute, ...\rdispatch = 32, 32, 1\rrender_pass = compute\rcompute = ComputeTest\r}\rpass ToScreen {\rresources = Local\rrender_pass = fullscreen\rvertex = ToScreen\rfragment = ToScreen\r}\rThe parsing will happen in void declarationPassResources( Parser* parser, Pass\u0026amp; pass ).\nAdding Resource Layout data to binary HFX So after this amazing journey we are ready to embed those informations into the BHFX and use it right away into the rendering API.\nThe big difference is if the hfx file contains a layout section.If it is not present, then all the informations will be gathered automatically and will be added with the writeAutomaticResourcesLayout method.\nFirst we will add the LocalConstant buffer created from the properties:\nstatic void writeAutomaticResourcesLayout( const hfx::Pass\u0026amp; pass, StringBuffer\u0026amp; pass_buffer, uint32_t\u0026amp; pass_offset ) {\rusing namespace hydra::graphics;\r// Add the local constant buffer obtained from all the properties in the layout.\rhydra::graphics::ResourceSetLayoutCreation::Binding binding = { hydra::graphics::ResourceType::Constants, 0, 1, \u0026quot;LocalConstants\u0026quot; };\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding );\rThen we will cycle through all the shader stages and write the resources into the memory:\n for ( size_t s = 0; s \u0026lt; pass.shader_stages.size(); ++s ) {\rconst Pass::ShaderStage shader_stage = pass.shader_stages[s];\rfor ( size_t p = 0; p \u0026lt; shader_stage.code-\u0026gt;resources.size(); p++ ) {\rconst hfx::CodeFragment::Resource\u0026amp; resource = shader_stage.code-\u0026gt;resources[p];\rswitch ( resource.type ) {\rcase ResourceType::Texture:\r{\rcopy( resource.name, binding.name, 32 );\rbinding.type = hydra::graphics::ResourceType::Texture;\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding ) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding );\rbreak;\r}\rcase ResourceType::TextureRW:\r{\rcopy( resource.name, binding.name, 32 );\rbinding.type = hydra::graphics::ResourceType::TextureRW;\rpass_buffer.append( (void*)\u0026amp;binding, sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding ) );\rpass_offset += sizeof( hydra::graphics::ResourceSetLayoutCreation::Binding );\rbreak;\r}\r}\r}\r}\r}\rIf instead there is a layout section, the method writeResourcesLayout is called and will be pretty straight-forward:\nstatic void writeResourcesLayout( const hfx::Pass\u0026amp; pass, StringBuffer\u0026amp; pass_buffer, uint32_t\u0026amp; pass_offset ) {\rusing namespace hydra::graphics;\rfor ( size_t r = 0; r \u0026lt; pass.resource_lists.size(); ++r ) {\rconst ResourceList* resource_list = pass.resource_lists[r];\rconst uint32_t resources_count = (uint32_t)resource_list-\u0026gt;resources.size();\rpass_buffer.append( (void*)resource_list-\u0026gt;resources.data(), sizeof(ResourceBinding) * resources_count );\rpass_offset += sizeof( ResourceBinding ) * resources_count;\r}\r}\rAnd this will be put at the end of the current pass section:\npass_buffer.append( (void*)\u0026amp;pass_header, sizeof( ShaderEffectFile::PassHeader ) );\rpass_buffer.append( shader_offset_buffer );\rpass_buffer.append( code_buffer );\rif ( automatic_layout ) {\rwriteAutomaticResourcesLayout( pass, pass_buffer, pass_offset );\r}\relse {\rwriteResourcesLayout( pass, pass_buffer, pass_offset );\r}\rConclusions and what\u0026rsquo;s next We arrived at the end of this article, and we started seeing how we can use HFX as a more complete language to embed different rendering features.We saw how to embed shader code and resource lists so that the rendering API can create everything without hard-coded generation of resources. This also showed when it was useful to create data instead of code.On the contrary, the UI and the Constants are generated in a new header file - thus code generation.There are pros and cons to both approaches, but I hope that knowing how to generate code and create a custom language will let you play with the concepts and explore your own needs.\nAs next steps, there are some questions opened: how to reload shaders ? Can I add new material properties without recompiling code ?\nWe will also see a simple implementation of a frame-graph, that I use since my years in Codemasters and in my indie project. This will be much more data-driven than code-generated, but again, the purpose of these articles is to explore the concepts and understanding when to use what.\nAs always please comment, feedback, share!\nThanks for reading! Gabriel\n","date":1568176933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568176933,"objectID":"981a49821ad5e37fd95cd7c0c273e7a6","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_2/","publishdate":"2019-09-11T00:42:13-04:00","relpermalink":"/post/writing_shader_effect_language_2/","section":"post","summary":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In Part 1 of this series we created a simple language to work as \u0026lsquo;shader effect\u0026rsquo; - a shader language superset to make our life easier, by adding missing features.The fact that there is not an industry standard for a shader effect language leads to either hand-crafted (and secret) languages, or to hardcoded permutations, or to other gray-area solutions.","tags":[],"title":"Writing a Shader Effect Language Part 2","type":"post"},{"authors":[],"categories":[],"content":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In this article we will create a simple language that can encapsulate shader code (called code fragments) and output different files for each fragment.This is the initial step to switch from an engine that loads single files for each shader stage (vertex, fragment, compute, \u0026hellip;) to one that uses an effect file that contains more than one shader.\nWe will start by motivation, then will define the language itself (very simple), then we will look at the Parser and last the Code Generator.\nHave a good read!\nMotivation In the incredible quest of data-driven rendering, after we defeated the dragon of code generation another multiple headed dragon arises: an hydra! We have different options here: be the brave warrior in shiny armor that tries to cut all the heads of the hydra, built some machines that can fight for us and send them, or both built the machines AND fight.\nOur code is invaluable, like our energies fighting the hydra. We need to carefully balance them and see how can we use for the BEST.\nWriting manual code is good, it is generally what is done, but it is slow and error prone. Going data-driven can be fast, but can give you a sense of losing control (not personally, but I heard few people saying that). Only generating code can quickly become a recipe for disaster: so many particular use cases need attention, that the code could be come a different kind of mess.\nWe will try to go down the route of code generation mixed with data-driven. As I wrote in my previous articles, it is a fine line and can be good to know when to go in which direction!\nI will divide the article in 2 parts. The first part (this one) will contain the new Shader Code Generator to generate shader permutations and add include support to GLSL. The second will require a low-level rendering library and will show Code Generation of more CPU areas of Rendering, the real goal of all these articles!\nThe code is available here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nEffect file structure Looking at effects, the first thing to do is to define a file that will represent our shaders. My choice is to create a simple language to embed shaders code and generate the CPU code necessary to render it.\nWhy not using Json ? While it is an amazing data-format, I still want a bigger degree of control of what to parse and what to generate. The decision is based on the fact that by writing a parser for the language, I can automate some code-generation that would be more intricate with Json. Also, this series itself is a personal exploration on the topic, so using Json was not an option for this level of complexity.\nThe HFX Format HFX (Hydra Effects) is a new language we will define to write out shaders. The first iteration will be barebone - it will simply be a shader permutation generator - but it will be the foundation to extensions that will allow us to write CPU rendering code that we want to automate.\nIn defining the format, there will be few keywords that will be defined, but the general architecture will make straightforward to copy-paste shader code fragments from any language into the HFX language. We will use the following keywords (and concepts).\nShader The root of a shader effect. It will contain everything we are writing.\nGlsl/Hlsl These will define the actual shader code, enclosed fragments. Fragments can be composed and reused. For Glsl in particular, code fragments needs to be embedded in defines for each stage. More on that later.\nPass, Technique, Variant This is the central part for the effects to work. I\u0026rsquo;ve researched a bit, between Microsoft effects, Unity effects, Godot and Bungie and the concepts are very similar, but they seem to differ a little and also each implementation becomes very engine-specific of course.The presentation by Bungie is amazing and their system is by far the more extensive and complex, we will work on a much simpler shader effect system.Let\u0026rsquo;s define a pass as a combination of shader code for at least one stage of the shader pipeline. For example a single compute shader or a couple vertex-fragment shader.\nVariants and techniques are loose concept to help separating shader paths. For example a variant could be a different post-process shader, like different implementations of SSAO.\nA technique could be a whole set of passes that target a specific platform.\nNot having my mind set on those still, I will omit them for now, as they are concepts that are less central than the code generation, and can be very subjective opinion-wise. Possibly I\u0026rsquo;ll get them in part 2.\nProperties Final piece of the puzzle. This will define the resources used by the shader effect on a per-effect level. Keeping an eye on the newer rendering APIs (DX12 and Vulkan) this defines also the layout of the resources and how they are used. Possibly the most intense part from an automation possibility (and thus code-generation). We will define this in part 2 of this article.\nHigh level workflow From a high level perspective what will happen in all this code is enclosed in this code:\ntext = ReadEntireFileIntoMemory( \u0026quot;..\\\\data\\\\SimpleFullscreen.hfx\u0026quot;, nullptr );\rinitLexer( \u0026amp;lexer, (char*)text );\rhfx::Parser effect_parser;\rhfx::initParser( \u0026amp;effect_parser, \u0026amp;lexer );\rhfx::generateAST( \u0026amp;effect_parser );\rhfx::CodeGenerator hfx_code_generator;\rhfx::initCodeGenerator( \u0026amp;hfx_code_generator, \u0026amp;effect_parser, 4096 );\rhfx::generateShaderPermutations( \u0026amp;hfx_code_generator, \u0026quot;..\\\\data\\\\\u0026quot; );\rWe separated the Lexer from the Parser so we can reuse the lexer functionalities, thus we can reuse it from the previous example (parsing the HydraDataFormat files).Then we initialize the Parser and generate the AST. This will save all the passes and code fragments we defined in the HFX file.Finally we will get the parsing informations and give them to the code generator, that will write out the files for each pass and stage.\nLet\u0026rsquo;s dig into the example!\nParser: welcome HFX! In most rendering-API (OpenGL, Vulkan, Direct3D12, \u0026hellip;) shaders are compiled by compiling the individual stages (vertex, fragment, compute, geometry, \u0026hellip;) and in some APIs (especially the newer ones) are compiled into a Shader State.\nAs first step of this shader language, single shader files will be created by the shader generation method in our code.\nWe will define a simple fullscreen HFX with code fragments and passes.\nFirst, we define the root shader (SimpleFullscreen.hfx, under folder \u0026lsquo;data\u0026rsquo;):\nshader SimpleFullscreen {\rThis is simply the container for all the code and passes that will define the shader effect.\nNow we need some actual code, so we can define a shader fragment.The keyword used in our language is glsl followed by a name and an open brace:\nglsl ToScreen {\rThis will define a code fragment named ToScreen, that can be referenced from the passes.Next we use a glsl trick to signal our parser to use includes:\n#pragma include \u0026quot;Platform.h\u0026quot;\rThis #pragma is actually ignored by the compiler, but will be used by the parser to actually add the include!BEWARE: this code will be included in BOTH vertex and fragment program!Anything outside of the VERTEX/FRAGMENT/COMPUTE macros will be, and this is done on purpose, like defining an interpolator struct only once or for common includes.\nNext we define the vertex program.BEWARE: vertex only code must be enclosed in VERTEX define!\n#if defined VERTEX\rout vec4 vTexCoord;\rvoid main() {\rvTexCoord.xy = vec2((gl_VertexID \u0026lt;\u0026lt; 1) \u0026amp; 2, gl_VertexID \u0026amp; 2);\rvTexCoord.zw = vTexCoord.xy;\rgl_Position = vec4(vTexCoord.xy * 2.0f + -1.0f, 0.0f, 1.0f);\r}\r#endif // VERTEX\rThis code is a simple fullscreen triangle that does not require any vertex buffer, but uses the vertex id to draw. Nothing fancy.\nNext is the fragment program, and again enclosed in FRAGMENT define:\n#if defined FRAGMENT\rin vec4 vTexCoord;\rout vec4 outColor;\rlayout(binding=0) uniform sampler2D input_texture;\rvoid main() {\rvec3 color = texture2D(input_texture, vTexCoord.xy).xyz;\routColor = vec4(color, 1);\r}\r#endif // FRAGMENT\r} // glsl ToScreen\rThis code simply reads a texture and outputs it to the screen.\nWe defined the code fragment ToScreen, containing both a vertex and a fragment program, and now we can actually generate the permutation that we need.The code for this in our effect file is:\npass ToScreen {\rvertex = ToScreen\rfragment = ToScreen\r}\rWe are simply defining a pass with the vertex and fragment program defined in the ToScreen code fragment (yes I don\u0026rsquo;t like this term too).\nRunning the code generator on this simple effect file will generate the two files ToScreen.vert and ToScreen.frag.\nThese can be read directly into your favourite OpenGL renderer and used as is!\nThe Parser Now that we have defined the effect and we know what is the outcome of generating code from the effect file, let\u0026rsquo;s look into the different component of the parser and code generator needed.\nBy design, we chose the Lexer to know nothing about the language, so that we can use it between different languages. The entry point to parse the effect is the method generateAST:\nvoid generateAST( Parser* parser ) {\r// Read source text until the end.\r// The main body can be a list of declarations.\rbool parsing = true;\rwhile ( parsing ) {\rToken token;\rnextToken( parser-\u0026gt;lexer, token );\rswitch ( token.type ) {\rcase Token::Token_Identifier:\r{\ridentifier( parser, token );\rbreak;\r}\rcase Token::Type::Token_EndOfStream:\r{\rparsing = false;\rbreak;\r}\r}\r}\r}\rThis code simply process the fileâ€Š-â€Šusing the lexerâ€Š-â€Šuntil the end of it, and reads only identifiers.It is the same as the previous article and the previous parser. What changes drastically is the identifier method!We will have 3 different set of identifiers, usable in different parts of the HFX file:\n Main identifiers, \u0026lsquo;shader\u0026rsquo;, \u0026lsquo;glsl\u0026rsquo;, \u0026lsquo;pass\u0026rsquo; Pass identifiers, \u0026lsquo;compute\u0026rsquo;, \u0026lsquo;vertex\u0026rsquo;, \u0026lsquo;fragment\u0026rsquo; Directive identifiers, \u0026lsquo;if defined\u0026rsquo;, \u0026lsquo;pragma include\u0026rsquo;, \u0026lsquo;endif\u0026rsquo;  Let\u0026rsquo;s have a look at the code for parsing the main identifiers:\ninline void identifier( Parser* parser, const Token\u0026amp; token ) {\r// Scan the name to know which for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase 's':\r{\rif ( expectKeyword( token.text, 6, \u0026quot;shader\u0026quot; ) ) {\rdeclarationShader( parser );\rreturn;\r}\rbreak;\r}\rcase 'g':\r{\rif ( expectKeyword( token.text, 4, \u0026quot;glsl\u0026quot; ) ) {\rdeclarationGlsl( parser );\rreturn;\r}\rbreak;\r}\rcase 'p':\r{\rif ( expectKeyword( token.text, 4, \u0026quot;pass\u0026quot; ) ) {\rdeclarationPass( parser );\rreturn;\r}\rbreak;\r}\r}\r}\r}\rThis code simply defers the parsing of a particular identifier using the declaration method corresponding to the identifier. We will look into detail on each method.\nParsing \u0026lsquo;shader\u0026rsquo; We are parsing now the following part from the HFX file:\n// HFX\rshader SimpleFullscreen {\rThis is the entry point of the effect itself.What should the parser do here ?Simply iterate through the main identifiers, \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;.Technically I could have separated the methods to have one with parsing shader only and the others parsing \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;, but did not want to complicate the code further.\nLet\u0026rsquo;s look at how we parse the identifier \u0026lsquo;shader\u0026rsquo;:\n// C++\rinline void declarationShader( Parser* parser ) {\r// Parse name\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r// Cache name string\rStringRef name = token.text;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r}\rwhile ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) {\ridentifier( parser, token );\r}\r}\rAs the previous article\u0026rsquo;s code, this will get the tokens from the lexer and generate data if the syntax is correct.When we enter the method the Lexer will be just at the beginning of the name (SimpleFullscreen), so the code will parse the name, the open brace, and parse everything else until it encounter the close brace.\nThe method identifier will parse also identifiers \u0026lsquo;glsl\u0026rsquo; and \u0026lsquo;pass\u0026rsquo;.\nParsing \u0026lsquo;glsl\u0026rsquo; This is the most complex parsing in the code.I will put both the HFX part and C++ code so hopefully it will be clearer what the parser is doing and why.\nAs a refresh and reference, this is the code fragment ToScreen defined in SimpleFullscreen.hfx:\n// HFX\rglsl ToScreen {\r#pragma include \u0026quot;Platform.h\u0026quot;\r#if defined VERTEX\rout vec4 vTexCoord;\rvoid main() {\rvTexCoord.xy = vec2((gl_VertexID \u0026lt;\u0026lt; 1) \u0026amp; 2, gl_VertexID \u0026amp; 2);\rvTexCoord.zw = vTexCoord.xy;\rgl_Position = vec4(vTexCoord.xy * 2.0f + -1.0f, 0.0f, 1.0f);\r}\r#endif // VERTEX\r#if defined FRAGMENT\rin vec4 vTexCoord;\rout vec4 outColor;\rlayout(binding=0) uniform sampler2D input_texture;\rvoid main() {\rvec3 color = texture2D(input_texture, vTexCoord.xy).xyz;\routColor = vec4(1, 1, 0, 1);\routColor = vec4(color, 1);\r}\r#endif // FRAGMENT\r}\rLet\u0026rsquo;s start from the beginning.When the parser finds the \u0026lsquo;glsl\u0026rsquo; keyword in the identifier method:\n// C++\rcase 'g':\r{\rif ( expectKeyword( token.text, 4, \u0026quot;glsl\u0026quot; ) ) {\rdeclarationGlsl( parser );\rreturn;\r}\rbreak;\r}\rIt calls the method void declarationGlsl( Parser parser )*.\nThe lexer reading the HFX is after the glsl keyword when entering the method, just before the ToScreen identifier:\n// HFX\rglsl (Here!)ToScreen {\rLet\u0026rsquo;s see the C++ code step by step.First parsing the name \u0026lsquo;ToScreen\u0026rsquo;:\n// C++\rinline void declarationGlsl( Parser* parser ) {\r// Parse name\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\ras seen in other methods as well.We are defining a new code fragment, thus we need to initialize it. There is tracking of the #ifdef depths to manage when some code must be included in a code fragment and when not:\n CodeFragment code_fragment = {};\r// Cache name string\rcode_fragment.name = token.text;\rfor ( size_t i = 0; i \u0026lt; CodeFragment::Count; i++ ) {\rcode_fragment.stage_ifdef_depth[i] = 0xffffffff;\r}\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r}\rNext is simply arriving at the first token that contains all the glsl code:\n // Advance token and cache the starting point of the code.\rnextToken( parser-\u0026gt;lexer, token );\rcode_fragment.code = token.text;\rAnd now some more parsing craftmanship.We cannot use anymore the simple check to end parsing when encountering a closed brace, because there can be different structs defined that will break that mechanism.Instead we track the number of open braces and when we close the last one, we consider finished the parsing of the code fragment!\n uint32_t open_braces = 1;\r// Scan until close brace token\rwhile ( open_braces ) {\rif ( token.type == Token::Token_OpenBrace )\r++open_braces;\relse if ( token.type == Token::Token_CloseBrace )\r--open_braces;\rThe only token that we care inside the code fragment is the hash, signalling either an include or a define, used for separating per-stage code.The parsing of the hash token will be done inside the directiveIdentifier method:\n // Parse hash for includes and defines\rif ( token.type == Token::Token_Hash ) {\r// Get next token and check which directive is\rnextToken( parser-\u0026gt;lexer, token );\rdirectiveIdentifier( parser, token, code_fragment );\r}\rBefore diving deep into the directive identifiers, let\u0026rsquo;s finish the main parsing routine.We advance to the next token until we close all the braces, and then save the text length of all the code fragment:\n nextToken( parser-\u0026gt;lexer, token );\r}\r// Calculate code string length\rcode_fragment.code.length = token.text.text - code_fragment.code.text;\rFinal step is to save the newly parsed code fragment into the parser data:\n parser-\u0026gt;code_fragments.emplace_back( code_fragment );\r}\rWe can now dive deep into the parsing of directives, namely #if defined, #pragma include and #endif.\nParsing \u0026lsquo;#if defined\u0026rsquo; When we encounter the Hash token within the glsl part, we need to parse further to understand the other keywords.#if defined is the most important directive for us, because it will tell the parser which shader stage we are parsing currently and thus where to direct the text!It starts from a common/shared stage, for shared code, and when encounters a #if defined it can signal a stage specific code.Namely when parsing the following line in HFX:\n// HFX\r#(Here!)if defined VERTEX\rThe parser needs to check 2 other identifiers. Remember that the parser is currently AFTER the Hash token, as beautifully written in the previous snippet!Let\u0026rsquo;s look at the code:\n// C++\rinline void directiveIdentifier( Parser* parser, const Token\u0026amp; token, CodeFragment\u0026amp; code_fragment ) {\rToken new_token;\rfor ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase 'i':\r{\r// Search for the pattern 'if defined'\rif ( expectKeyword( token.text, 2, \u0026quot;if\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rif ( expectKeyword( new_token.text, 7, \u0026quot;defined\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\r// Use 0 as not set value for the ifdef depth.\r++code_fragment.ifdef_depth;\rif ( expectKeyword( new_token.text, 6, \u0026quot;VERTEX\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Vertex;\r}\relse if ( expectKeyword( new_token.text, 8, \u0026quot;FRAGMENT\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Fragment;\r}\relse if ( expectKeyword( new_token.text, 7, \u0026quot;COMPUTE\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Compute;\r}\r}\rreturn;\r}\rbreak;\r}\rLet\u0026rsquo;s dissect this code!\nStarting from the current token, just after the #(Hash), we need to check the correct composition of the keywords.We expect \u0026lsquo;if\u0026rsquo;, and then if found we go to the next token:\nif ( expectKeyword( token.text, 2, \u0026quot;if\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rWe search for the \u0026lsquo;defined\u0026rsquo; identifier and if found we go to the next identifier:\nif ( expectKeyword( new_token.text, 7, \u0026quot;defined\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rThe parser is currently here:\n#if defined (Here!)VERTEX\rAnd thus the last step is to check which shader stage is currently starting. This is done here:\nif ( expectKeyword( new_token.text, 6, \u0026quot;VERTEX\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Vertex;\r}\rIn this central piece of code, we set the current stage to Vertex (because we found the keyword \u0026lsquo;VERTEX\u0026rsquo;) and we save the current ifdef depth.Why that ? Because when we will parse #endif, we will do the same for the open/close braces depth in the main glsl parser: we want to be sure that the defines are paired correctly and we are saving the per-stage code in the correct way!This will be more clear when we see the #endif parsing.\nMoving on, we will do the same for all the other keywords (\u0026lsquo;FRAGMENT\u0026rsquo; and \u0026lsquo;COMPUTE\u0026rsquo; for now):\nelse if ( expectKeyword( new_token.text, 8, \u0026quot;FRAGMENT\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Fragment;\r}\relse if ( expectKeyword( new_token.text, 7, \u0026quot;COMPUTE\u0026quot; ) ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = code_fragment.ifdef_depth;\rcode_fragment.current_stage = CodeFragment::Compute;\r}\rAnd the parsing of #if defined is over!Parsing \u0026lsquo;#pragma include\u0026rsquo; In HFX we are parsing the following:\n// HFX\r#pragma include \u0026quot;Platform.h\u0026quot;\rWith the following code (inside directiveIdentifier method):\n// C++\rcase 'p':\r{\rif ( expectKeyword( token.text, 6, \u0026quot;pragma\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rif ( expectKeyword( new_token.text, 7, \u0026quot;include\u0026quot; ) ) {\rnextToken( parser-\u0026gt;lexer, new_token );\rcode_fragment.includes.emplace_back( new_token.text );\rcode_fragment.includes_stage.emplace_back( code_fragment.current_stage );\r}\rreturn;\r}\rbreak;\r}\rThis is simply saving the filename after the include, that being surrounded by \u0026quot;\u0026rdquo; is classified as string, and is using the current stage to know which stage should include that file!Parsing \u0026lsquo;#endif\u0026rsquo; Final part is the #endif identifier:\ncase 'e':\r{\rif ( expectKeyword( token.text, 5, \u0026quot;endif\u0026quot; ) ) {\rif ( code_fragment.stage_ifdef_depth[CodeFragment::Vertex] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Vertex] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\relse if ( code_fragment.stage_ifdef_depth[CodeFragment::Fragment] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Fragment] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\relse if ( code_fragment.stage_ifdef_depth[CodeFragment::Compute] == code_fragment.ifdef_depth ) {\rcode_fragment.stage_ifdef_depth[CodeFragment::Compute] = 0xffffffff;\rcode_fragment.current_stage = CodeFragment::Common;\r}\r--code_fragment.ifdef_depth;\rreturn;\r}\rbreak;\r}\rThis is mirroring the #if defined and simply goes back to set the current stage to common/shared and reset the per-stage ifdef depth.\nWe can now proceed to the final part of the parsing, the passes!This is the glue to generate the different files from the code fragments.\nParsing \u0026lsquo;pass\u0026rsquo; Reading the HFX file, we are now in the final part of the file:\n// HFX\rpass ToScreen {\rvertex = ToScreen\rfragment = ToScreen\r}\rA pass is simply a collection of code fragments associated with each shader stage (vertex, fragment, compute).When we parsed the fragments, we saved them in the parser to be retrieved.\nTo refresh our memory, this is the actual Pass struct in C++:\n// C++\rstruct Pass {\rStringRef name;\rconst CodeFragment* vs = nullptr;\rconst CodeFragment* fs = nullptr;\rconst CodeFragment* cs = nullptr;\r}; // struct Pass\rGoing back to the main directive method, we call the declarationPass method when we encounter the \u0026lsquo;pass\u0026rsquo; identifier.We will parse the following line:\n// HFX\rpass ToScreen {\rWith the following code (similar to everything else, it should be easier to read now):\n// C++\rinline void declarationPass( Parser* parser ) {\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\rPass pass = {};\r// Cache name string\rpass.name = token.text;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) {\rreturn;\r}\rAfter we saved the pass name we can start reading the individual stages using the passIdentifier method:\n while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) {\rpassIdentifier( parser, token, pass );\r}\rAnd then save the newly parsed pass.\n parser-\u0026gt;passes.emplace_back( pass );\r}\rFor each identifier now, we will check which stage we are parsing.Currently we are here, after the open brace and all the whitespace:\n// HFX\rpass ToScreen {\r(Here!)vertex = ToScreen\rfragment = ToScreen\r}\rWhat is next is thus checking the identifier and filling the corresponding shader stage of the pass.I will post all the code of the method, because is similar to most code we seen and should be straightforward:\n// C++\rinline void passIdentifier( Parser* parser, const Token\u0026amp; token, Pass\u0026amp; pass ) {\r// Scan the name to know which stage we are parsing for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) {\rchar c = *(token.text.text + i);\rswitch ( c ) {\rcase 'c':\r{\rif ( expectKeyword( token.text, 7, \u0026quot;compute\u0026quot;) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.cs );\rreturn;\r}\rbreak;\r}\rcase 'v':\r{\rif ( expectKeyword( token.text, 6, \u0026quot;vertex\u0026quot; ) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.vs );\rreturn;\r}\rbreak;\r}\rcase 'f':\r{\rif ( expectKeyword( token.text, 8, \u0026quot;fragment\u0026quot; ) ) {\rdeclarationShaderStage( parser, \u0026amp;pass.fs );\rreturn;\r}\rbreak;\r}\r}\r}\r}\rThe real \u0026lsquo;magic\u0026rsquo; here is the \u0026lsquo;declarationShaderStage\u0026rsquo; method.This method parses the couple \u0026lsquo;identifier\u0026rsquo; \u0026lsquo;=\u0026rsquo; \u0026lsquo;identifier\u0026rsquo;, and searches the code fragment with the same name:\ninline void declarationShaderStage( Parser* parser, const CodeFragment** out_fragment ) {\rToken token;\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Equals ) ) {\rreturn;\r}\rif ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) {\rreturn;\r}\r*out_fragment = findCodeFragment( parser, token.text );\r}\rAfter all the stages of the current pass are parsed, we save the pass and finish parsing the file!Shader Permutation Generation The final step of this amazing journey is the simplest, and it is actually to generate the single files we need.In our case another specific class, CodeGenerator, will generate the different files from the parsed HFX file.\nAfter we\u0026rsquo;ve done with the parsing, we can call the generateShaderPermutations method that will generate files for each shader stage in each pass:\nvoid generateShaderPermutations( CodeGenerator* code_generator, const char* path ) {\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_1.clear();\rcode_generator-\u0026gt;string_buffer_2.clear();\r// For each pass and for each pass generate permutation file.\rconst uint32_t pass_count = (uint32_t)code_generator-\u0026gt;parser-\u0026gt;passes.size();\rfor ( uint32_t i = 0; i \u0026lt; pass_count; i++ ) {\r// Create one file for each code fragment\rconst Pass\u0026amp; pass = code_generator-\u0026gt;parser-\u0026gt;passes[i];\rif ( pass.cs ) {\routputCodeFragment( code_generator, path, CodeFragment::Compute, pass.cs );\r}\rif ( pass.fs ) {\routputCodeFragment( code_generator, path, CodeFragment::Fragment, pass.fs );\r}\rif ( pass.vs ) {\routputCodeFragment( code_generator, path, CodeFragment::Vertex, pass.vs );\r}\r}\r}\rThe code should be straightforward, and the real action happens into the outputCodeFragment method.Let\u0026rsquo;s have a look at the code.First we define some data, like the file extensions for each shader stage or the defines to compile the code:\n// Additional data to be added to output shaders.\rstatic const char* s_shader_file_extension[CodeFragment::Count] = { \u0026quot;.vert\u0026quot;, \u0026quot;.frag\u0026quot;, \u0026quot;.compute\u0026quot;, \u0026quot;.h\u0026quot; };\rstatic const char* s_shader_stage_defines[CodeFragment::Count] = { \u0026quot;#define VERTEX\\r\\n\u0026quot;, \u0026quot;#define FRAGMENT\\r\\n\u0026quot;, \u0026quot;#define COMPUTE\\r\\n\u0026quot;, \u0026quot;\u0026quot; };\rThen we start to write the file.We will use the string_buffer_0 to dynamically generate the path of the file without allocating memory:\nvoid outputCodeFragment( CodeGenerator* code_generator, const char* path, CodeFragment::Stage stage, const CodeFragment* code_fragment ) {\r// Create file\rFILE* output_file;\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_0.append( path );\rcode_generator-\u0026gt;string_buffer_0.append( code_fragment-\u0026gt;name );\rcode_generator-\u0026gt;string_buffer_0.append( s_shader_file_extension[stage] );\rfopen_s( \u0026amp;output_file, code_generator-\u0026gt;string_buffer_0.data, \u0026quot;wb\u0026quot; );\rif ( !output_file ) {\rprintf( \u0026quot;Error opening file. Aborting. \\n\u0026quot; );\rreturn;\r}\rAnd then use string_buffer_1 to instead generate the actual code into the file.First, and most important, we will add all the includes for this particular stage by opening the file, reading it into memory and adding it into the final code buffer.\nWe will still use string_buffer_0 to generate the path of the file:\n code_generator-\u0026gt;string_buffer_1.clear();\r// Append includes for the current stage.\rfor ( size_t i = 0; i \u0026lt; code_fragment-\u0026gt;includes.size(); i++ ) {\rif ( code_fragment-\u0026gt;includes_stage[i] != stage \u0026amp;\u0026amp; code_fragment-\u0026gt;includes_stage[i] != CodeFragment::Common ) {\rcontinue;\r}\r// Open and read file\rcode_generator-\u0026gt;string_buffer_0.clear();\rcode_generator-\u0026gt;string_buffer_0.append( path );\rcode_generator-\u0026gt;string_buffer_0.append( code_fragment-\u0026gt;includes[i] );\rchar* include_code = ReadEntireFileIntoMemory( code_generator-\u0026gt;string_buffer_0.data, nullptr );\rcode_generator-\u0026gt;string_buffer_1.append( include_code );\rcode_generator-\u0026gt;string_buffer_1.append( \u0026quot;\\r\\n\u0026quot; );\r}\rAfter that is done we can copy the define needed for the current shader stage:\n code_generator-\u0026gt;string_buffer_1.append( \u0026quot;\\t\\t\u0026quot; );\rcode_generator-\u0026gt;string_buffer_1.append( s_shader_stage_defines[stage] );\rAnd finally the actual code:\n code_generator-\u0026gt;string_buffer_1.append( \u0026quot;\\r\\n\\t\\t\u0026quot; );\rcode_generator-\u0026gt;string_buffer_1.append( code_fragment-\u0026gt;code );\rWrite to file and close it and we are done!\n fprintf( output_file, \u0026quot;%s\u0026quot;, code_generator-\u0026gt;string_buffer_1.data );\rfclose( output_file );\r}\rAnd this will generate the shader permutations for each pass with a single file, using the standard GLSL convention for files extensions.\nConclusions and next part We parsed our simple shader language to enhance and embed glsl code fragments into our codebase by generating single files that can be used into any OpenGL based renderer.We also laid out the foundation for a more powerful tool - namely code generation - even though there are some intermediate steps to be taken to arrive there.First of all, we will need a target rendering library (something like the amazing Sokol), so we can specialize our CPU rendering code. I already wrote something like Sokol but with a more Vulkan/D3D12 interface in mind, and I will use that. Still unsure if I will write a specific post on that.\nIn the next article we will add support for the new graphics library and develop the language more to generate code that will manage Constant buffers, automatically creating a CPU-side class, adding UI to edit it in realtime and possibly load/save the values.\nOf course, any feedback/improvements/suggestions on anything related here (article, code, etc) please let me know.\nStay tuned! Gabriel\n","date":1565111055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566011055,"objectID":"2335acd956430a4df447b79b03bb937a","permalink":"https://jorenjoestar.github.io/post/writing_shader_effect_language_1/","publishdate":"2019-08-06T13:04:15-04:00","relpermalink":"/post/writing_shader_effect_language_1/","section":"post","summary":"Overview Data Driven Rendering Series:\n https://jorenjoestar.github.io/post/writing_shader_effect_language_1/ https://jorenjoestar.github.io/post/writing_shader_effect_language_2/ https://jorenjoestar.github.io/post/writing_shader_effect_language_3/ https://jorenjoestar.github.io/post/data_driven_rendering_pipeline/  In this article we will create a simple language that can encapsulate shader code (called code fragments) and output different files for each fragment.This is the initial step to switch from an engine that loads single files for each shader stage (vertex, fragment, compute, \u0026hellip;) to one that uses an effect file that contains more than one shader.\nWe will start by motivation, then will define the language itself (very simple), then we will look at the Parser and last the Code Generator.","tags":[],"title":"Writing a Shader Effect Language Part 1","type":"post"},{"authors":[],"categories":[],"content":"   UI using ImGUI, SDL and the code generated with this article.   Motivation Following my previous article about Flatbuffers and data reflection the quest for Data-Driven Rendering continues!In this article I want to show how to write a very simple code-generator to help you automate writing of code in any language.The code is here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nThere is a balance that constantly needs to be found between code and data, and having a code-generator in my opinion helps tremendously in focus on the code that is necessary to be written.From a data perspective, normally the â€˜bakingâ€™ pipeline is a series of DCC formats as source transformed into very project specific and optimized data.Code-wise, depending on the engine/technology you are using, â€˜bakingâ€™ of the code is more uncommon.In a time in which iteration time has become almost more important than the tech itself, playing with this balance can be the key for any successful software. It could sound exaggerated, but I really believe in that.As always, both ImGui and SDL will be our sword and shields for this adventure.This will be the second step into data-driven rendering: code generation.Are we writing a compiler ? Short answer: yes!\nLong answer: we will be writing the simplest possible compiler that reads a source file and transform in a destination file, like Flatbuffers.\nThere are few links on both theory and practice that can help shed some light on the subject: The â€œDragon Bookâ€ (called because of the dragon in the cover) is still THE to-go in compiler writing as far as I know.It is an intense book and explores writing a full compiler with depth, starting from Automata theory (just reminds me of how everything you study can be useful, I did 2 exams at University about that, wondering when I would use it! Hello prof Di Battista!) to full code examples:\nhttps://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811\nThis is for me the best website on the subject, very precise and readable and follows closely what is inside the Dragon Book:\nhttps://craftinginterpreters.com/\nAnd github page:\nhttps://github.com/munificent/craftinginterpreters\nMy interest was rekindled in 2015, when I was following the amazing Casey Muratori and his Handmade Hero.He generates code for introspection purposes, and really show a simple and effective way of generating code that works for you.\nWikipedia itself also contains a lot of good articles on the subject. The more you know about the it, the more you want to know. It is fascinating and very, very deep!\nCompiler 101 A real compiler is a very complex and fascinating subject/software so I will try to get the simplest possible approach giving my (flawed and incomplete) perspective.\nA compiler is a series of transformations applied to data (you can apply this definition to every software actuallyâ€¦).\nThe input data is a text, and normally the output is still text, but with very different meaning.\nThe raw depth of the subject is astonishing, consider that we are defining a grammar and thus a language, and how to express concepts into it.\nThe main steps are the following:\n Lexer/scanner/tokenizer Parser Code generation  We will define the code generator from a custom language called HDF (Hydra Definition Format) to C++. HDF will be a subset of Flatbuffers in this exercise, but once the concepts are clear it can be expanded to more stuff.\nLexer/Scanner/Tokenizer A lexer or scanner (or tokenizer) is a software that translates an input string into a list of Tokens based on Lexemes. A Lexeme is one or more characters that create a Token. Think of a keyword (like â€˜ifâ€™, â€˜classâ€™, â€˜staticâ€™ â€¦).\nA Token is identified by a unique Lexeme and abstracts the Lexeme itself. It normally contains a type and some attributes, for example it can save where that lexeme is into the input text, the line. The final structure of the token can vary a bit.\nIn trying to find a simple definition for this step:\n The act of Tokenizing is the act of abstracting the input text.\n For example, given the following input text:\nstatic void amazing_method() {}; It will generate the list of tokens â€˜keyword, identifier, identifier, open parenthesis, close parenthesis, open brace, close brace, semicolonâ€™.\nThis IS abstracting the text!\nNormally a lexer/scanner is used by the parser to go through the code and retrieve a token and use it in some way. Letâ€™s start seeing what a lexer could be!\nCode Let\u0026rsquo;s see the code used by the lexer.\nFirst thing will be to define the Token:\n// Lexer/Tokenizer code. It is abstract enough so is not grammar specific. // struct Token { enum Type { Token_Unknown, Token_OpenParen, Token_CloseParen, Token_Colon, Token_Semicolon, Token_Asterisk, Token_OpenBracket, Token_CloseBracket, Token_OpenBrace, Token_CloseBrace, Token_OpenAngleBracket, Token_CloseAngleBracket, Token_String, Token_Identifier, Token_Number, Token_EndOfStream, }; // enum Type Type type; StringRef text; }; // struct Token It is basically a enum with a StringRef.A StringRef is basically a substring - used to avoid allocations when parsing by simply saving where the Token is in the parsed text and how long it is.\nNext is the Lexer itself:\n// // The role of the Lexer is to divide the input string into a list of Tokens. struct Lexer { char* position = nullptr; uint32_t line = 0; uint32_t column = 0; bool error = false; uint32_t error_line = 0; }; // struct Lexer The most important variable is position - it saves where the Lexer is in the current text for parsing.\nFrom now on there will be only methods.\nFirst some character classification that will help the Lexer:\n// // All those methods are to classify a character. // inline bool IsEndOfLine( char c ) { bool Result = ((c == '\\n') || (c == '\\r')); return(Result); } inline bool IsWhitespace( char c ) { bool Result = ((c == ' ') || (c == '\\t') || (c == '\\v') || (c == '\\f') || IsEndOfLine( c )); return(Result); } inline bool IsAlpha( char c ) { bool Result = (((c \u0026gt;= 'a') \u0026amp;\u0026amp; (c \u0026lt;= 'z')) || ((c \u0026gt;= 'A') \u0026amp;\u0026amp; (c \u0026lt;= 'Z'))); return(Result); } inline bool IsNumber( char c ) { bool Result = ((c \u0026gt;= '0') \u0026amp;\u0026amp; (c \u0026lt;= '9')); return(Result); } These should be quite straightforward.\nThen we have the most important method for the lexer: nextToken.This method will contain all the logic to go to the next token, and we will see it step by step.\nFirst is skipping all the whitespaces (empty characters, tabs, returns, etc) to arrive at the correct character in the text.\n// // This is the main method. Skip whitespaces and get next token. Save also the current position in the input string. // void nextToken( Lexer* lexer, Token\u0026amp; token ) { // Skip all whitespace first so that the token is without them. skipWhitespace( lexer ); The code for skipping the whitespace is pretty straight-forward. First it checks if it is a pure whitespace:\nvoid skipWhitespace( Lexer* lexer ) { // Scan text until whitespace is finished. for ( ;; ) { // Check if it is a pure whitespace first. if ( IsWhitespace( lexer-\u0026gt;position[0] ) ) { // Handle change of line if ( IsEndOfLine( lexer-\u0026gt;position[0] ) ) ++lexer-\u0026gt;line; // Advance to next character ++lexer-\u0026gt;position; Then it checks if it is a single line comment:\n } // Check for single line comments (\u0026quot;//\u0026quot;) else if ( (lexer-\u0026gt;position[0] == '/') \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == '/') ) { lexer-\u0026gt;position += 2; while ( lexer-\u0026gt;position[0] \u0026amp;\u0026amp; !IsEndOfLine( lexer-\u0026gt;position[0] ) ) { ++lexer-\u0026gt;position; } And last it checks for c-style multiline comments:\n } // Check for c-style multi-lines comments else if ( (lexer-\u0026gt;position[0] == '/') \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == '*') ) { lexer-\u0026gt;position += 2; // Advance until the string is closed. Remember to check if line is changed. while ( !((lexer-\u0026gt;position[0] == '*') \u0026amp;\u0026amp; (lexer-\u0026gt;position[1] == '/')) ) { // Handle change of line if ( IsEndOfLine( lexer-\u0026gt;position[0] ) ) ++lexer-\u0026gt;line; // Advance to next character ++lexer-\u0026gt;position; } if ( lexer-\u0026gt;position[0] == '*' ) { lexer-\u0026gt;position += 2; } } else { break; } } } After skipped all the whitespaces, we initialize the new token:\n // Initialize token token.type = Token::Token_Unknown; token.text.text = lexer-\u0026gt;position; token.text.length = 1; token.line = lexer-\u0026gt;line; We get the current character and advance the position, so we can analize it.\n char c = lexer-\u0026gt;position[0]; ++lexer-\u0026gt;position; Here comes the character analisys using a simple switch.\n switch ( c ) { case '\\0': { token.type = Token::Token_EndOfStream; } break; case '(': { token.type = Token::Token_OpenParen; } break; case ')': { token.type = Token::Token_CloseParen; } break; case ':': { token.type = Token::Token_Colon; } break; case ';': { token.type = Token::Token_Semicolon; } break; case '*': { token.type = Token::Token_Asterisk; } break; case '[': { token.type = Token::Token_OpenBracket; } break; case ']': { token.type = Token::Token_CloseBracket; } break; case '{': { token.type = Token::Token_OpenBrace; } break; case '}': { token.type = Token::Token_CloseBrace; } break; There are some special cases left.First parsing a string starting from a \u0026lsquo;\u0026quot;\u0026rsquo; character.It requires to scan the text until it finds another \u0026lsquo;\u0026quot;\u0026rsquo; to indicate the end of the string.It also supports multiple-line strings with the characters \u0026ldquo;\\\u0026rdquo; (double back-slash)\n case '\u0026quot;': { token.type = Token::Token_String; token.text.text = lexer-\u0026gt;position; while ( lexer-\u0026gt;position[0] \u0026amp;\u0026amp; lexer-\u0026gt;position[0] != '\u0026quot;' ) { if ( (lexer-\u0026gt;position[0] == '\\\\') \u0026amp;\u0026amp; lexer-\u0026gt;position[1] ) { ++lexer-\u0026gt;position; } ++lexer-\u0026gt;position; } // Saves total string length token.text.length = lexer-\u0026gt;position - token.text.text; if ( lexer-\u0026gt;position[0] == '\u0026quot;' ) { ++lexer-\u0026gt;position; } } break; Then the final classification step: first is checking if the token is an identifier (a string literal that starts with a character and is followed by characters, underscores or numbers).If not a identifier, check to see if it is a number. This should be expanded to correctly parse numbers, but for now is not used.. If everything else fails, than we don\u0026rsquo;t recognize the token.\n default: { // Identifier/keywords if ( IsAlpha( c ) ) { token.type = Token::Token_Identifier; while ( IsAlpha( lexer-\u0026gt;position[0] ) || IsNumber( lexer-\u0026gt;position[0] ) || (lexer-\u0026gt;position[0] == '_') ) { ++lexer-\u0026gt;position; } token.text.length = lexer-\u0026gt;position - token.text.text; } // Numbers else if ( IsNumber( c ) ) { token.type = Token::Token_Number; } else { token.type = Token::Token_Unknown; } } break; } } With this code we already have a working Lexer!I like to use the lexer in an abstract way - not knowing anything about the underlying language - so that it can be reused for different custom languages (Dr.Wily eyebrows movement goes here).If you want to dive deeper into this, the amazing Crafting Interpreters contains a great page on scanning:\nhttps://www.craftinginterpreters.com/scanning.html\nAlso, some c-style parsing can be found here from the amazing Niklas Frykohlm:\nhttps://github.com/niklasfrykholm/nflibs/blob/master/nf_json_parser.c\nAnd another amazing parser from STB:\nhttps://github.com/nothings/stb/blob/master/stb_c_lexer.h\nParser So far we have abstracted the input text into a list of Tokens, and now we need to generate some more information before arriving at generating new code.\nAs far as I understood it, a parser reads the tokens and generates an Abstract Syntax Tree.\nSometimes, and in simpler parsers, the act of parsing itself can generates a new code if the language we are targeting is simple. Again, I prefer to separate Lexer and Parser to reuse the Lexer for different languages and separate the responsabilities!\n Given a list of tokens and a grammar, a parser generates an Abstract Syntax Tree.\n  It gives meaning to the input text, and is responsible to check the syntax correctness.\n A simple definition for a grammar is the following:\n A grammar is a set of production rules that transforms a series of non-terminals into terminals.\n Putting everything in the perspective of data and transformations we can define:\n Terminals are finalized data Non-terminals are data that must be transformed Production rules are transformations of non-terminals to terminals  Another definition of a parser than it could be :\n A parser is a software that transforms non-terminals in terminals following production rules.\n Grammar It is time to write the formal grammar (a context-free grammar) and see how it maps to code.It will be very simple â€” much simpler than many examples you find around â€” but it is a starting point.We will not deal with any expression, statements and such, not in the context of this code generator. I will point out some examples for more complex stuff, but I want to study more the subject for that to be more precise about the subject.Each line will be a production rule (a transformation), with the left-side being always a non-terminal.We are using regular expressions syntax here:\n alphabet â†’ [a-zA-z] number â†’[0â€“9] identifier â†’ alphabet (alphabet | number | â€œ_â€)* variable_declaration â†’ identifier identifier â€œ;â€ struct_declaration â†’ â€œstructâ€ identifier â€œ{â€œ (variable_declaration)+ â€œ}â€ â€œ;â€ enum_declaration â†’ â€œenumâ€ identifier â€œ{â€œ (identifier)+ â€œ}â€ module â†’ (struct_declaration | enum_declaration)+*  First we define what an identifier is â€” a sequence of alpha-numerical characters that can contains also the underscore character.Notice that with the identifier production rule, the identifier cannot start with an underscore.A variable then is declared simply by two identifiers: the first for the type and the second for the name, following a semicolon.A struct is simply a list of variable declarations. Notice the â€œ+â€ in the rule â€” this means that at least one element must be present.Enums are literally a name for the enum and a list of identifiers in curly braces.Finally the module is the root of our grammar. It will contain all the declarations we describe. See it as the data file we are writing to generate the code â€” one file is one module.Now that we defined a simple grammar, we can move to the theory behind the parser.Predictive Recursive Descent Parser The grammar we defined is a context-free-grammar.Depending on the type of grammar we can write different parsers.One of the most common type of parser (and easier to start with) is the Predictive Recursive Descent Parser, and that is what we will write given our grammar. You can dive into all the details of writing a context-free grammar, writing a Left-to-right Leftmost-derivation grammar (LL(k)) and such and be amazed by all the concepts behind.Again, I am personally starting on this subject, so my knowledge is not deep.\nBack to the parser, the main characteristics of this parser are:\n Descent = top-down. Start from root and generate the Abstract Syntax Tree. Recursive = the parser has mutually recursive methods, one for each non-terminal. Predictive = no backtracking needed. For our simple grammar we do not need any backtracking.  So the parser will start from the root (module non-terminal) and by sequentially reading all the tokens will generate a tree that represent our syntax.\nLetâ€™s see some code!\nCode The central piece of code is the Parser.It uses the Lexer and saves the Types by parsing the input text.// // The Parser parses Tokens using the Lexer and generate an Abstract Syntax Tree. struct Parser { Lexer* lexer = nullptr; ast::Type* types = nullptr; uint32_t types_count = 0; uint32_t types_max = 0; }; // struct Parser Let\u0026rsquo;s have a look at the class Type.This class will let us identify correctly primitive types, enums, struct and commands - a special keyword I create to show a concept that can be used away from the canonical C/C++ languages.By saving a list of names and types we can successfully parse all the types listed above.\n// // Define the language specific structures. namespace ast { struct Type { enum Types { Types_Primitive, Types_Enum, Types_Struct, Types_Command, Types_None }; enum PrimitiveTypes { Primitive_Int32, Primitive_Uint32, Primitive_Int16, Primitive_Uint16, Primitive_Int8, Primitive_Uint8, Primitive_Int64, Primitive_Uint64, Primitive_Float, Primitive_Double, Primitive_Bool, Primitive_None }; Types type; PrimitiveTypes primitive_type; StringRef name; std::vector\u0026lt;StringRef\u0026gt; names; std::vector\u0026lt;const Type*\u0026gt; types; bool exportable = true; }; // struct Type } // namespace ast And now the actual code making the magic happens!Entry point for the parsing is generateAST.It simply goes through ALL the tokens until it reaches the end of the file.At this level of parsing, we parse only identifiers (keywords like \u0026lsquo;struct\u0026rsquo;, \u0026lsquo;enum\u0026rsquo;, \u0026hellip;).\nvoid generateAST( Parser* parser ) { // Read source text until the end. // The main body can be a list of declarations. bool parsing = true; while ( parsing ) { Token token; nextToken( parser-\u0026gt;lexer, token ); switch ( token.type ) { case Token::Token_Identifier: { identifier( parser, token ); break; } case Token::Type::Token_EndOfStream: { parsing = false; break; } } } } The method \u0026lsquo;identifier\u0026rsquo; searches for the language keywords and acts accordingly.The method \u0026lsquo;expectKeyword\u0026rsquo; simply checks that the keywords are the same.inline void identifier( Parser* parser, const Token\u0026amp; token ) { // Scan the name to know which for ( uint32_t i = 0; i \u0026lt; token.text.length; ++i ) { char c = *(token.text.text + i); switch ( c ) { case 's': { if ( expectKeyword( token.text, 6, \u0026quot;struct\u0026quot; ) ) { declarationStruct( parser ); return; } break; } case 'e': { if ( expectKeyword( token.text, 4, \u0026quot;enum\u0026quot; ) ) { declarationEnum( parser ); return; } break; } case 'c': { if ( expectKeyword( token.text, 7, \u0026quot;command\u0026quot; ) ) { declarationCommand( parser ); return; } break; } } } } The next methods are the real core of parsing a language. When declaring a struct, the token we have are:\n Identifier \u0026lsquo;struct\u0026rsquo; (parsed already by generateAST method) Name of the struct Open braces Zero or more variables  The method expectToken checks the presence of the expected token and saves the line if an error occurs.\ninline void declarationStruct( Parser* parser ) { // name Token token; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) { return; } // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = name; type.type = ast::Type::Types_Struct; type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { declarationVariable( parser, token.text, type ); } } } The parsing of a variable is even simpler, just a type followed by the name. When reading the type, it searches through the list of all types saved until then.\ninline void declarationVariable( Parser* parser, const StringRef\u0026amp; type_name, ast::Type\u0026amp; type ) { const ast::Type* variable_type = findType( parser, type_name ); Token token; // Name if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Semicolon ) ) { return; } type.types.emplace_back( variable_type ); type.names.emplace_back( name ); } The parsing of the enum is:\n \u0026lsquo;enum\u0026rsquo; keyword Enum name (optional) Semicolon and type, taken from Flatbuffers syntax Open brace List of identifiers that corresponds to the enum values   inline void declarationEnum( Parser* parser ) { Token token; // Name if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; // Optional ': type' for the enum nextToken( parser-\u0026gt;lexer, token ); if ( token.type == Token::Token_Colon ) { // Skip to open brace nextToken( parser-\u0026gt;lexer, token ); // Token now contains type_name nextToken( parser-\u0026gt;lexer, token ); // Token now contains open brace. } if ( token.type != Token::Token_OpenBrace ) { return; } // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = name; type.type = ast::Type::Types_Enum; type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { type.names.emplace_back( token.text ); } } } A command is a special construct that I use in my code, normally with a CommandBuffer, and with the current syntax from HDF:\ncommand WindowEvents { Click { int16 x; int16 y; int16 button; } Move { int16 x; int16 y; } Wheel { int16 z; } }; And this is the parsing of the command.I think this can be the best example of mapping between the language and the parsing.Parsing is:\n Name Open brace Scan of identifiers until close brace For each identifier, add a type and scan for internal variables.  inline void declarationCommand( Parser* parser ) { // name Token token; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_Identifier ) ) { return; } // Cache name string StringRef name = token.text; if ( !expectToken( parser-\u0026gt;lexer, token, Token::Token_OpenBrace ) ) { return; } // Add new type ast::Type\u0026amp; command_type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; command_type.name = name; command_type.type = ast::Type::Types_Command; command_type.exportable = true; // Parse struct internals while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { // Create a new type for each command // Add new type ast::Type\u0026amp; type = parser-\u0026gt;types[parser-\u0026gt;types_count++]; type.name = token.text; type.type = ast::Type::Types_Struct; type.exportable = false; while ( !equalToken( parser-\u0026gt;lexer, token, Token::Token_CloseBrace ) ) { if ( token.type == Token::Token_Identifier ) { declarationVariable( parser, token.text, type ); } } command_type.names.emplace_back( type.name ); command_type.types.emplace_back( \u0026amp;type ); } } } Abstract Syntax Tree We choose to simply have data definitions, and Iâ€™ve decided that the nodes of the tree will be types.A type can be a primitive type, a container of variables (like a Struct in C, but without methods) enums and commands.Commands are just a way of showing the creation of a construct that I use and requires some boilerplate code, but I donâ€™t want to write that code.If we remember the definition of the class Type from the code before, it all boils down to a name,a list of names and optionally types.With this simple definition I can express primitive types, structs and enums all in one!For enums, I save the anme of the enum and in the name list all the different values. That is enough to later generate the code.For structs, again the name is saved, and then the variables. A variable is a tuple of identifiers â€˜type, nameâ€™. When parsing them, the type is searched in the registered ones.A trick here is to initialize the parser with primitive types, and then add each type (both struct and enums) when parsing them.Code Generation The last stage will generate the files in the language that we want, using the informations from the AST.This part will literally write the code for us, the all purpose of this code.The most fundamental question is: â€œwhat code do I want to generate?â€.A simple but deep question.We are trying to remove the writing of boilerplate code from or lives, so anything that you consider boilerplate and easy to automate goes here. Even if until here we wrote in C++, the final output can be any language.This means that you can define data and translate it to multiple languages!For our example, we will output C++ code and add UI using ImGui, similar to the Flatbuffers example I wrote before.Letâ€™s see the three different construct we can output with our language.Enum We defined an enum as a name and a list of named values. For the simplicity of this example, we are not assigning manual values to the enum, but it is something easily changeable, and I will do it in the future. Given the enum in HDF:\nenum BlendOperation : byte { Add, Subtract, RevSubtract, Min, Max } Which code do we want to generate ?\nWhen I write enums, I almost always need the stringed version of the values. Also I want to add a last value, Count, so that I can use it if I need to allocate anything based on the enum.As a bonus, I can create a second enum with the bit shifts â€” called mask â€” for some use cases.All of this will be automatically done by the code generator, starting with a simple enum!In this piece of code, I will use three different streams for the different parts of the enum (enum itself, value names and mask) and combine them into the final generated file.Also to note that the strings here are â€˜String Refâ€™ â€” basically a string that points to the input source code and stores the length of the string, so that there is no need to allocate it newly.I will use a temporary buffer to null terminate it and write into the output file.This will be the generated code:\nnamespace BlendOperation { enum Enum { Add, Subtract, RevSubtract, Min, Max, Count }; enum Mask { Add_mask = 1 \u0026lt;\u0026lt; 0, Subtract_mask = 1 \u0026lt;\u0026lt; 1, RevSubtract_mask = 1 \u0026lt;\u0026lt; 2, Min_mask = 1 \u0026lt;\u0026lt; 3, Max_mask = 1 \u0026lt;\u0026lt; 4, Count_mask = 1 \u0026lt;\u0026lt; 5 }; static const char* s_value_names[] = { \u0026quot;Add\u0026quot;, \u0026quot;Subtract\u0026quot;, \u0026quot;RevSubtract\u0026quot;, \u0026quot;Min\u0026quot;, \u0026quot;Max\u0026quot;, \u0026quot;Count\u0026quot; }; static const char* ToString( Enum e ) { return s_value_names[(int)e]; } } // namespace BlendOperation The enum itself (inside a namespace), a mask and the string version for debugging purposes.All generated from that one line!Let\u0026rsquo;s go into a step by step review of the code.First there is the initialization of some auxiliary buffers to handle dynamic strings without allocating memory.These are the usages:\n Values will contain all the enum comma separated values Value_names will contain the string version of the values Value_masks will contain an optional bitmask for the values.   void outputCPPEnum( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { // Empty enum: skip output. if ( type.names.size() == 0 ) return; code_generator-\u0026gt;string_buffer_0.clear(); code_generator-\u0026gt;string_buffer_1.clear(); code_generator-\u0026gt;string_buffer_2.clear(); StringBuffer\u0026amp; values = code_generator-\u0026gt;string_buffer_0; StringBuffer\u0026amp; value_names = code_generator-\u0026gt;string_buffer_1; StringBuffer\u0026amp; value_masks = code_generator-\u0026gt;string_buffer_2; We start by adding the character \u0026lsquo;\u0026quot;\u0026rsquo; in the names - they will be C strings!Then we have a couple of options, just as demonstration: add mask (for the bitmask) and add max, that adds a last element to the generated enum. value_names.append( \u0026quot;\\\u0026quot;\u0026quot; ); bool add_max = true; bool add_mask = true; Next step is the core: go through all the names saved in the enum ast::Type during the parsing phase, and add the literal as is in the enum, the literal in string version and optional mask.We also need to take care of the enum with 1 values, they behave in a different way. char name_buffer[256]; // Enums with more than 1 values if ( type.names.size() \u0026gt; 1 ) { const uint32_t max_values = type.names.size() - 1; for ( uint32_t v = 0; v \u0026lt; max_values; ++v ) { if ( add_mask ) { value_masks.append( type.names[v] ); value_masks.append( \u0026quot;_mask = 1 \u0026lt;\u0026lt; \u0026quot; ); value_masks.append( _itoa( v, name_buffer, 10 ) ); value_masks.append( \u0026quot;, \u0026quot; ); } values.append( type.names[v] ); values.append( \u0026quot;, \u0026quot; ); value_names.append( type.names[v] ); value_names.append( \u0026quot;\\\u0026quot;, \\\u0026quot;\u0026quot; ); } if ( add_mask ) { value_masks.append( type.names[max_values] ); value_masks.append( \u0026quot;_mask = 1 \u0026lt;\u0026lt; \u0026quot; ); value_masks.append( _itoa( max_values, name_buffer, 10 ) ); } values.append( type.names[max_values] ); value_names.append( type.names[max_values] ); value_names.append( \u0026quot;\\\u0026quot;\u0026quot; ); } else { if ( add_mask ) { value_masks.append( type.names[0] ); value_masks.append( \u0026quot;_mask = 1 \u0026lt;\u0026lt; \u0026quot; ); value_masks.append( _itoa( 0, name_buffer, 10 ) ); } values.append( type.names[0] ); value_names.append( type.names[0] ); value_names.append( \u0026quot;\\\u0026quot;\u0026quot; ); } After writing all the values we can add the optional max value in the output:\n if ( add_max ) { values.append( \u0026quot;, Count\u0026quot; ); value_names.append( \u0026quot;, \\\u0026quot;Count\\\u0026quot;\u0026quot; ); if ( add_mask ) { value_masks.append( \u0026quot;, Count_mask = 1 \u0026lt;\u0026lt; \u0026quot; ); value_masks.append( _itoa( type.names.size(), name_buffer, 10 ) ); } } Until now we just saved all those values in the StringBuffers, but still not in the file.The final piece of code output to file the enum with all the additional data: copy( type.name, name_buffer, 256 ); fprintf( output, \u0026quot;namespace %s {\\n\u0026quot;, name_buffer ); fprintf( output, \u0026quot;\\tenum Enum {\\n\u0026quot; ); fprintf( output, \u0026quot;\\t\\t%s\\n\u0026quot;, values.data ); fprintf( output, \u0026quot;\\t};\\n\u0026quot; ); // Write the mask if ( add_mask ) { fprintf( output, \u0026quot;\\n\\tenum Mask {\\n\u0026quot; ); fprintf( output, \u0026quot;\\t\\t%s\\n\u0026quot;, value_masks.data ); fprintf( output, \u0026quot;\\t};\\n\u0026quot; ); } // Write the string values fprintf( output, \u0026quot;\\n\\tstatic const char* s_value_names[] = {\\n\u0026quot; ); fprintf( output, \u0026quot;\\t\\t%s\\n\u0026quot;, value_names.data ); fprintf( output, \u0026quot;\\t};\\n\u0026quot; ); fprintf( output, \u0026quot;\\n\\tstatic const char* ToString( Enum e ) {\\n\u0026quot; ); fprintf( output, \u0026quot;\\t\\treturn s_value_names[(int)e];\\n\u0026quot; ); fprintf( output, \u0026quot;\\t}\\n\u0026quot; ); fprintf( output, \u0026quot;} // namespace %s\\n\\n\u0026quot;, name_buffer ); } Struct Structs are the bread-and-butter of data definition. In this simple example we do not handle pointers or references, so it is pretty straight-forward, but as a start in coding generation this could already be powerful for many cases. Letâ€™s start with a definition for our dream Data-Driven-Rendering:\n// file.hdf struct RenderTarget { uint16 width; uint16 height; float scale_x; float scale_y; TextureFormat format; }; struct RenderPass { RenderTarget rt0; }; We want to generate both the ready to use header in C++ and UI using ImGui.The output for this struct will be obtained by simply iterating through all its members and, based on the type of the member, write some code.For primitive types there is a translation that must be done to the C++ language â€” thus we saved a list of c++ primitive types keyword into the code.For the UI area we will define two methods: reflectMembers, that simply adds the ImGui commands needed, and reflectUI, that embeds the members into a Window. This is done so that when starting from a root type I can create a window that let me edit its value, and recursively it can add other memberâ€™s UI if they are coming from another struct.This is shown with the RenderPass struct.This will be the generated code, that includes ImGui too:// CodeGenerated.h struct RenderTarget { uint16_t width; uint16_t height; float scale_x; float scale_y; TextureFormat::Enum format; void reflectMembers() { ImGui::InputScalar( \u0026quot;width\u0026quot;, ImGuiDataType_U16, \u0026amp;width ); ImGui::InputScalar( \u0026quot;height\u0026quot;, ImGuiDataType_U16, \u0026amp;height ); ImGui::InputScalar( \u0026quot;scale_x\u0026quot;, ImGuiDataType_Float, \u0026amp;scale_x ); ImGui::InputScalar( \u0026quot;scale_y\u0026quot;, ImGuiDataType_Float, \u0026amp;scale_y ); ImGui::Combo( \u0026quot;format\u0026quot;, (int32_t*)\u0026amp;format, TextureFormat::s_value_names, TextureFormat::Count ); } void reflectUI() { ImGui::Begin(\u0026quot;RenderTarget\u0026quot;); reflectMembers(); ImGui::End(); } }; // struct RenderTarget Now let\u0026rsquo;s have a look at the code that will generate that.First some init steps: clear and alias the StringBuffer, allocate some char buffers on the stack, copy the StringRef into the name buffer: void outputCPPStruct( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { const char* tabs = \u0026quot;\u0026quot;; code_generator-\u0026gt;string_buffer_0.clear(); StringBuffer\u0026amp; ui_code = code_generator-\u0026gt;string_buffer_0; char name_buffer[256], member_name_buffer[256], member_type_buffer[256]; copy( type.name, name_buffer, 256 ); Next is already a powerful piece of code.Outputting the UI code and iterating through each member. if ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026quot;\\n\\tvoid reflectMembers() {\\n\u0026quot; ); } fprintf( output, \u0026quot;%sstruct %s {\\n\\n\u0026quot;, tabs, name_buffer ); for ( int i = 0; i \u0026lt; type.types.size(); ++i ) { const ast::Type\u0026amp; member_type = *type.types[i]; const StringRef\u0026amp; member_name = type.names[i]; copy( member_name, member_name_buffer, 256 ); We are in the middle of the loop, and we want to check if the current member type is a primitive one, then it needs some work to do.First, output the language specific primitive type keyword (using the s_primitive_type_cpp array).Second, add some ImGui code to edit the field directly. // Translate type name based on output language. switch ( member_type.type ) { case ast::Type::Types_Primitive: { strcpy_s( member_type_buffer, 256, s_primitive_type_cpp[member_type.primitive_type] ); fprintf( output, \u0026quot;%s\\t%s %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { switch ( member_type.primitive_type ) { case ast::Type::Primitive_Int8: case ast::Type::Primitive_Uint8: case ast::Type::Primitive_Int16: case ast::Type::Primitive_Uint16: case ast::Type::Primitive_Int32: case ast::Type::Primitive_Uint32: case ast::Type::Primitive_Int64: case ast::Type::Primitive_Uint64: case ast::Type::Primitive_Float: case ast::Type::Primitive_Double: { ui_code.append( \u0026quot;\\t\\tImGui::InputScalar( \\\u0026quot;%s\\\u0026quot;, %s, \u0026amp;%s );\\n\u0026quot;, member_name_buffer, s_primitive_type_imgui[member_type.primitive_type], member_name_buffer ); break; } case ast::Type::Primitive_Bool: { ui_code.append( \u0026quot;\\t\\tImGui::Checkbox( \\\u0026quot;%s\\\u0026quot;, \u0026amp;%s );\\n\u0026quot;, member_name_buffer, member_name_buffer ); break; } } } break; } In case of a struct as a member, use the typename as is and call the \u0026lsquo;reflectMembers\u0026rsquo; method for the UI generation: case ast::Type::Types_Struct: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;%s\\t%s %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026quot;\\t\\tImGui::Text(\\\u0026quot;%s\\\u0026quot;);\\n\u0026quot;, member_name_buffer ); ui_code.append( \u0026quot;\\t\\t%s.reflectMembers();\\n\u0026quot;, member_name_buffer ); } break; } For enums use the format namespace::Enum that comes with the generated code (and can be anything else) and add a Combo for ImGui. The combo is using the string array generated previously! This is powerful! case ast::Type::Types_Enum: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;%s\\t%s::Enum %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); if ( code_generator-\u0026gt;generate_imgui ) { ui_code.append( \u0026quot;\\t\\tImGui::Combo( \\\u0026quot;%s\\\u0026quot;, (int32_t*)\u0026amp;%s, %s::s_value_names, %s::Count );\\n\u0026quot;, member_name_buffer, member_name_buffer, member_type_buffer, member_type_buffer ); } break; } To finish up simlpy add the reflectUI method, that embed the members reflection in a window and finish. default: { break; } } } ui_code.append( \u0026quot;\\t}\u0026quot; ); ui_code.append( \u0026quot;\\n\\n\\tvoid reflectUI() {\\n\\t\\tImGui::Begin(\\\u0026quot;%s\\\u0026quot;);\\n\\t\\treflectMembers();\\n\\t\\tImGui::End();\\n\\t}\\n\u0026quot;, name_buffer ); fprintf( output, \u0026quot;%s\\n\u0026quot;, ui_code.data ); fprintf( output, \u0026quot;\\n%s}; // struct %s\\n\\n\u0026quot;, tabs, name_buffer ); } Command I wanted to include an example of something that does not exist in any language, but it shows the power of removing boilerplate code.\nI define commands as little structs with a type used anytime I need to do some command parsing, normally from a ring buffer.\nThe command should have an enum with all the types already, and each struct should have its type assigned. The type is normally used to cycle through the commands and do something accordingly.\nIt will output structs because of the need to allocate them in the ring buffer, thus must be simple.\nFirst let\u0026rsquo;s see the HDF file. The example are window events commands:command WindowEvents { Click { int16 x; int16 y; int16 button; } Move { int16 x; int16 y; } Wheel { int16 z; } }; The generated code will be:namespace WindowEvents { enum Type { Type_Click, Type_Move, Type_Wheel }; struct Click { int16_t x; int16_t y; int16_t button; static Type GetType() { return Type_Click; } }; // struct Wheel struct Move { int16_t x; int16_t y; static Type GetType() { return Type_Move; } }; // struct Wheel struct Wheel { int16_t z; static Type GetType() { return Type_Wheel; } }; // struct Wheel }; // namespace WindowEvents And finally the C++ code that generates the output.The output starts with an enum with all the types, that I normally use to switch commands:void outputCPPCommand( CodeGenerator* code_generator, FILE* output, const ast::Type\u0026amp; type ) { char name_buffer[256], member_name_buffer[256], member_type_buffer[256]; copy( type.name, name_buffer, 256 ); fprintf( output, \u0026quot;namespace %s {\\n\u0026quot;, name_buffer ); // Add enum with all types fprintf( output, \u0026quot;\\tenum Type {\\n\u0026quot; ); fprintf( output, \u0026quot;\\t\\t\u0026quot; ); for ( int i = 0; i \u0026lt; type.types.size() - 1; ++i ) { const ast::Type\u0026amp; command_type = *type.types[i]; copy( command_type.name, name_buffer, 256 ); fprintf( output, \u0026quot;Type_%s, \u0026quot;, name_buffer ); } const ast::Type* last_type = type.types[type.types.size() - 1]; copy( last_type-\u0026gt;name, name_buffer, 256 ); fprintf( output, \u0026quot;Type_%s\u0026quot;, name_buffer ); fprintf( output, \u0026quot;\\n\\t};\\n\\n\u0026quot; ); Then we output all the command structs (like Click, Move, \u0026hellip;).For each command type we output a struct with all its members. This is similar to the output of the structs: const char* tabs = \u0026quot;\\t\u0026quot;; for ( int i = 0; i \u0026lt; type.types.size(); ++i ) { const ast::Type\u0026amp; command_type = *type.types[i]; copy( command_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;%sstruct %s {\\n\\n\u0026quot;, tabs, member_type_buffer ); for ( int i = 0; i \u0026lt; command_type.types.size(); ++i ) { const ast::Type\u0026amp; member_type = *command_type.types[i]; const StringRef\u0026amp; member_name = command_type.names[i]; copy( member_name, member_name_buffer, 256 ); // Translate type name based on output language. switch ( member_type.type ) { case ast::Type::Types_Primitive: { strcpy_s( member_type_buffer, 256, s_primitive_type_cpp[member_type.primitive_type] ); fprintf( output, \u0026quot;%s\\t%s %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); break; } case ast::Type::Types_Struct: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;%s\\t%s %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); break; } case ast::Type::Types_Enum: { copy( member_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;%s\\t%s::Enum %s;\\n\u0026quot;, tabs, member_type_buffer, member_name_buffer ); break; } default: { break; } } } copy( command_type.name, member_type_buffer, 256 ); fprintf( output, \u0026quot;\\n%s\\tstatic Type GetType() { return Type_%s; }\\n\u0026quot;, tabs, member_type_buffer ); fprintf( output, \u0026quot;\\n%s}; // struct %s\\n\\n\u0026quot;, tabs, name_buffer ); } copy( type.name, name_buffer, 256 ); fprintf( output, \u0026quot;}; // namespace %s\\n\\n\u0026quot;, name_buffer ); } Conclusions We learnt how to write a complete Code Generator, an incredible tool that can speed up the development if used correctly and remove most boilerplate code possible.\nThe usage of the command keyword was an example of something I use and I donâ€™t want to write code, something that is custom enough and hopefully will give you more ideas on how you can break free from languages constriction when you writeâ€¦your own language!\nIn the quest for data-driven rendering, the next step will be to use the knowledge from code generation to create a shader effect language, that can generate both CPU and GPU code for you.\nThis article is the longest and more code-heavy I have ever written. There are many concepts that I am beginning to be familiar with, but still not so used to.\nSo please comment, give feedback, share! Thank you for reading!\n","date":1564267563,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565131563,"objectID":"3b9e1a648d0f65843a8cdecb32355e6c","permalink":"https://jorenjoestar.github.io/post/writing_a_simple_code_generator/","publishdate":"2019-07-27T18:46:03-04:00","relpermalink":"/post/writing_a_simple_code_generator/","section":"post","summary":"UI using ImGUI, SDL and the code generated with this article.   Motivation Following my previous article about Flatbuffers and data reflection the quest for Data-Driven Rendering continues!In this article I want to show how to write a very simple code-generator to help you automate writing of code in any language.The code is here:\nhttps://github.com/JorenJoestar/DataDrivenRendering\nThere is a balance that constantly needs to be found between code and data, and having a code-generator in my opinion helps tremendously in focus on the code that is necessary to be written.","tags":[],"title":"Writing a simple Code Generator","type":"post"},{"authors":[],"categories":[],"content":"   Some of the UI for the Hydra NES emulator, using ImGUI.   Writing an emulator is an incredibly fun learning experience.\nIt is an exquisite exercise in reverse-engineering from both documentation and code.\nIn this post I want to share some tips on how and where to start based on my experience on the NES emulator I am writing.\nInformation The gathering of information is the most important (and hard!) process that will live through all the writing process.\nLuckily for us there are many websites to help in this:\nhttps://wiki.nesdev.com/w/index.php/NES_reference_guide\nhttp://forums.nesdev.com/\nhttp://obelisk.me.uk/6502/reference.html\nhttp://www.oxyron.de/html/opcodes02.html\nIt is paramount to create a list of websites and resources (maybe through some notes, like in Evernote or such) about different topics regarding the hardware to be emulated.\nHaving a central hub is powerful and counteract the sparseness of the different informations (some in txt files, different websites, forum blogposts, â€¦).\nI canâ€™t stress enough how important it is.The amazing NesDev Wiki is the hub you need. Almost every possible information is there.\nArchitecture Next step is to understand the architecture.Write diagrams, take notes, search for the relationships of the component.What does every hardware component do ?What can that specific hardware piece access to ?As you will see, writing the emulator is an iterative process of improving each component until you have something that works very well, and then refine for an infinite amount of time.On a very basic level, there should be a CPU, some form of GPU (PPU, Picture Processing Unit), some audio chip, some input peripheral and cartridge/disc/rom.\nNES architecture The NES is a beautiful machine equipped with the following:\nCPU : Ricoh RP2A03 (NTSC) / RP2A07 (PAL) 8 bit processor that contains both CPU and APU (audio) hardware. The addresses are 16 bit, but the data is 8. It contains only specific registers: 2 indices, accumulator, stack pointer, program counter and status.\nPPU : Ricoh RP2C02 (NTSC) / RP2C07 (PAL) This is what today would be called GPU. It outputs to a 256x240 pixels buffer, it has 2kib or RAM, 32 bytes for palette RAM and 288 bytes for sprite RAM. The PPU is tile based and it takes 8 PPU cycles to load a line of a background tile. Sprites are sent through DMA and background is filled during Vertical Blank state normally. A frame lasts more scanline that the one visible, so that the game can upload data to the PPU when not rendering.\nAPU : Ricoh RP2A03 (NTSC) / RP2A07 (PAL) (Contained in the CPU itself.) The sound is analogic and it comes from 5 different channels: 2 pulse, 1 triangle, 1 noise and 1 DMC. All the channels aside from the DMC create signals that are combined to output the sounds and music. The DMC loads samples using the DMA.\nCartridge/Mappers : This is a very unique topic strict to the NES as far as I know. Cartridges had unique hardware and they were used to swap banks of memory in realtime to access different parts of the cartridge. There are hundred of mappers that have unique behaviours! The biggest gist of the mappers is how they switch banks: by WRITING to the address where the execution code is it triggers the bank-switching logic. There can be internal batteries and working RAMs too, but they are very rare.\nMemory mapped I/O The different hardware access using â€˜memory mapped I/Oâ€™, that is a way of saying that when you read or write to a specific address it could be memory or it could be an hardware-component.\nExamples: reading from address 0x4016 gives you the gamepad status, while reading from 0x1000 reads from the CPU ram.\nHaving clear these accesses will help in understanding even better the machine.\nBoth CPU and PPU have different memory maps. Let\u0026rsquo;s see them, it will help in understanding the internal of the NES better.\nCPU Memory Map    The CPU can access basically every hardware component in the NES.PPU, APU, gamepads, both read and write.It reads the ROM part of a cartridge (called PRG) and executes its instructions.Through PPU registers it can instruct the PPU to read graphical informations from the CHR part of the cartridge.It can upload sprites on the PPU Sprite Memory through DMA, upload data to the APU, or manage its internal RAM.\nFrom the source code, this is a working example of CPU Reading method:\nuint8 Nes::MemoryController::CpuRead( uint16 address ) { if ( address \u0026lt; 0x2000 ) { return cpu-\u0026gt;ram[address \u0026amp; 0x7FF]; } else if ( address \u0026lt; 0x4000 ) { return ppu-\u0026gt;CpuRead( address ); } else if ( address \u0026lt; 0x4014 ) { return apu-\u0026gt;CpuRead( address ); } else if ( address \u0026gt;= 0x4018 ) { return mapper-\u0026gt;PrgRead( address ); } switch ( address ) { case 0x4015: { return apu-\u0026gt;ReadStatus(); break; } case 0x4016: { return controllers-\u0026gt;ReadState(); break; } case 0x4017: { return 0x40; break; } } return 0; } And CPU Write:\nvoid Nes::MemoryController::CpuWrite( uint16 address, uint8 data ) { if ( address \u0026lt; 0x2000 ) { cpu-\u0026gt;ram[address \u0026amp; 0x7FF] = data; } else if ( address \u0026lt; 0x4000 ) { ppu-\u0026gt;CpuWrite( address, data ); return; } else if ( address \u0026lt; 0x4014 ) { return apu-\u0026gt;CpuWrite( address, data ); } else if ( address \u0026gt;= 0x4018 ) { mapper-\u0026gt;PrgWrite( address, data ); return; } switch ( address ) { // Sprite DMA case 0x4014: { cpu-\u0026gt;ExecuteSpriteDMA( data ); return; break; } case 0x4015: case 0x4017: { apu-\u0026gt;CpuWrite( address, data ); return; break; } case 0x4016: { controllers-\u0026gt;WriteState( data ); return; break; } } } The pattern is always the same: check the address of the instruction and choose which hardware component to interact with.\nHopefully its clear that based on the address different components can be accessed. Let\u0026rsquo;s have a look at the PPU too.\nPPU Memory Map    Similar to the CPU, reading and writing on the PPU access different components, even though they are far less.The PPU either accesses its 2 rams (palette and nametable, normally from the CPU) or reads the CHR (that is the graphical data stored in the cartridge) memory.\nReading:\nuint8 Nes::MemoryController::PpuRead( uint16 address ) { address \u0026amp;= 0X3FFF; if ( address \u0026lt;= 0x1FFF ) { return mapper-\u0026gt;ChrRead( address ); } else if ( address \u0026lt;= 0x3EFF ) { return ppu-\u0026gt;nametableRam[NameTableMirroring( address, mapper-\u0026gt;mirroring )]; } else if ( address \u0026lt;= 0x3FFF ) { // Palette mirroring is handled in the write code. return ppu-\u0026gt;paletteRam[address \u0026amp; 0x1F] \u0026amp; ((ppu-\u0026gt;mask \u0026amp; Nes::Ppu::MaskFlag_GreyScale ? 0x30 : 0xFF)); } return 0; } On the writing side, there the code shows the intricancy of emulation. When writing to the paletter ram, there is a mirroring mechanism happening in the hardware that is emulated with a lookup table. Something to look out to: writing to CHR is 99% of the time useless, unless there is an additional RAM in the cartdige.\nvoid Nes::MemoryController::PpuWrite( uint16 address, uint8 data ) { address \u0026amp;= 0X3FFF; if ( address \u0026lt;= 0x1FFF ) { mapper-\u0026gt;ChrWrite( address, data ); return; } else if ( address \u0026lt;= 0x3EFF ) { ppu-\u0026gt;nametableRam[NameTableMirroring( address, mapper-\u0026gt;mirroring )] = data; return; } else if ( address \u0026lt;= 0x3FFF ) { static uint8 const palette_write_mirror[0x20] = { 0x10, 0x01, 0x02, 0x03, 0x14, 0x05, 0x06, 0x07, 0x18, 0x09, 0x0A, 0x0B, 0x1C, 0x0D, 0x0E, 0x0F, 0x00, 0x11, 0x12, 0x13, 0x04, 0x15, 0x16, 0x17, 0x08, 0x19, 0x1A, 0x1B, 0x0C, 0x1D, 0x1E, 0x1F }; ppu-\u0026gt;paletteRam[palette_write_mirror[address \u0026amp; 0x1F]] = data; return; } } Takeaways I created the memory controller as the main dispatcher of data between hardware components, to separate the duties better. We can see the following relationships based on that:\n CPU can access PPU, APU, controllers and cartridge (PRG) PPU can access screen, its own rams and cartridge (CHR) memory controller is the hub that connects everything  I am not sure this is the best emulator architecture, but that is what I figured out.\nTest roms A fundamental approach to create a robust emulator is to have some tests to rely on.Sadly it is not common for all hardware, but again the NES provide plenty of roms that tests almost every aspect of your emulator!It quickly becomes a test-driven development.NES test roms link\nFind roms, read the source code and try to understand what they are doing and why.\nCoding start If you are writing your first emulator, I suggest to focus mostly on the emulation part.\nWhat do I mean by that ?Avoid trying too many things at once!Focus your energies towards the emulation.Use libraries that are reliable and simple and that you know.GLFW, SDL2, etc are your friends here.You want to eliminate most unknowns unknowns before hand.Of course, if you are brave enough, you can also write an emulator in a new language.\nBut for me, I preferred to concentrate on the emulation side first, in C++, using my core library, especially knowing that I could dedicate some night-time here and there, No surprises (not really true, still some happened!).\nI will possibly port the emulator to use SDL if needed, but right now the emulation code is the most important.\nThis is the mantra that helped me concentrate only on the emulation code. Again, writing-wise I am not happy about the code quality. But what I am learning from different perspectives is invaluable!\nNES coding start The quintessential basic steps to start a NES emulator coding are:\n Write CPU basics (fetch/decode/execute loop, registers) Basic memory bus (read/write to/from memory and registers) Load a rom and start executing instruction step by step.  It is already a lot, and it will require to read multiple times the different wiki pages and forum posts.\nFor a typical console, the main loop (simplified) can be something like this:\nvoid CpuTick() { uint8_t opcode = Read(program_counter++); uint8_t operand = FetchOperand(opcode); ExecuteOpcode(opcode, operand); } void ExecuteFrame() { uint32_t cycles_per_frame = â€¦ while (cycles_per_frame â€” ) { CpuTick(); } } To jumpstart your NES emulator you can use the majestic rom nestest.nes and its log file: it gives you a test of all instructions of the CPU and prints the status of the CPU after each one.\nAlso it does not require any PPU rendering: compare the status of your CPU with the text file line by line and its done!\nYou can see some ugly but useful code in MainState::ExecuteCpuTest in my emulator for an idea.\nA line from the nestest.log file looks like this:\n// C000 4C F5 C5 JMP $C5F5 A:00 X:00 Y:00 P:24 SP:FD PPU: 0, 0 CYC:7 it gives you the ProgramCounter (C000), byte code (1, 2 or 3 bytes depending on the instructions), human-readable-instruction (JMP) , the CPU register contents (A, X, Y, P, SP) and the theorethical PPU scanline, pixel and clock cycle.\nThere are two interesting points:\n The ProgramCounter before execution should be set to C000 for this rom only and only when logging. The CPU cycles STARTS at 7. In a power-up/reset method there is some work done BEFORE executing any code. This is needed only if you want to have a precise cycle-to-cycle comparison.  You can create a simple test method like this:\nvoid TestEmulatorCPU() { Reset(); while(true) { CpuTick(); CompareCpuStatusWithLog(); } } and catch the problems in your CPU instructions implementation!\nConclusion This is a little help in understanding how to start with an emulator.\nIt is a beautiful journey, but it is full of trial and errors.\nI am myself far from over with my emulator, and also far from being happy on HOW I write the emulator itself.\nThere are emulators of much more complex machines out there (almost every machine you can imagine!) and it blows my mind to know there are people that can emulate such complex hardware.\nThe ideal situation would be to being able of not being lost in visual emulation of the circuitry, but for now that is out of my league.\nI am thinking of creating some a series of videos and code associated starting from scratch, if anyone is interested. Please leave a comment/feedback on the article, the source code, anything!\nI hope it will help.\n","date":1564267535,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564267535,"objectID":"33334c3b94bfe395ca48dde1b71dc142","permalink":"https://jorenjoestar.github.io/post/emulation_where_to_start/","publishdate":"2019-07-27T18:45:35-04:00","relpermalink":"/post/emulation_where_to_start/","section":"post","summary":"Some of the UI for the Hydra NES emulator, using ImGUI.   Writing an emulator is an incredibly fun learning experience.\nIt is an exquisite exercise in reverse-engineering from both documentation and code.\nIn this post I want to share some tips on how and where to start based on my experience on the NES emulator I am writing.\nInformation The gathering of information is the most important (and hard!","tags":[],"title":"Emulation: where to start? A use case.","type":"post"},{"authors":[],"categories":[],"content":"   Auto generated UI from Flatbuffers files.   Motivation Finding a good balance between code and data in Rendering.What is the necessary code that should be written ?Why ?In rendering many areas can be described in a fast and robust way using data.A pipeline (in D3D12/Vulkan lingo) for example is a collection of different states: depth stencil, alpha blend, rasterizer, shaders, etc.All those state can be hard-coded or defined in data.Moving them to data can help with the visibility of them, that instead of being buried somewhere into the code can be retrieved before even running the application.\nAs a bigger-scope example, a frame-graph can be implicitly defined inside the code, if different areas, or in data.Recent posts about it started raising attention to the problem, especially after the introduction of lower-level APIs like D3D12 and Vulkan and their resource barriers.Iâ€™ve personally used something like json (xml back in the day) since 2009, after asking myself the very silly question: what is the biggest dependency in rendering?Render Targets!\n Since then I saw only in the Codemasters postprocess system (since Dirt 2) a similar approach, and have never being able to advocate towards it.The only full use case I have is my personal indie game (a full deferred rendering pipeline with many different rendering needs) all defined in a json file (render_pipeline.json).Anyway, a couple of examples of this data-driven mentality can be found here:\nhttp://bitsquid.blogspot.com/2017/03/stingray-renderer-walkthrough-7-data.html\nI chose to see what is a good way of describing low-level rendering resources, the bricks towards data-driven rendering.Iâ€™ve already tried defining them in a json file, but wanted something more direct â€” something I can copy easily with minimal parsing.\nI found 4 possible approaches:\n Custom data language Already existing data language Json (already used) Hard-coding everything  In this experiment Iâ€™ve chosen Flatbuffers for the easy of use, the good performances and the feature set that seems complete.As an exercise, I wanted to create some UI based on the data coming from Flatbuffers without having to write too much code.\nFlatbuffers Flatbuffers is a serialization library developer by Google used by many companies.\nhttps://google.github.io/flatbuffers/\nCompared to Protocol Buffers (still developed by Google) it tries to go towards a very simple parsing/unpacking (actually ABSENT in Flatbuffers, so much faster to read/write) and serialization speed.\nFlatbuffers is mainly a compiler that accepts .fbs (FlatBuffers Schema) files and can generate code for serialization purposes.\nThe advantage is that it automatically generates the parsing files in the language you prefer (C++, Java, C#, Go, C, Lua, Javascript, Rust) without you needing to write the always tedious serialize/deserialize methods.\nIt is largely based on either simple c-structs or tables with offsets for more complex object.\nThe objective here will be to create a schema file, define a couple of resources (like textures) and use those to automatically generate UI.I will be using the SDL + ImGUI sample from the amazing ImGUI as a base.The flow will be the following:\n Write schema files Generate reflection informations Parse schemas Generate UI  Schema Files Letâ€™s write our first schema file. A bigger version (that I am using for my low-level renderer) is included in the github repository.\nnamespace rendering; enum TextureFormat : ushort { UNKNOWN, R32G32B32A32_TYPELESS, R32G32B32A32_FLOAT, R32G32B32A32_UINT, R32G32B32A32_SINT, R32G32B32_TYPELESS, R32G32B32_FLOAT, R32G32B32_UINT, R32G32B32_SINT, R16G16B16A16_TYPELESS, R16G16B16A16_FLOAT, R16G16B16A16_UNORM, R16G16B16A16_UINT, R16G16B16A16_SNORM, R16G16B16A16_SINT, R32G32_TYPELESS, R32G32_FLOAT, R32G32_UINT, R32G32_SINT, R10G10B10A2_TYPELESS, R10G10B10A2_UNORM, R10G10B10A2_UINT, R11G11B10_FLOAT, R8G8B8A8_TYPELESS, R8G8B8A8_UNORM, R8G8B8A8_UNORM_SRGB, R8G8B8A8_UINT, R8G8B8A8_SNORM, R8G8B8A8_SINT, R16G16_TYPELESS, R16G16_FLOAT, R16G16_UNORM, R16G16_UINT, R16G16_SNORM, R16G16_SINT, R32_TYPELESS, R32_FLOAT, R32_UINT, R32_SINT, R8G8_TYPELESS, R8G8_UNORM, R8G8_UINT, R8G8_SNORM, R8G8_SINT, R16_TYPELESS, R16_FLOAT, R16_UNORM, R16_UINT, R16_SNORM, R16_SINT, R8_TYPELESS, R8_UNORM, R8_UINT, R8_SNORM, R8_SINT, R9G9B9E5_SHAREDEXP, D32_FLOAT_S8X24_UINT, D32_FLOAT, D24_UNORM_S8_UINT, D24_UNORM_X8_UINT, D16_UNORM, S8_UINT, BC1_TYPELESS, BC1_UNORM, BC1_UNORM_SRGB, BC2_TYPELESS, BC2_UNORM, BC2_UNORM_SRGB, BC3_TYPELESS, BC3_UNORM, BC3_UNORM_SRGB, BC4_TYPELESS, BC4_UNORM, BC4_SNORM, BC5_TYPELESS, BC5_UNORM, BC5_SNORM, B5G6R5_UNORM, B5G5R5A1_UNORM, B8G8R8A8_UNORM, B8G8R8X8_UNORM, R10G10B10_XR_BIAS_A2_UNORM, B8G8R8A8_TYPELESS, B8G8R8A8_UNORM_SRGB, B8G8R8X8_TYPELESS, B8G8R8X8_UNORM_SRGB, BC6H_TYPELESS, BC6H_UF16, BC6H_SF16, BC7_TYPELESS, BC7_UNORM, BC7_UNORM_SRGB, FORCE_UINT } attribute \u0026quot;ui\u0026quot;; struct RenderTarget { width : ushort (ui: \u0026quot;min:1, max:16384\u0026quot;); height : ushort; scale_x : float; scale_y : float; format : TextureFormat; } There are few things here to discuss.\n Enums. Flatbuffers can generate enums with string version of each values and conversions between enum and string. Struct. It is exactly like C/C++: a simple struct that can be memcopied. Different than a Table (that can point to other structs and Tables). Attributes. This can be used to define custom parsable attributes linked to a member of a struct/table. They can be used, for example, to drive the UI generation.  Generating Reflection Informations After we generated the schema file, we can serialize it and load/save it from disk. But we need reflection data to be able to automatically generate the UI we need! There are two main reflection mechanisms in Flatbuffers: mini-reflection and full-reflection. We will use both to generate a UI using ImGUI and see the differences.\nMini-Reflection This is the simplest of the two and works by generating an additional header file for each .fbs file we use. The command line is the following:\nflatc --cpp RenderDefinitions.fbs --reflect-names This will generate the RenderDefinitions_Generated.h file that must be included in your application and has the downside of needing you to recompile every time you change the data.\nAlso, and this is the biggest downside, I could not find any way to parse custom per-member attributes.\nI hope I am wrong, but could not find any documentation on the topic: everything seems to point towards the full reflection mechanism.\nSo why bothering with the mini-reflection ?\nMini-reflection generates code, and this became useful for one of the most tedious C/C++ code to write: enums!\nI canâ€™t count how many times I wrote an enum, I wanted the string with the same value for it (for example to read from a json file and get the proper enum value) and every time an enum is changed is painful.\nSo a lesson from the mini-reflection is to have a code-generator for enums for C/C++, and I will show an example soon in another article.\nBack to the enums, Flatbuffers generates:\n Enum Name array Value array Enum to name method  A nice property of the generated code for the enum is that it is easy to copy-paste in any c++ file â€” no Flatbuffers involved!\nThis is my first choice now when I want to write an enum in any c++ application.\nFull-reflection This is the most used (or at least documented) form of reflection in Flatbuffers.\nIt use a very elegant solution, totally data-driven: it reads a reflection schema file that can parseâ€¦ANY other schema!\nThis very Inception-esque mechanism gives the full access to all the types, including Attributes.\nBy executing this command:\nflatc.exe -b --schema reflection.fbs RenderDefinitions.fbs the RenderDefinitions.bfbs (binary fbs) file is generated.\nThis is the file that needs to be read to fully reflect the types inside the .fbs file. The order of operations is the following:\n Generate a binary fbs with flatc (with the command line shown) Load the bfbs file generated Load the schema from the bfbs Reflect  The fbfs file contains all the informations from the schema: types, enums, attributes.\nParsing schemas and Generating UI For both reflection mechanisms the objective is the same: given a type (RenderTarget) generate an editor that can edit properties and potentially load/save them.\nMini-Reflection The UI generation is pretty straightforward with mini-reflection.\nEach type defined in the .fbs file contains a type_name-TypeTable() method that gives accent to a TypeTable.\nThis contains a list of per-member type, name and default values.\nWhat is really missing here is the attributes, that could be used to generate custom UI in a more specific way (eg. adding a min/max/step to a slider).\nThe code doing this is in the github sample.\nThere are few interesting points here.\nImGui usability In order to use ImGui to modify a struct, I had to create the class FlatBuffersReflectionTable to instantiate a struct with a similar layout than the Flatbuffers struct.\nThis is annoying but I could not find a way around different than this.\nWith this in-place, a ImGUI slider can point to a memory area that can be used to save/load the data. Letâ€™s begin by retrieving the TypeTable:\nconst TypeTable* rt_table = rendering::RenderTargetTypeTable(); The TypeTable is what is included in the generated header and contains the reflection informations. Listing the members and their type is pretty straight-forward:\nfor ( uint32_t i = 0; i \u0026lt; type_table.num_elems; ++i ) { const flatbuffers::TypeCode\u0026amp; type_code = type_table.type_codes[i]; ImGui::Text( \u0026quot;%s: %s\u0026quot;, type_table.names[i], flatbuffers::ElementaryTypeNames()[type_code.base_type] ); sprintf_s( s_string_buffer, 128, \u0026quot;%s\u0026quot;, type_table.names[i] ); if ( type_code.sequence_ref == 0 ) { if ( type_table.type_refs[type_code.sequence_ref] ) { const flatbuffers::TypeTable* enum_type = type_table.type_refs[type_code.sequence_ref](); ImGui::Combo( s_string_buffer, (int32_t*)reflection_table.GetData( i ), enum_type-\u0026gt;names, enum_type-\u0026gt;num_elems ); } } else { switch ( type_code.base_type ) { case flatbuffers::ET_BOOL: { ImGui::Checkbox( s_string_buffer, (bool*)reflection_table.GetData( i ) ); break; } } } } The interesting parts:\nflatbuffers::TypeCode* contains the reflection information for a type.\nGiven a type_code, sequence_ref can be used to check if it is an enum, pointer, or primitive type. In this case is used for enum, showing a combo with all the selectable values.\nBase_type contains instead the primitive type. In this example a bool can be mapped to a checkbox. This uses the custom reflection_table class to have a memory area for ImGUI.\nFor mini-reflection this is basically it.\nFull-reflection Code here is longer but it follows the 4 steps highlighted before.\nAll the code is inside the ReflectUIFull method.\nHere the binary fbs file and its corresponding schema are loaded.\n// 1. Obtain the schema from the binary fbs generated std::string bfbsfile; flatbuffers::LoadFile(\u0026quot;..\\\\data\\\\RenderDefinitions.bfbs\u0026quot;, true, \u0026amp;bfbsfile ); const reflection::Schema\u0026amp; schema = *reflection::GetSchema( bfbsfile.c_str() ); The schema can be used to list the types:\n// 2. List all the types present in the fbs. auto types = schema.objects(); for ( size_t i = 0; i \u0026lt; types-\u0026gt;Length(); i++ ) { const reflection::Object* type = types-\u0026gt;Get( i ); ImGui::Text( \u0026quot; %s\u0026quot;, type-\u0026gt;name()-\u0026gt;c_str() ); } (Using the auto here because I am lazy. The type is some multiple templates of offsetsâ€¦) We can also list all the enums:\nauto enums = schema.enums(); for ( size_t i = 0; i \u0026lt; enums-\u0026gt;Length(); i++ ) { const reflection::Enum* enum_ = enums-\u0026gt;Get( i ); ImGui::Text( \u0026quot; %s\u0026quot;, enum_-\u0026gt;name()-\u0026gt;c_str() ); } A problem I found (with a workaround in the code) is that enums do not have an easily to access array of string values.\nSo I generated one for the sake of example, but I am far from happy with the solution!\nGoing forward, we can get the type we want to reflect (notice the full namespace.type):\nauto render_target_type = types-\u0026gt;LookupByKey( \u0026quot;rendering.RenderTarget\u0026quot; ); and begin the work on each field: auto fields = render_target_type-\u0026gt;fields(); if ( fields ) { // 5.1. List all the fields for ( size_t i = 0; i \u0026lt; fields-\u0026gt;Length(); i++ ) { auto field = fields-\u0026gt;Get( i ); ... and the UI can be generated.\nFor each field, the primitive type can be accessed with the following:\nreflection::BaseType field_base_type = field-\u0026gt;type()-\u0026gt;base_type(); and again, I found a workaround to know if a type is primitive or an enum.\nLast piece of the puzzle: attributes!\nauto field_attributes = field-\u0026gt;attributes(); if ( field_attributes ) { auto ui = field_attributes-\u0026gt;LookupByKey( \u0026quot;ui\u0026quot; ); if ( ui ) { ImGui::Text(\u0026quot;UI attribute: %s\u0026quot;, ui-\u0026gt;value()-\u0026gt;c_str()); } } These can be parsed as strings and can be used to drive UI code (like a slider with min, max and steps).\nConclusions In the end, Iâ€™ve managed to generate UI based on a type without too much code.\nThere was some reverse-engineering to do because I could not find proper documentation (I possibly miss some links to a in-depth example of reflection!) but nothing major.\nThe full source code:\n(https://github.com/JorenJoestar/FlatbuffersReflection)\n","date":1564141046,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564141046,"objectID":"afba9f8775578a3383382dfbe5a617c4","permalink":"https://jorenjoestar.github.io/post/flatbuffers_reflection_data_driven_rendering/","publishdate":"2019-07-26T07:37:26-04:00","relpermalink":"/post/flatbuffers_reflection_data_driven_rendering/","section":"post","summary":"Auto generated UI from Flatbuffers files.   Motivation Finding a good balance between code and data in Rendering.What is the necessary code that should be written ?Why ?In rendering many areas can be described in a fast and robust way using data.A pipeline (in D3D12/Vulkan lingo) for example is a collection of different states: depth stencil, alpha blend, rasterizer, shaders, etc.All those state can be hard-coded or defined in data.","tags":[],"title":"Flatbuffers, Reflection and Data-Driven Rendering","type":"post"},{"authors":[],"categories":[],"content":"   Legend of Zelda emulated plus debugging windows.   Hello everyone!\nToday I release the source code of my bare-bone NES emulator, written in C++.\nI had the idea to write an emulator of one of my favorite console (after the SNES) years ago, and started in 2015 to write the first code (actually in 2008, but it was too daunting even to start). Then I concentrated on my other big project (still ongoing) and left all the NES code on a side. Years passed and finally last winter I decided to give it a go to arrive at a â€˜usableâ€™ emulator level and release the source code.\nHere it is! (https://github.com/JorenJoestar/HydraNes)\nMotivation Main motivation both to write and to share this code is knowledge.\nI shamelessly wrote bad code just with the purpose of seeing something on screen as fast as I could. And I am very honest about that: not happy for the form, but happy for the knowledge I gained! Also, I think that this code is compact enough to be followed and to understand the basics of NES emulation coding.\nThe code The NES code lives in the Nes.h/.cpp pair of files. The APU is implemented using Blarggâ€™s implementation: when Iâ€™ll have other time I will attemp to finish my own implementation, but for now it is ok like that.\nThe flow is the following:\n NES is initialized After loading a rom (from the Cartridge window) the mapper will be selected and memory copied to local buffers. CPU starts its continuous emulation. CPU will execute until a frame is produced. This is checked by the PPU frame changing. PPU execution is bound to memory accesses, both read and write. Each CPU memory access corresponds to 3 PPU cycles (in NTSC, the only region emulated). After the frame is ended the APU emulation is advanced.  Interesting spots There are different areas of the code that are interesting, but I would like to highlight some.\nCpu::Step() This is where all the CPU instructions are executed. I opted for a macro based approach instead of tables of function pointers.\nFor each cpu cycle:\n Fetch the instruction opcode Calculate the operand address (called â€˜effectiveAddressâ€™) Execute the operation  All the operations and addressing modes are in the Nes.h file. Addressing modes are the way the NES gets its operand for each operation. Operations are the instruction themselves â€” using those operands.\nPpu::Step() PPU by itself is the most difficult part to emulate (APU is easier on the channels, but harder on the mix and signal generation!).\nI will make a post about that soon, but in the meantime here the code is and implements the behaviours described here:\nhttps://wiki.nesdev.com/w/index.php/File:Ntsc_timing.png\nThe PPU draws in tiles of 8x8 pixels, so for each pixels created on the screen there will be a gathering of all the data necessary to calculate the final color.\nThe rendering is divided in background and sprites.\nBackground is just 8x8 pixel per tile choosen from the nametable (a screen table of which tiles are visible) and sprites are either 8x8 or 8x16 rectangles coming from a different memory area (uploaded using DMA).\nThere are many quirks and uniqueness about the PPU, like the pattern table (a 16x16 grid storing the higher 2 bits of all the underlying background pixels), or the vertical blank period, or the open bus.\nPpu::DrawPixel() The color of a pixel comes from one of the 16 entries of the palette VRAM, and to do so 4 bits must be calculated for background and for sprites.\nFor background tiles, 2 pixels comes from the â€˜textureâ€™ (CHR-ROM) and 2 from the attribute table. Sprites contains all those informations together.\nThe output is a silly SSBO that contains RGBA colors to be used in a compute shader that outputs to the screen.\nCpuRead/Write, PpuRead/Write All those methods are essential because the NES uses memory mapping i/o to access the different hardware.\nFor example the PPU access the cartridge through the mapper in the memory controller to read drawing informations, the CPU writes to the PPU using address $2007, etc.\nEnding notes I will prepare more detailed posts about the NES architecture and emulation, even though there are still some concepts that are not clear to me and require a deeper investigation.\nSo far this is the most satisfactory personal project Iâ€™ve done, and one of the few that arrived at a usable level.\nIn the future I want to improve this emulator and use the knowledge to explore the writing of a SNES emulator!\nAny question or comment please let me know!\nGabriel\n","date":1563861890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563861890,"objectID":"96109d55d54d12572b76d4542ac35062","permalink":"https://jorenjoestar.github.io/post/releasing_nes_emulator_source/","publishdate":"2019-07-23T02:04:50-04:00","relpermalink":"/post/releasing_nes_emulator_source/","section":"post","summary":"Legend of Zelda emulated plus debugging windows.   Hello everyone!\nToday I release the source code of my bare-bone NES emulator, written in C++.\nI had the idea to write an emulator of one of my favorite console (after the SNES) years ago, and started in 2015 to write the first code (actually in 2008, but it was too daunting even to start). Then I concentrated on my other big project (still ongoing) and left all the NES code on a side.","tags":[],"title":"Releasing NES Emulator Source","type":"post"}]